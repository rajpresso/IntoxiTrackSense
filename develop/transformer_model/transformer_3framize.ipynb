{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "over_90=[]\n",
    "import os\n",
    "for folders in os.listdir('/home/alpaco/project/drunk_prj/data/videofile/32.이동 행위'):\n",
    "    vid_path = os.path.join('/home/alpaco/project/drunk_prj/data/videofile/32.이동 행위',folders)\n",
    "    for video in os.listdir(vid_path):\n",
    "        video_path = os.path.join(vid_path,video)\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"동영상을 열 수 없습니다.\")\n",
    "        else:\n",
    "            # 총 프레임 수 확인\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames >=90:\n",
    "            csv_s = video.split('.')[0]+'.csv'\n",
    "            new_path = os.path.join('/home/alpaco/project/drunk_prj/data/normal_ver2',folders)\n",
    "            new_path = os.path.join(new_path,'abs')\n",
    "            new_path = os.path.join(new_path,csv_s)\n",
    "            over_90.append([new_path,total_frames//2-45,total_frames//2+45])\n",
    "        # 자원 해제\n",
    "        cap.release()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_900=[]\n",
    "for file in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_video1/totter/Abs'):\n",
    "    folder = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_video1/totter/Abs',file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_b_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_b_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_a_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_a_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_a_aft_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_b_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_c_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_c_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_c_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_a_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_b_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_c_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_c_aft_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_b_aft_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_a_aft_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_a_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_a_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_c_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_b_aft_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_b_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_c_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_a_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_c_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_b_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_a_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_b_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_c_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_b_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_c_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_a_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_b_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_c_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_a_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_a_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_a_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_b_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_b_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_c_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_b_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_c_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_b_for_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_a_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_c_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_c_for_DF2_(116_206).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_b_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_a_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_a_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_b_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_a_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_c_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_b_for_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_c_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_c_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_a_for_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_a_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_b_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_a_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_a_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_b_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_b_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_a_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_c_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_a_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_b_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_c_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_b_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_b_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_a_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_c_for_DF2_(59_149).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_c_for_DF2_(59_149).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_a_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_b_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_b_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_a_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_a_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_c_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_b_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_c_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_b_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_a_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_b_for_DF2_(54_144).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/abs_normal_30초\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in over_900:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam02_drunken04_place03_night_summer_153_1969_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam02_drunken04_place03_night_summer_153_1969_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_winter_413_2350_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_winter_413_2350_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam03_drunken04_place03_night_spring_174_1098_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam03_drunken04_place03_night_spring_174_1098_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam02_drunken03_place03_night_spring_210_1287_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam02_drunken03_place03_night_spring_210_1287_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam03_drunken03_place03_night_summer_1957_3339_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam03_drunken03_place03_night_summer_1957_3339_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_winter_394_2328_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_winter_394_2328_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam03_drunken01_place03_night_summer_283_2580_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam03_drunken01_place03_night_summer_283_2580_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_winter_382_4237_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_winter_382_4237_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam03_drunken04_place03_night_winter_173_2227_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam03_drunken04_place03_night_winter_173_2227_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_winter_413_2350_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_winter_413_2350_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam01_drunken04_place02_night_winter_7033_8201_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam01_drunken04_place02_night_winter_7033_8201_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-5_cam02_drunken03_place03_night_summer_1671_2626_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-5_cam02_drunken03_place03_night_summer_1671_2626_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_winter_5718_6654_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_winter_5718_6654_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam02_drunken03_place03_night_summer_1995_3379_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam02_drunken03_place03_night_summer_1995_3379_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_spring_4613_6061_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_spring_4613_6061_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam01_drunken03_place03_night_summer_1136_4740_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam01_drunken03_place03_night_summer_1136_4740_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_summer_397_2910_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_summer_397_2910_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_spring_526_3559_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_spring_526_3559_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_summer_6040_8183_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_summer_6040_8183_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_winter_382_4237_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_winter_382_4237_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam02_drunken03_place03_night_winter_176_1512_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam02_drunken03_place03_night_winter_176_1512_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_summer_442_2906_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_summer_442_2906_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-1_cam02_drunken03_place03_night_winter_3063_4354_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-1_cam02_drunken03_place03_night_winter_3063_4354_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-1_cam01_drunken03_place03_night_winter_2938_4297_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-1_cam01_drunken03_place03_night_winter_2938_4297_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam01_drunken01_place03_night_winter_2202_3244_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam01_drunken01_place03_night_winter_2202_3244_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_winter_382_4237_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_winter_382_4237_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam03_drunken03_place03_night_winter_803_3075_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam03_drunken03_place03_night_winter_803_3075_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_winter_4195_5643_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_winter_4195_5643_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam02_drunken04_place02_night_winter_6974_8224_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam02_drunken04_place02_night_winter_6974_8224_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam01_drunken03_place03_night_summer_1136_4740_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam01_drunken03_place03_night_summer_1136_4740_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-5_cam03_drunken04_place03_night_spring_2140_3078_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-5_cam03_drunken04_place03_night_spring_2140_3078_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/229-5_cam02_drunken04_place03_night_winter_138_1161_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/229-5_cam02_drunken04_place03_night_winter_138_1161_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam01_drunken01_place03_night_winter_1611_2920_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam01_drunken01_place03_night_winter_1611_2920_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_winter_421_4278_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_winter_421_4278_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam01_drunken01_place03_night_summer_495_2457_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam01_drunken01_place03_night_summer_495_2457_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam02_drunken01_place03_night_summer_178_1550_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam02_drunken01_place03_night_summer_178_1550_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_winter_141_3004_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_winter_141_3004_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam01_drunken04_place03_night_winter_265_1768_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam01_drunken04_place03_night_winter_265_1768_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_spring_173_3282_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_spring_173_3282_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_winter_1990_3986_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_winter_1990_3986_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam01_drunken03_place03_night_winter_479_1427_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam01_drunken03_place03_night_winter_479_1427_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam01_drunken01_place03_night_summer_336_1535_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam01_drunken01_place03_night_summer_336_1535_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_winter_141_3004_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_winter_141_3004_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam02_drunken01_place03_night_summer_1704_3061_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam02_drunken01_place03_night_summer_1704_3061_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam03_drunken01_place03_night_spring_4992_6262_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam03_drunken01_place03_night_spring_4992_6262_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam03_drunken02_place01_night_winter_122_1113_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam03_drunken02_place01_night_winter_122_1113_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_winter_421_4278_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_winter_421_4278_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam01_drunken03_place03_night_winter_1478_3040_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam01_drunken03_place03_night_winter_1478_3040_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam02_drunken01_place03_night_winter_1776_2994_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam02_drunken01_place03_night_winter_1776_2994_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_summer_576_4476_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_summer_576_4476_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam01_drunken03_place03_night_winter_473_4519_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam01_drunken03_place03_night_winter_473_4519_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_spring_5112_6252_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_spring_5112_6252_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_summer_151_3017_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_summer_151_3017_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam03_drunken04_place03_night_summer_155_1916_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam03_drunken04_place03_night_summer_155_1916_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam01_drunken03_place03_night_winter_492_1623_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam01_drunken03_place03_night_winter_492_1623_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam02_drunken03_place03_night_summer_126_1653_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam02_drunken03_place03_night_summer_126_1653_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam02_drunken01_place03_night_winter_2241_3356_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam02_drunken01_place03_night_winter_2241_3356_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam02_drunken04_place02_night_summer_7295_8596_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam02_drunken04_place02_night_summer_7295_8596_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam01_drunken04_place03_night_summer_147_1942_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam01_drunken04_place03_night_summer_147_1942_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam03_drunken03_place03_night_winter_121_1588_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam03_drunken03_place03_night_winter_121_1588_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam01_drunken01_place03_night_spring_4990_6291_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam01_drunken01_place03_night_spring_4990_6291_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam01_drunken01_place03_night_summer_363_1764_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam01_drunken01_place03_night_summer_363_1764_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam01_drunken04_place02_night_summer_7333_8646_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam01_drunken04_place02_night_summer_7333_8646_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam01_drunken02_place01_night_winter_1935_2973_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam01_drunken02_place01_night_winter_1935_2973_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam02_drunken03_place03_night_winter_472_4543_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam02_drunken03_place03_night_winter_472_4543_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_summer_442_2906_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_summer_442_2906_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam03_drunken01_place03_night_spring_4998_6499_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam03_drunken01_place03_night_spring_4998_6499_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_summer_495_4494_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_summer_495_4494_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam03_drunken04_place03_night_summer_400_1810_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam03_drunken04_place03_night_summer_400_1810_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam01_drunken01_place03_night_summer_1751_3124_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam01_drunken01_place03_night_summer_1751_3124_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-5_cam02_drunken04_place03_night_winter_222_1402_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-5_cam02_drunken04_place03_night_winter_222_1402_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-1_cam02_drunken03_place03_night_winter_213_1876_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-1_cam02_drunken03_place03_night_winter_213_1876_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam02_drunken01_place03_night_spring_5000_6280_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam02_drunken01_place03_night_spring_5000_6280_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam02_drunken01_place03_night_summer_204_2607_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam02_drunken01_place03_night_summer_204_2607_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_summer_495_4494_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_summer_495_4494_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam03_drunken03_place03_night_winter_803_3075_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam03_drunken03_place03_night_winter_803_3075_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam01_drunken01_place03_night_spring_388_1488_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam01_drunken01_place03_night_spring_388_1488_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam01_drunken03_place03_night_summer_1136_4740_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam01_drunken03_place03_night_summer_1136_4740_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam02_drunken03_place03_night_winter_472_4543_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam02_drunken03_place03_night_winter_472_4543_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam01_drunken03_place03_night_winter_2716_3841_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam01_drunken03_place03_night_winter_2716_3841_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam03_drunken01_place03_night_winter_2260_3383_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam03_drunken01_place03_night_winter_2260_3383_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_spring_2261_3878_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_spring_2261_3878_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_winter_140_2996_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_winter_140_2996_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_winter_140_2996_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_winter_140_2996_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/618-9_cam02_drunken01_place03_night_summer_7673_8654_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/618-9_cam02_drunken01_place03_night_summer_7673_8654_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_spring_332_3141_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_spring_332_3141_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_summer_495_4494_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_summer_495_4494_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam01_drunken04_place02_night_spring_6703_7742_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam01_drunken04_place02_night_spring_6703_7742_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam03_drunken03_place03_night_winter_481_4532_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam03_drunken03_place03_night_winter_481_4532_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam03_drunken04_place03_night_winter_173_2227_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam03_drunken04_place03_night_winter_173_2227_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam03_drunken01_place03_night_spring_570_1572_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam03_drunken01_place03_night_spring_570_1572_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-2_cam02_drunken01_place03_night_spring_2247_3165_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-2_cam02_drunken01_place03_night_spring_2247_3165_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_spring_5041_6469_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_spring_5041_6469_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam01_drunken03_place03_night_winter_473_4519_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam01_drunken03_place03_night_winter_473_4519_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_summer_234_1492_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_summer_234_1492_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam03_drunken02_place01_night_summer_107_1221_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam03_drunken02_place01_night_summer_107_1221_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_summer_1841_2949_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_summer_1841_2949_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-2_cam01_drunken01_place03_night_spring_2255_3161_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-2_cam01_drunken01_place03_night_spring_2255_3161_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam03_drunken03_place03_night_winter_481_4532_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam03_drunken03_place03_night_winter_481_4532_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam03_drunken01_place03_night_winter_261_2243_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam03_drunken01_place03_night_winter_261_2243_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam02_drunken04_place03_night_winter_297_1793_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam02_drunken04_place03_night_winter_297_1793_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_winter_436_2936_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_winter_436_2936_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam03_drunken03_place03_night_winter_2733_3932_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam03_drunken03_place03_night_winter_2733_3932_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_winter_6874_8356_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_winter_6874_8356_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_winter_4115_5522_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_winter_4115_5522_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam03_drunken03_place03_night_spring_165_1183_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam03_drunken03_place03_night_spring_165_1183_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-2_cam01_drunken03_place03_night_summer_1136_4740_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-2_cam01_drunken03_place03_night_summer_1136_4740_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam03_drunken01_place03_night_summer_6537_8047_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam03_drunken01_place03_night_summer_6537_8047_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam02_drunken04_place03_night_winter_168_2251_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam02_drunken04_place03_night_winter_168_2251_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_summer_6067_8181_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_summer_6067_8181_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-1_cam03_drunken03_place03_night_winter_162_1576_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-1_cam03_drunken03_place03_night_winter_162_1576_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam03_drunken04_place02_night_winter_6973_8183_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam03_drunken04_place02_night_winter_6973_8183_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_winter_7324_8409_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_winter_7324_8409_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_spring_4528_5885_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_spring_4528_5885_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_spring_5038_6650_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_spring_5038_6650_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam02_drunken01_place03_night_winter_5791_7513_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam02_drunken01_place03_night_winter_5791_7513_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam02_drunken03_place03_night_winter_142_1602_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam02_drunken03_place03_night_winter_142_1602_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam01_drunken01_place03_night_winter_234_2164_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam01_drunken01_place03_night_winter_234_2164_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-2_cam03_drunken01_place03_night_spring_2266_3496_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-2_cam03_drunken01_place03_night_spring_2266_3496_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam03_drunken01_place03_night_spring_6807_8169_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam03_drunken01_place03_night_spring_6807_8169_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_spring_332_3141_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_spring_332_3141_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam03_drunken01_place03_night_winter_3994_5252_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam03_drunken01_place03_night_winter_3994_5252_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_spring_394_3564_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_spring_394_3564_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam01_drunken01_place03_night_spring_256_1162_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam01_drunken01_place03_night_spring_256_1162_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_spring_526_3559_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_spring_526_3559_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_summer_4248_5174_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_summer_4248_5174_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam03_drunken01_place03_night_summer_411_1463_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam03_drunken01_place03_night_summer_411_1463_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-5_cam02_drunken03_place03_night_winter_1634_2751_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-5_cam02_drunken03_place03_night_winter_1634_2751_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_winter_5782_6696_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_winter_5782_6696_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_summer_6067_8181_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_summer_6067_8181_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_summer_189_3078_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_summer_189_3078_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_summer_1877_3123_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_summer_1877_3123_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam03_drunken03_place03_night_spring_96_1488_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam03_drunken03_place03_night_spring_96_1488_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam03_drunken03_place03_night_winter_481_4532_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam03_drunken03_place03_night_winter_481_4532_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_spring_420_1965_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_spring_420_1965_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam01_drunken02_place01_night_summer_264_1413_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam01_drunken02_place01_night_summer_264_1413_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_spring_293_1245_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_spring_293_1245_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam02_drunken01_place03_night_summer_397_2910_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam02_drunken01_place03_night_summer_397_2910_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_summer_151_3017_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_summer_151_3017_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_spring_394_3564_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_spring_394_3564_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_summer_406_1537_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_summer_406_1537_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_winter_1990_3986_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_winter_1990_3986_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/229-2_cam03_drunken04_place03_night_spring_93_1021_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/229-2_cam03_drunken04_place03_night_spring_93_1021_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam01_drunken04_place02_night_spring_3377_5818_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam01_drunken04_place02_night_spring_3377_5818_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam01_drunken03_place03_night_winter_473_4519_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam01_drunken03_place03_night_winter_473_4519_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam02_drunken03_place03_night_winter_472_4543_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam02_drunken03_place03_night_winter_472_4543_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_summer_189_3078_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_summer_189_3078_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam02_drunken01_place03_night_spring_1416_3428_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam02_drunken01_place03_night_spring_1416_3428_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_spring_1440_3441_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_spring_1440_3441_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_winter_7296_8365_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_winter_7296_8365_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_spring_455_3546_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_spring_455_3546_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam03_drunken03_place03_night_winter_481_4532_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam03_drunken03_place03_night_winter_481_4532_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam02_drunken01_place03_night_winter_248_2231_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam02_drunken01_place03_night_winter_248_2231_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_spring_158_3267_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_spring_158_3267_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam01_drunken01_place03_night_spring_6823_8222_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam01_drunken01_place03_night_spring_6823_8222_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam01_drunken01_place03_night_spring_1333_3315_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam01_drunken01_place03_night_spring_1333_3315_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-5_cam01_drunken03_place03_night_summer_1639_2579_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-5_cam01_drunken03_place03_night_summer_1639_2579_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam01_drunken02_place01_night_winter_321_1389_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam01_drunken02_place01_night_winter_321_1389_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_spring_455_3546_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_spring_455_3546_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/229-5_cam01_drunken04_place03_night_summer_173_1204_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/229-5_cam01_drunken04_place03_night_summer_173_1204_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/229-5_cam03_drunken04_place03_night_summer_127_1218_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/229-5_cam03_drunken04_place03_night_summer_127_1218_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam03_drunken01_place03_night_spring_1440_3441_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam03_drunken01_place03_night_spring_1440_3441_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_spring_2279_3416_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_spring_2279_3416_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_spring_158_3267_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_spring_158_3267_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_summer_495_4494_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_summer_495_4494_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_winter_141_3004_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_winter_141_3004_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam01_drunken01_place03_night_winter_632_1560_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam01_drunken01_place03_night_winter_632_1560_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam03_drunken03_place03_night_summer_156_1626_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam03_drunken03_place03_night_summer_156_1626_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam02_drunken01_place03_night_spring_1416_3428_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam02_drunken01_place03_night_spring_1416_3428_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-5_cam01_drunken03_place03_night_winter_1645_2747_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-5_cam01_drunken03_place03_night_winter_1645_2747_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam03_drunken01_place03_night_spring_228_1563_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam03_drunken01_place03_night_spring_228_1563_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam03_drunken01_place03_night_winter_261_2243_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam03_drunken01_place03_night_winter_261_2243_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_spring_5131_6267_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_spring_5131_6267_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_spring_7074_8010_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_spring_7074_8010_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_summer_189_3078_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_summer_189_3078_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_spring_332_3141_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_spring_332_3141_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-5_cam01_drunken01_place03_night_spring_1031_1959_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-5_cam01_drunken01_place03_night_spring_1031_1959_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam02_drunken03_place03_night_spring_141_1493_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam02_drunken03_place03_night_spring_141_1493_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam02_drunken01_place03_night_spring_242_1249_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam02_drunken01_place03_night_spring_242_1249_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam02_drunken01_place03_night_winter_248_2231_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam02_drunken01_place03_night_winter_248_2231_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam02_drunken03_place03_night_spring_1378_2664_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam02_drunken03_place03_night_spring_1378_2664_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam02_drunken03_place03_night_winter_472_4543_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam02_drunken03_place03_night_winter_472_4543_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam02_drunken01_place03_night_summer_151_1472_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam02_drunken01_place03_night_summer_151_1472_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam02_drunken02_place01_night_winter_1813_2977_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam02_drunken02_place01_night_winter_1813_2977_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_spring_455_3546_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_spring_455_3546_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam01_drunken01_place03_night_summer_495_2457_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam01_drunken01_place03_night_summer_495_2457_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_summer_151_3017_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_summer_151_3017_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam02_drunken01_place03_night_spring_141_1497_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam02_drunken01_place03_night_spring_141_1497_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_winter_4185_5658_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_winter_4185_5658_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_winter_140_2996_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_winter_140_2996_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_winter_7281_8371_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_winter_7281_8371_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_spring_5106_6243_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_spring_5106_6243_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam01_drunken04_place03_night_winter_194_2250_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam01_drunken04_place03_night_winter_194_2250_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam02_drunken01_place03_night_spring_6819_8189_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam02_drunken01_place03_night_spring_6819_8189_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam03_drunken01_place03_night_summer_289_2180_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam03_drunken01_place03_night_summer_289_2180_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/229-2_cam03_drunken04_place03_night_summer_112_1036_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/229-2_cam03_drunken04_place03_night_summer_112_1036_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam03_drunken04_place03_night_winter_248_1762_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam03_drunken04_place03_night_winter_248_1762_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam03_drunken01_place03_night_summer_283_2580_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam03_drunken01_place03_night_summer_283_2580_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-4_cam02_drunken03_place03_night_winter_1597_3116_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-4_cam02_drunken03_place03_night_winter_1597_3116_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-5_cam03_drunken03_place03_night_winter_1622_2757_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-5_cam03_drunken03_place03_night_winter_1622_2757_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_summer_462_4454_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_summer_462_4454_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam03_drunken02_place01_night_winter_1713_2835_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam03_drunken02_place01_night_winter_1713_2835_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam03_drunken01_place03_night_summer_289_2180_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam03_drunken01_place03_night_summer_289_2180_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam02_drunken01_place03_night_summer_6579_8095_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam02_drunken01_place03_night_summer_6579_8095_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_winter_421_4278_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_winter_421_4278_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_summer_462_4454_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_summer_462_4454_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_spring_158_3267_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_spring_158_3267_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam03_drunken03_place03_night_spring_4649_6001_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam03_drunken03_place03_night_spring_4649_6001_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-2_cam02_drunken04_place03_night_summer_153_1969_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-2_cam02_drunken04_place03_night_summer_153_1969_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_summer_6040_8183_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_summer_6040_8183_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_summer_576_4476_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_summer_576_4476_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_summer_6070_8216_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_summer_6070_8216_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam01_drunken01_place03_night_winter_234_2164_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam01_drunken01_place03_night_winter_234_2164_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-1_cam01_drunken03_place03_night_winter_198_1608_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-1_cam01_drunken03_place03_night_winter_198_1608_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam02_drunken04_place02_night_spring_6577_7747_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam02_drunken04_place02_night_spring_6577_7747_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam03_drunken03_place03_night_summer_117_1029_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam03_drunken03_place03_night_summer_117_1029_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam01_drunken01_place03_night_summer_1842_3078_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam01_drunken01_place03_night_summer_1842_3078_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_winter_443_4252_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_winter_443_4252_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/214-2_cam03_drunken04_place02_night_spring_6701_7707_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/214-2_cam03_drunken04_place02_night_spring_6701_7707_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_summer_576_4476_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_summer_576_4476_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-4_cam01_drunken02_place01_night_spring_117_1033_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-4_cam01_drunken02_place01_night_spring_117_1033_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_summer_462_4454_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_summer_462_4454_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-1_cam01_drunken01_place03_night_summer_2790_3777_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-1_cam01_drunken01_place03_night_summer_2790_3777_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_spring_526_3559_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_spring_526_3559_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam03_drunken01_place03_night_summer_576_4476_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam03_drunken01_place03_night_summer_576_4476_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-1_cam03_drunken03_place03_night_winter_3034_4264_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-1_cam03_drunken03_place03_night_winter_3034_4264_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-2_cam01_drunken03_place03_night_winter_473_4519_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-2_cam01_drunken03_place03_night_winter_473_4519_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam02_drunken02_place01_night_winter_227_1241_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam02_drunken02_place01_night_winter_227_1241_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/212-6_cam01_drunken03_place03_night_spring_131_1922_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/212-6_cam01_drunken03_place03_night_spring_131_1922_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam02_drunken03_place03_night_spring_173_3282_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam02_drunken03_place03_night_spring_173_3282_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_summer_462_4454_part3.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_summer_462_4454_part3.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam02_drunken04_place03_night_winter_168_2251_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam02_drunken04_place03_night_winter_168_2251_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-5_cam01_drunken01_place03_night_spring_451_1524_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-5_cam01_drunken01_place03_night_spring_451_1524_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_spring_394_3564_part2.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_spring_394_3564_part2.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/216-1_cam02_drunken02_place01_night_summer_270_1447_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/216-1_cam02_drunken02_place01_night_summer_270_1447_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/217-6_cam01_drunken04_place03_night_winter_194_2250_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/217-6_cam01_drunken04_place03_night_winter_194_2250_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_winter_436_2936_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_winter_436_2936_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-4_cam02_drunken01_place03_night_summer_204_2607_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-4_cam02_drunken01_place03_night_summer_204_2607_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/218-2_cam01_drunken01_place03_night_spring_1333_3315_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/218-2_cam01_drunken01_place03_night_spring_1333_3315_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam01_drunken01_place03_night_summer_6070_8216_part1.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam01_drunken01_place03_night_summer_6070_8216_part1.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/209-3_cam02_drunken01_place03_night_winter_421_4278_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/209-3_cam02_drunken01_place03_night_winter_421_4278_part0.mp4_combined.csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/chroma3frame/278-3_cam01_drunken03_place03_night_summer_4111_5095_part0.mp4_combined.csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/chroma3frame0넣기/278-3_cam01_drunken03_place03_night_summer_4111_5095_part0.mp4_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/abs_normal_30초'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/abs_normal_0넣기'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue  # 중단하지 않고 다음 파일로 이동\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열 확인\n",
    "    if 'label' not in df.columns:\n",
    "        print(f\"'label' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열 확인\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열을 기준으로 그룹화 및 프레임 보정\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        # 전체 프레임 범위 생성\n",
    "        full_frames = pd.DataFrame({'frame': range(min_frame, max_frame + 1)})\n",
    "\n",
    "        # 기존 데이터와 병합하여 누락된 프레임 추가\n",
    "        merged = pd.merge(full_frames, group, on='frame', how='left')\n",
    "\n",
    "        # 누락된 값 채우기\n",
    "        for col in merged.columns:\n",
    "            if col != 'frame' and col != 'label':  # frame과 label 열은 유지\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "        merged['label'] = merged['label'].fillna(label)\n",
    "\n",
    "        # 처리된 데이터 저장\n",
    "        processed_dfs.append(merged)\n",
    "\n",
    "    # 그룹 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_900=[]\n",
    "for folders in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_video1/totter/Rel'):\n",
    "    vid_path = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_video1/totter/Rel',folders)\n",
    "    for video in os.listdir(vid_path):\n",
    "        start,end = int(video.split('_')[-3]),int((video.split('_')[-2]).split('.')[0])\n",
    "        total = end- start\n",
    "        if total >=900:\n",
    "            video_name = os.path.join(vid_path,video)\n",
    "            over_900.append([video_name,total//2-450,total//2+450])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam03_drunken03_place03_night_summer_151_3017_totter_(983_1883).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam03_drunken03_place03_night_winter_2733_3932_totter_(149_1049).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam02_drunken03_place03_night_winter_1597_3116_totter_(309_1209).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam01_drunken03_place03_night_winter_1478_3040_totter_(331_1231).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam01_drunken03_place03_night_summer_1136_4740_totter_(1352_2252).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam03_drunken03_place03_night_winter_162_1576_totter_(257_1157).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam01_drunken03_place03_night_summer_133_1529_totter_(248_1148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam03_drunken03_place03_night_winter_140_2996_totter_(978_1878).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam01_drunken03_place03_night_winter_479_1427_totter_(24_924).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam02_drunken03_place03_night_winter_213_1876_totter_(381_1281).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam02_drunken03_place03_night_summer_1995_3379_totter_(242_1142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam03_drunken03_place03_night_spring_165_1183_totter_(59_959).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam01_drunken03_place03_night_winter_198_1608_totter_(255_1155).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam03_drunken03_place03_night_winter_4195_5643_totter_(274_1174).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_winter_4185_5658_totter_(286_1186).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_spring_332_3141_totter_(954_1854).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam03_drunken03_place03_night_summer_1957_3339_totter_(241_1141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_spring_173_3282_totter_(1104_2004).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_spring_4528_5885_totter_(228_1128).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_winter_436_2936_totter_(800_1700).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_summer_189_3078_totter_(994_1894).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam02_drunken03_place03_night_winter_176_1512_totter_(218_1118).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_summer_442_2906_totter_(782_1682).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_summer_4111_5095_totter_(42_942).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_winter_141_3004_totter_(981_1881).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam03_drunken03_place03_night_summer_156_1626_totter_(285_1185).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam02_drunken03_place03_night_winter_3063_4354_totter_(195_1095).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_summer_4248_5174_totter_(13_913).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam01_drunken03_place03_night_winter_2716_3841_totter_(112_1012).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_spring_4613_6061_totter_(274_1174).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_winter_4115_5522_totter_(253_1153).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_summer_4693_5799_totter_(103_1003).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam02_drunken03_place02_night_spring_7694_8961_totter_(183_1083).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam02_drunken03_place02_night_winter_1594_3038_totter_(272_1172).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam01_drunken03_place02_night_winter_4408_5592_totter_(142_1042).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam01_drunken03_place02_night_winter_8030_8939_totter_(4_904).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam02_drunken03_place02_night_spring_7390_8709_totter_(209_1109).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam01_drunken03_place02_night_winter_1681_3116_totter_(267_1167).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam02_drunken03_place02_night_winter_7950_8870_totter_(10_910).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_winter_1589_3053_totter_(282_1182).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-3_cam01_drunken03_place02_night_winter_7568_8475_totter_(3_903).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_spring_3793_4965_totter_(136_1036).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_winter_8296_9225_totter_(14_914).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam01_drunken03_place02_night_spring_7460_8736_totter_(188_1088).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_spring_7371_8417_totter_(73_973).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam03_drunken03_place02_night_summer_4668_5697_totter_(64_964).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-3_cam01_drunken03_place02_night_summer_1205_2292_totter_(93_993).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam03_drunken03_place02_night_winter_4353_5577_totter_(162_1062).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam03_drunken03_place02_night_spring_7702_8943_totter_(170_1070).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam02_drunken03_place02_night_summer_7436_8740_totter_(202_1102).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam03_drunken03_place02_night_spring_3786_4969_totter_(141_1041).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam01_drunken03_place02_night_summer_1227_2149_totter_(11_911).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_summer_7625_8630_totter_(52_952).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_spring_2435_3361_totter_(13_913).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam01_drunken03_place02_night_summer_4699_5755_totter_(78_978).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam01_drunken03_place02_night_spring_7893_9008_totter_(107_1007).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_winter_4418_5531_totter_(106_1006).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam01_drunken03_place03_night_winter_1645_2747_totter_(101_1001).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam01_drunken04_place02_night_spring_3310_4503_totter_(146_1046).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_spring_6703_7742_totter_(69_969).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam01_drunken03_place03_night_winter_492_1623_totter_(115_1015).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_spring_3372_5824_totter_(776_1676).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_winter_6973_8183_totter_(155_1055).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_spring_3377_5818_totter_(770_1670).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam03_drunken03_place03_night_summer_1644_2608_totter_(32_932).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam03_drunken03_place03_night_spring_96_1488_totter_(246_1146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam01_drunken03_place03_night_winter_473_4519_totter_(1573_2473).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_spring_3440_5836_totter_(748_1648).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_winter_3801_6423_totter_(861_1761).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_winter_7033_8201_totter_(134_1034).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam02_drunken03_place03_night_winter_472_4543_totter_(1585_2485).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam03_drunken03_place03_night_winter_481_4532_totter_(1575_2475).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_winter_3778_6383_totter_(852_1752).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam01_drunken03_place03_night_summer_1639_2579_totter_(20_920).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam02_drunken03_place03_night_summer_1671_2626_totter_(27_927).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_summer_4043_6527_totter_(792_1692).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam02_drunken03_place03_night_winter_142_1602_totter_(280_1180).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam02_drunken04_place02_night_summer_3827_5457_totter_(365_1265).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_summer_7333_8646_totter_(206_1106).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam02_drunken03_place03_night_winter_1634_2751_totter_(108_1008).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_spring_6701_7707_totter_(53_953).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_summer_7269_8601_totter_(216_1116).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam03_drunken03_place03_night_summer_117_1029_totter_(6_906).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam03_drunken03_place03_night_winter_121_1588_totter_(283_1183).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_spring_6577_7747_totter_(135_1035).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_summer_7295_8596_totter_(200_1100).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_winter_6974_8224_totter_(175_1075).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam02_drunken03_place03_night_spring_141_1493_totter_(226_1126).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam02_drunken03_place03_night_spring_1378_2664_totter_(193_1093).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam01_drunken03_place03_night_spring_131_1922_totter_(445_1345).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_summer_4001_6485_totter_(792_1692).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam03_drunken03_place03_night_winter_1622_2757_totter_(117_1017).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_winter_3804_6397_totter_(846_1746).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_summer_3995_6473_totter_(789_1689).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam01_drunken04_place02_night_summer_3701_5505_totter_(452_1352).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam03_drunken01_place03_night_winter_381_2313_totter_(516_1416).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam01_drunken01_place03_night_spring_4990_6291_totter_(200_1100).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_spring_4992_6262_totter_(185_1085).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam01_drunken01_place03_night_spring_6823_8222_totter_(249_1149).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_summer_397_2910_totter_(806_1706).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_spring_5038_6650_totter_(356_1256).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_spring_6807_8169_totter_(231_1131).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_summer_406_1537_totter_(115_1015).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_spring_6819_8189_totter_(235_1135).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_spring_455_3546_totter_(1095_1995).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_summer_6579_8095_totter_(308_1208).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_winter_5718_6654_totter_(18_918).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_winter_5752_6663_totter_(5_905).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_winter_382_4237_totter_(1477_2377).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_winter_5791_7513_totter_(411_1311).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_spring_394_3564_totter_(1135_2035).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_spring_2261_3878_totter_(358_1258).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_winter_7296_8365_totter_(84_984).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_summer_576_4476_totter_(1500_2400).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-2_cam03_drunken01_place03_night_spring_2266_3496_totter_(165_1065).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-2_cam02_drunken01_place03_night_spring_2247_3165_totter_(9_909).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_spring_5106_6243_totter_(118_1018).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_winter_5808_7585_totter_(438_1338).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_spring_2279_3416_totter_(118_1018).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_winter_7281_8371_totter_(95_995).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-2_cam01_drunken01_place03_night_spring_2255_3161_totter_(3_903).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam03_drunken01_place03_night_spring_4998_6499_totter_(300_1200).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_spring_526_3559_totter_(1066_1966).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_summer_6040_8183_totter_(621_1521).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_summer_462_4454_totter_(1546_2446).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_winter_421_4278_totter_(1478_2378).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_winter_413_2350_totter_(518_1418).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_winter_2378_3736_totter_(229_1129).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_winter_3994_5252_totter_(179_1079).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_summer_1841_2949_totter_(104_1004).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_spring_5000_6280_totter_(190_1090).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam03_drunken04_place03_night_spring_174_1098_totter_(12_912).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam03_drunken02_place01_night_summer_107_1221_totter_(107_1007).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-5_cam02_drunken04_place03_night_winter_222_1402_totter_(140_1040).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam01_drunken04_place03_night_winter_194_2250_totter_(578_1478).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-5_cam03_drunken04_place03_night_spring_2140_3078_totter_(19_919).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam03_drunken04_place03_night_winter_248_1762_totter_(307_1207).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam03_drunken02_place01_night_winter_122_1113_totter_(45_945).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam03_drunken02_place01_night_winter_1713_2835_totter_(111_1011).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam01_drunken02_place01_night_winter_1935_2973_totter_(69_969).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam02_drunken02_place01_night_winter_227_1241_totter_(57_957).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam01_drunken04_place03_night_summer_147_1942_totter_(447_1347).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam01_drunken02_place01_night_summer_264_1413_totter_(124_1024).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-4_cam01_drunken02_place01_night_spring_117_1033_totter_(8_908).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam02_drunken04_place03_night_summer_153_1969_totter_(458_1358).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam02_drunken04_place03_night_winter_168_2251_totter_(591_1491).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam02_drunken02_place01_night_summer_270_1447_totter_(138_1038).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam03_drunken04_place03_night_spring_4523_5987_totter_(282_1182).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam02_drunken04_place03_night_winter_297_1793_totter_(298_1198).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam01_drunken04_place03_night_winter_265_1768_totter_(301_1201).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam02_drunken02_place01_night_winter_1813_2977_totter_(132_1032).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam01_drunken02_place01_night_winter_321_1389_totter_(84_984).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam03_drunken04_place03_night_summer_155_1916_totter_(430_1330).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam02_drunken04_place03_night_spring_4527_5936_totter_(254_1154).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam03_drunken04_place03_night_summer_400_1810_totter_(255_1155).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam03_drunken01_place03_night_summer_289_2180_totter_(495_1395).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam01_drunken04_place03_night_summer_173_1204_totter_(65_965).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam01_drunken01_place03_night_spring_256_1162_totter_(3_903).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam03_drunken01_place03_night_spring_1440_3441_totter_(550_1450).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam03_drunken01_place03_night_winter_261_2243_totter_(541_1441).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam03_drunken01_place03_night_winter_2260_3383_totter_(111_1011).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam01_drunken01_place03_night_summer_495_2457_totter_(531_1431).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam02_drunken01_place03_night_winter_248_2231_totter_(541_1441).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam02_drunken04_place03_night_summer_122_1208_totter_(93_993).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-2_cam03_drunken04_place03_night_summer_112_1036_totter_(12_912).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam01_drunken01_place03_night_winter_1611_2920_totter_(204_1104).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam01_drunken01_place03_night_winter_234_2164_totter_(515_1415).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam01_drunken01_place03_night_spring_1333_3315_totter_(541_1441).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam01_drunken01_place03_night_spring_388_1488_totter_(100_1000).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam01_drunken01_place03_night_spring_451_1524_totter_(86_986).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam03_drunken01_place03_night_spring_293_1245_totter_(26_926).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam01_drunken01_place03_night_summer_1842_3078_totter_(168_1068).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam02_drunken01_place03_night_summer_151_1472_totter_(210_1110).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam02_drunken01_place03_night_summer_1704_3061_totter_(228_1128).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam02_drunken01_place03_night_winter_1776_2994_totter_(159_1059).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam02_drunken01_place03_night_spring_541_1523_totter_(41_941).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam03_drunken04_place03_night_summer_127_1218_totter_(95_995).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam03_drunken04_place03_night_winter_171_1180_totter_(54_954).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam02_drunken01_place03_night_spring_141_1497_totter_(228_1128).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam02_drunken01_place03_night_summer_204_2607_totter_(751_1651).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam02_drunken01_place03_night_summer_178_1550_totter_(236_1136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam01_drunken04_place03_night_winter_156_1119_totter_(31_931).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam01_drunken01_place03_night_summer_336_1535_totter_(149_1049).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/croki_30초/229-2_cam03_drunken04_place03_night_spring_93_1021_totter_(14_914).csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/croki_30초\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in over_900:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_spring_5038_6650_totter_(356_1256).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam02_drunken01_place03_night_spring_5038_6650_totter_(356_1256).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam01_drunken03_place03_night_winter_479_1427_totter_(24_924).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-4_cam01_drunken03_place03_night_winter_479_1427_totter_(24_924).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam01_drunken03_place02_night_spring_7893_9008_totter_(107_1007).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-2_cam01_drunken03_place02_night_spring_7893_9008_totter_(107_1007).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam03_drunken04_place03_night_summer_155_1916_totter_(430_1330).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-2_cam03_drunken04_place03_night_summer_155_1916_totter_(430_1330).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam02_drunken03_place03_night_winter_1597_3116_totter_(309_1209).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-4_cam02_drunken03_place03_night_winter_1597_3116_totter_(309_1209).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_summer_462_4454_totter_(1546_2446).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam02_drunken01_place03_night_summer_462_4454_totter_(1546_2446).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam02_drunken03_place03_night_spring_141_1493_totter_(226_1126).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam02_drunken03_place03_night_spring_141_1493_totter_(226_1126).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam02_drunken03_place02_night_spring_7390_8709_totter_(209_1109).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam02_drunken03_place02_night_spring_7390_8709_totter_(209_1109).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam03_drunken03_place03_night_spring_165_1183_totter_(59_959).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-2_cam03_drunken03_place03_night_spring_165_1183_totter_(59_959).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam01_drunken03_place03_night_summer_1639_2579_totter_(20_920).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-5_cam01_drunken03_place03_night_summer_1639_2579_totter_(20_920).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam01_drunken03_place02_night_winter_8030_8939_totter_(4_904).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-2_cam01_drunken03_place02_night_winter_8030_8939_totter_(4_904).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-2_cam02_drunken01_place03_night_spring_2247_3165_totter_(9_909).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-2_cam02_drunken01_place03_night_spring_2247_3165_totter_(9_909).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-4_cam01_drunken02_place01_night_spring_117_1033_totter_(8_908).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-4_cam01_drunken02_place01_night_spring_117_1033_totter_(8_908).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam01_drunken03_place03_night_winter_198_1608_totter_(255_1155).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-1_cam01_drunken03_place03_night_winter_198_1608_totter_(255_1155).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam03_drunken03_place02_night_winter_4353_5577_totter_(162_1062).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam03_drunken03_place02_night_winter_4353_5577_totter_(162_1062).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam03_drunken01_place03_night_winter_381_2313_totter_(516_1416).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam03_drunken01_place03_night_winter_381_2313_totter_(516_1416).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam03_drunken03_place03_night_winter_4195_5643_totter_(274_1174).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam03_drunken03_place03_night_winter_4195_5643_totter_(274_1174).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_winter_5718_6654_totter_(18_918).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam01_drunken01_place03_night_winter_5718_6654_totter_(18_918).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam02_drunken03_place03_night_winter_176_1512_totter_(218_1118).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-4_cam02_drunken03_place03_night_winter_176_1512_totter_(218_1118).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam02_drunken03_place03_night_winter_213_1876_totter_(381_1281).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-1_cam02_drunken03_place03_night_winter_213_1876_totter_(381_1281).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_summer_397_2910_totter_(806_1706).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam02_drunken01_place03_night_summer_397_2910_totter_(806_1706).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam02_drunken01_place03_night_summer_178_1550_totter_(236_1136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam02_drunken01_place03_night_summer_178_1550_totter_(236_1136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam01_drunken01_place03_night_winter_1611_2920_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam01_drunken01_place03_night_winter_1611_2920_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam01_drunken03_place02_night_winter_4408_5592_totter_(142_1042).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam01_drunken03_place02_night_winter_4408_5592_totter_(142_1042).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam03_drunken04_place03_night_spring_4523_5987_totter_(282_1182).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-6_cam03_drunken04_place03_night_spring_4523_5987_totter_(282_1182).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_spring_6703_7742_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam01_drunken04_place02_night_spring_6703_7742_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam02_drunken02_place01_night_winter_227_1241_totter_(57_957).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam02_drunken02_place01_night_winter_227_1241_totter_(57_957).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam03_drunken03_place03_night_winter_121_1588_totter_(283_1183).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam03_drunken03_place03_night_winter_121_1588_totter_(283_1183).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam01_drunken03_place03_night_winter_1645_2747_totter_(101_1001).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-5_cam01_drunken03_place03_night_winter_1645_2747_totter_(101_1001).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_winter_6973_8183_totter_(155_1055).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam03_drunken04_place02_night_winter_6973_8183_totter_(155_1055).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam03_drunken04_place03_night_summer_127_1218_totter_(95_995).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-5_cam03_drunken04_place03_night_summer_127_1218_totter_(95_995).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_summer_6579_8095_totter_(308_1208).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam02_drunken01_place03_night_summer_6579_8095_totter_(308_1208).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam03_drunken03_place03_night_summer_151_3017_totter_(983_1883).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam03_drunken03_place03_night_summer_151_3017_totter_(983_1883).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_spring_3440_5836_totter_(748_1648).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam02_drunken04_place02_night_spring_3440_5836_totter_(748_1648).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam01_drunken01_place03_night_summer_336_1535_totter_(149_1049).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam01_drunken01_place03_night_summer_336_1535_totter_(149_1049).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_winter_382_4237_totter_(1477_2377).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam01_drunken01_place03_night_winter_382_4237_totter_(1477_2377).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam02_drunken02_place01_night_summer_270_1447_totter_(138_1038).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam02_drunken02_place01_night_summer_270_1447_totter_(138_1038).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam02_drunken03_place03_night_winter_142_1602_totter_(280_1180).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam02_drunken03_place03_night_winter_142_1602_totter_(280_1180).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam01_drunken03_place03_night_summer_133_1529_totter_(248_1148).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-4_cam01_drunken03_place03_night_summer_133_1529_totter_(248_1148).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam01_drunken03_place02_night_summer_4699_5755_totter_(78_978).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam01_drunken03_place02_night_summer_4699_5755_totter_(78_978).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam01_drunken04_place03_night_summer_173_1204_totter_(65_965).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-5_cam01_drunken04_place03_night_summer_173_1204_totter_(65_965).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_winter_7033_8201_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam01_drunken04_place02_night_winter_7033_8201_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam02_drunken03_place02_night_winter_7950_8870_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-2_cam02_drunken03_place02_night_winter_7950_8870_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_summer_406_1537_totter_(115_1015).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam01_drunken01_place03_night_summer_406_1537_totter_(115_1015).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_winter_1589_3053_totter_(282_1182).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam03_drunken03_place02_night_winter_1589_3053_totter_(282_1182).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam03_drunken02_place01_night_winter_1713_2835_totter_(111_1011).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam03_drunken02_place01_night_winter_1713_2835_totter_(111_1011).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_winter_4115_5522_totter_(253_1153).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam01_drunken03_place03_night_winter_4115_5522_totter_(253_1153).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_summer_4043_6527_totter_(792_1692).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam01_drunken04_place02_night_summer_4043_6527_totter_(792_1692).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam02_drunken04_place03_night_winter_297_1793_totter_(298_1198).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-2_cam02_drunken04_place03_night_winter_297_1793_totter_(298_1198).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_winter_7281_8371_totter_(95_995).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam01_drunken01_place03_night_winter_7281_8371_totter_(95_995).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_winter_8296_9225_totter_(14_914).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam02_drunken03_place02_night_winter_8296_9225_totter_(14_914).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-2_cam01_drunken01_place03_night_spring_2255_3161_totter_(3_903).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-2_cam01_drunken01_place03_night_spring_2255_3161_totter_(3_903).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_winter_3778_6383_totter_(852_1752).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam02_drunken04_place02_night_winter_3778_6383_totter_(852_1752).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam01_drunken01_place03_night_spring_388_1488_totter_(100_1000).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam01_drunken01_place03_night_spring_388_1488_totter_(100_1000).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam02_drunken01_place03_night_spring_141_1497_totter_(228_1128).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam02_drunken01_place03_night_spring_141_1497_totter_(228_1128).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_winter_4418_5531_totter_(106_1006).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam02_drunken03_place02_night_winter_4418_5531_totter_(106_1006).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_winter_141_3004_totter_(981_1881).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam02_drunken03_place03_night_winter_141_3004_totter_(981_1881).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam01_drunken03_place02_night_spring_7460_8736_totter_(188_1088).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam01_drunken03_place02_night_spring_7460_8736_totter_(188_1088).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam02_drunken01_place03_night_summer_204_2607_totter_(751_1651).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam02_drunken01_place03_night_summer_204_2607_totter_(751_1651).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam03_drunken03_place03_night_summer_117_1029_totter_(6_906).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam03_drunken03_place03_night_summer_117_1029_totter_(6_906).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam02_drunken03_place03_night_summer_1995_3379_totter_(242_1142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-2_cam02_drunken03_place03_night_summer_1995_3379_totter_(242_1142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam03_drunken03_place03_night_summer_156_1626_totter_(285_1185).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-4_cam03_drunken03_place03_night_summer_156_1626_totter_(285_1185).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_spring_3377_5818_totter_(770_1670).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam01_drunken04_place02_night_spring_3377_5818_totter_(770_1670).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam02_drunken01_place03_night_winter_248_2231_totter_(541_1441).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam02_drunken01_place03_night_winter_248_2231_totter_(541_1441).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_winter_421_4278_totter_(1478_2378).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam02_drunken01_place03_night_winter_421_4278_totter_(1478_2378).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_winter_436_2936_totter_(800_1700).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam01_drunken03_place03_night_winter_436_2936_totter_(800_1700).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_spring_6819_8189_totter_(235_1135).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam02_drunken01_place03_night_spring_6819_8189_totter_(235_1135).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam01_drunken03_place03_night_spring_131_1922_totter_(445_1345).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam01_drunken03_place03_night_spring_131_1922_totter_(445_1345).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam01_drunken01_place03_night_winter_234_2164_totter_(515_1415).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam01_drunken01_place03_night_winter_234_2164_totter_(515_1415).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam01_drunken01_place03_night_spring_6823_8222_totter_(249_1149).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam01_drunken01_place03_night_spring_6823_8222_totter_(249_1149).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_winter_5808_7585_totter_(438_1338).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam03_drunken01_place03_night_winter_5808_7585_totter_(438_1338).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-3_cam01_drunken03_place02_night_summer_1205_2292_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-3_cam01_drunken03_place02_night_summer_1205_2292_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam03_drunken03_place03_night_summer_1957_3339_totter_(241_1141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-2_cam03_drunken03_place03_night_summer_1957_3339_totter_(241_1141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam03_drunken03_place03_night_winter_162_1576_totter_(257_1157).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-1_cam03_drunken03_place03_night_winter_162_1576_totter_(257_1157).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam03_drunken03_place03_night_winter_481_4532_totter_(1575_2475).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-2_cam03_drunken03_place03_night_winter_481_4532_totter_(1575_2475).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-2_cam03_drunken04_place03_night_spring_93_1021_totter_(14_914).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-2_cam03_drunken04_place03_night_spring_93_1021_totter_(14_914).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_summer_1841_2949_totter_(104_1004).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam01_drunken01_place03_night_summer_1841_2949_totter_(104_1004).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam02_drunken04_place03_night_summer_122_1208_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-5_cam02_drunken04_place03_night_summer_122_1208_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam01_drunken01_place03_night_spring_256_1162_totter_(3_903).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam01_drunken01_place03_night_spring_256_1162_totter_(3_903).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_spring_5000_6280_totter_(190_1090).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam02_drunken01_place03_night_spring_5000_6280_totter_(190_1090).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_summer_4111_5095_totter_(42_942).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam01_drunken03_place03_night_summer_4111_5095_totter_(42_942).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-5_cam02_drunken04_place03_night_winter_222_1402_totter_(140_1040).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-5_cam02_drunken04_place03_night_winter_222_1402_totter_(140_1040).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam01_drunken04_place03_night_winter_265_1768_totter_(301_1201).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-2_cam01_drunken04_place03_night_winter_265_1768_totter_(301_1201).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_spring_4992_6262_totter_(185_1085).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam03_drunken01_place03_night_spring_4992_6262_totter_(185_1085).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam02_drunken01_place03_night_summer_151_1472_totter_(210_1110).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam02_drunken01_place03_night_summer_151_1472_totter_(210_1110).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_summer_7625_8630_totter_(52_952).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam03_drunken03_place02_night_summer_7625_8630_totter_(52_952).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam02_drunken01_place03_night_winter_5791_7513_totter_(411_1311).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam02_drunken01_place03_night_winter_5791_7513_totter_(411_1311).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam02_drunken04_place03_night_winter_168_2251_totter_(591_1491).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-6_cam02_drunken04_place03_night_winter_168_2251_totter_(591_1491).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam03_drunken03_place02_night_summer_4668_5697_totter_(64_964).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam03_drunken03_place02_night_summer_4668_5697_totter_(64_964).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam03_drunken04_place03_night_winter_171_1180_totter_(54_954).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-5_cam03_drunken04_place03_night_winter_171_1180_totter_(54_954).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-2_cam03_drunken01_place03_night_spring_2266_3496_totter_(165_1065).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-2_cam03_drunken01_place03_night_spring_2266_3496_totter_(165_1065).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_spring_2261_3878_totter_(358_1258).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam02_drunken01_place03_night_spring_2261_3878_totter_(358_1258).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_spring_4613_6061_totter_(274_1174).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam02_drunken03_place03_night_spring_4613_6061_totter_(274_1174).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam03_drunken02_place01_night_summer_107_1221_totter_(107_1007).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam03_drunken02_place01_night_summer_107_1221_totter_(107_1007).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam02_drunken03_place02_night_spring_7694_8961_totter_(183_1083).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-2_cam02_drunken03_place02_night_spring_7694_8961_totter_(183_1083).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam03_drunken02_place01_night_winter_122_1113_totter_(45_945).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam03_drunken02_place01_night_winter_122_1113_totter_(45_945).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_summer_4693_5799_totter_(103_1003).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam02_drunken03_place02_night_summer_4693_5799_totter_(103_1003).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam01_drunken03_place02_night_winter_1681_3116_totter_(267_1167).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam01_drunken03_place02_night_winter_1681_3116_totter_(267_1167).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-5_cam01_drunken04_place03_night_winter_156_1119_totter_(31_931).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-5_cam01_drunken04_place03_night_winter_156_1119_totter_(31_931).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam02_drunken04_place02_night_summer_3827_5457_totter_(365_1265).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/623-2_cam02_drunken04_place02_night_summer_3827_5457_totter_(365_1265).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_spring_332_3141_totter_(954_1854).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam01_drunken03_place03_night_spring_332_3141_totter_(954_1854).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_summer_7269_8601_totter_(216_1116).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam03_drunken04_place02_night_summer_7269_8601_totter_(216_1116).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-5_cam03_drunken04_place03_night_spring_2140_3078_totter_(19_919).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-5_cam03_drunken04_place03_night_spring_2140_3078_totter_(19_919).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_summer_442_2906_totter_(782_1682).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam01_drunken03_place03_night_summer_442_2906_totter_(782_1682).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_summer_4001_6485_totter_(792_1692).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam02_drunken04_place02_night_summer_4001_6485_totter_(792_1692).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-3_cam01_drunken03_place02_night_winter_7568_8475_totter_(3_903).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-3_cam01_drunken03_place02_night_winter_7568_8475_totter_(3_903).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam02_drunken03_place02_night_summer_7436_8740_totter_(202_1102).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam02_drunken03_place02_night_summer_7436_8740_totter_(202_1102).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-1_cam02_drunken03_place03_night_winter_3063_4354_totter_(195_1095).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-1_cam02_drunken03_place03_night_winter_3063_4354_totter_(195_1095).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam03_drunken04_place03_night_summer_400_1810_totter_(255_1155).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-6_cam03_drunken04_place03_night_summer_400_1810_totter_(255_1155).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_winter_7296_8365_totter_(84_984).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam03_drunken01_place03_night_winter_7296_8365_totter_(84_984).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam03_drunken03_place03_night_winter_140_2996_totter_(978_1878).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam03_drunken03_place03_night_winter_140_2996_totter_(978_1878).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam02_drunken01_place03_night_winter_413_2350_totter_(518_1418).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam02_drunken01_place03_night_winter_413_2350_totter_(518_1418).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_spring_173_3282_totter_(1104_2004).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam02_drunken03_place03_night_spring_173_3282_totter_(1104_2004).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam03_drunken03_place03_night_spring_96_1488_totter_(246_1146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam03_drunken03_place03_night_spring_96_1488_totter_(246_1146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam01_drunken01_place03_night_spring_451_1524_totter_(86_986).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam01_drunken01_place03_night_spring_451_1524_totter_(86_986).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_summer_3995_6473_totter_(789_1689).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam03_drunken04_place02_night_summer_3995_6473_totter_(789_1689).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_spring_2279_3416_totter_(118_1018).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam01_drunken01_place03_night_spring_2279_3416_totter_(118_1018).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam01_drunken04_place03_night_summer_147_1942_totter_(447_1347).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-2_cam01_drunken04_place03_night_summer_147_1942_totter_(447_1347).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam01_drunken01_place03_night_winter_2378_3736_totter_(229_1129).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam01_drunken01_place03_night_winter_2378_3736_totter_(229_1129).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam02_drunken01_place03_night_winter_1776_2994_totter_(159_1059).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam02_drunken01_place03_night_winter_1776_2994_totter_(159_1059).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam03_drunken04_place03_night_spring_174_1098_totter_(12_912).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-6_cam03_drunken04_place03_night_spring_174_1098_totter_(12_912).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_summer_189_3078_totter_(994_1894).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam02_drunken03_place03_night_summer_189_3078_totter_(994_1894).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-5_cam03_drunken01_place03_night_spring_4998_6499_totter_(300_1200).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-5_cam03_drunken01_place03_night_spring_4998_6499_totter_(300_1200).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam01_drunken04_place03_night_winter_194_2250_totter_(578_1478).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-6_cam01_drunken04_place03_night_winter_194_2250_totter_(578_1478).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam02_drunken04_place03_night_summer_153_1969_totter_(458_1358).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-2_cam02_drunken04_place03_night_summer_153_1969_totter_(458_1358).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam03_drunken03_place02_night_spring_3786_4969_totter_(141_1041).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam03_drunken03_place02_night_spring_3786_4969_totter_(141_1041).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_spring_6701_7707_totter_(53_953).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam03_drunken04_place02_night_spring_6701_7707_totter_(53_953).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam01_drunken03_place02_night_summer_1227_2149_totter_(11_911).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam01_drunken03_place02_night_summer_1227_2149_totter_(11_911).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam02_drunken02_place01_night_winter_1813_2977_totter_(132_1032).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam02_drunken02_place01_night_winter_1813_2977_totter_(132_1032).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam01_drunken01_place03_night_spring_4990_6291_totter_(200_1100).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam01_drunken01_place03_night_spring_4990_6291_totter_(200_1100).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_winter_4185_5658_totter_(286_1186).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam02_drunken03_place03_night_winter_4185_5658_totter_(286_1186).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam02_drunken01_place03_night_spring_541_1523_totter_(41_941).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam02_drunken01_place03_night_spring_541_1523_totter_(41_941).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_winter_5752_6663_totter_(5_905).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam03_drunken01_place03_night_winter_5752_6663_totter_(5_905).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_spring_455_3546_totter_(1095_1995).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam02_drunken01_place03_night_spring_455_3546_totter_(1095_1995).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam01_drunken01_place03_night_spring_1333_3315_totter_(541_1441).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam01_drunken01_place03_night_spring_1333_3315_totter_(541_1441).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam01_drunken03_place03_night_winter_2716_3841_totter_(112_1012).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-2_cam01_drunken03_place03_night_winter_2716_3841_totter_(112_1012).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_spring_6807_8169_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam03_drunken01_place03_night_spring_6807_8169_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/229-2_cam03_drunken04_place03_night_summer_112_1036_totter_(12_912).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/229-2_cam03_drunken04_place03_night_summer_112_1036_totter_(12_912).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_winter_3804_6397_totter_(846_1746).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam03_drunken04_place02_night_winter_3804_6397_totter_(846_1746).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam01_drunken01_place03_night_summer_1842_3078_totter_(168_1068).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam01_drunken01_place03_night_summer_1842_3078_totter_(168_1068).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_spring_6577_7747_totter_(135_1035).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam02_drunken04_place02_night_spring_6577_7747_totter_(135_1035).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam03_drunken03_place03_night_winter_2733_3932_totter_(149_1049).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-2_cam03_drunken03_place03_night_winter_2733_3932_totter_(149_1049).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam02_drunken03_place02_night_winter_1594_3038_totter_(272_1172).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam02_drunken03_place02_night_winter_1594_3038_totter_(272_1172).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam02_drunken01_place03_night_summer_1704_3061_totter_(228_1128).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam02_drunken01_place03_night_summer_1704_3061_totter_(228_1128).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-5_cam03_drunken01_place03_night_summer_289_2180_totter_(495_1395).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-5_cam03_drunken01_place03_night_summer_289_2180_totter_(495_1395).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-2_cam03_drunken04_place03_night_winter_248_1762_totter_(307_1207).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-2_cam03_drunken04_place03_night_winter_248_1762_totter_(307_1207).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_summer_7295_8596_totter_(200_1100).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam02_drunken04_place02_night_summer_7295_8596_totter_(200_1100).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_spring_5106_6243_totter_(118_1018).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam02_drunken01_place03_night_spring_5106_6243_totter_(118_1018).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_spring_2435_3361_totter_(13_913).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam03_drunken03_place02_night_spring_2435_3361_totter_(13_913).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-4_cam03_drunken03_place02_night_spring_7371_8417_totter_(73_973).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-4_cam03_drunken03_place02_night_spring_7371_8417_totter_(73_973).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_spring_526_3559_totter_(1066_1966).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam03_drunken01_place03_night_spring_526_3559_totter_(1066_1966).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam03_drunken03_place03_night_summer_1644_2608_totter_(32_932).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-5_cam03_drunken03_place03_night_summer_1644_2608_totter_(32_932).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam01_drunken01_place03_night_spring_394_3564_totter_(1135_2035).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam01_drunken01_place03_night_spring_394_3564_totter_(1135_2035).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam01_drunken02_place01_night_winter_321_1389_totter_(84_984).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam01_drunken02_place01_night_winter_321_1389_totter_(84_984).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam01_drunken02_place01_night_winter_1935_2973_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam01_drunken02_place01_night_winter_1935_2973_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-6_cam01_drunken03_place03_night_winter_492_1623_totter_(115_1015).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-6_cam01_drunken03_place03_night_winter_492_1623_totter_(115_1015).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-2_cam01_drunken03_place03_night_summer_1136_4740_totter_(1352_2252).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-2_cam01_drunken03_place03_night_summer_1136_4740_totter_(1352_2252).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-1_cam03_drunken01_place03_night_winter_3994_5252_totter_(179_1079).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-1_cam03_drunken01_place03_night_winter_3994_5252_totter_(179_1079).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam02_drunken03_place03_night_summer_4248_5174_totter_(13_913).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam02_drunken03_place03_night_summer_4248_5174_totter_(13_913).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam03_drunken01_place03_night_spring_293_1245_totter_(26_926).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam03_drunken01_place03_night_spring_293_1245_totter_(26_926).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam03_drunken01_place03_night_summer_576_4476_totter_(1500_2400).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam03_drunken01_place03_night_summer_576_4476_totter_(1500_2400).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam03_drunken03_place03_night_winter_1622_2757_totter_(117_1017).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-5_cam03_drunken03_place03_night_winter_1622_2757_totter_(117_1017).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam01_drunken01_place03_night_summer_495_2457_totter_(531_1431).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam01_drunken01_place03_night_summer_495_2457_totter_(531_1431).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_summer_7333_8646_totter_(206_1106).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam01_drunken04_place02_night_summer_7333_8646_totter_(206_1106).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/217-6_cam02_drunken04_place03_night_spring_4527_5936_totter_(254_1154).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/217-6_cam02_drunken04_place03_night_spring_4527_5936_totter_(254_1154).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/216-1_cam01_drunken02_place01_night_summer_264_1413_totter_(124_1024).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/216-1_cam01_drunken02_place01_night_summer_264_1413_totter_(124_1024).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-3_cam01_drunken03_place03_night_spring_4528_5885_totter_(228_1128).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-3_cam01_drunken03_place03_night_spring_4528_5885_totter_(228_1128).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam01_drunken04_place02_night_spring_3310_4503_totter_(146_1046).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/623-2_cam01_drunken04_place02_night_spring_3310_4503_totter_(146_1046).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam01_drunken04_place02_night_winter_3801_6423_totter_(861_1761).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam01_drunken04_place02_night_winter_3801_6423_totter_(861_1761).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam02_drunken03_place03_night_winter_472_4543_totter_(1585_2485).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-2_cam02_drunken03_place03_night_winter_472_4543_totter_(1585_2485).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/278-4_cam01_drunken03_place03_night_winter_1478_3040_totter_(331_1231).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/278-4_cam01_drunken03_place03_night_winter_1478_3040_totter_(331_1231).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-2_cam03_drunken03_place02_night_spring_7702_8943_totter_(170_1070).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-2_cam03_drunken03_place02_night_spring_7702_8943_totter_(170_1070).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam02_drunken03_place03_night_spring_1378_2664_totter_(193_1093).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-2_cam02_drunken03_place03_night_spring_1378_2664_totter_(193_1093).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/209-3_cam02_drunken01_place03_night_summer_6040_8183_totter_(621_1521).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/209-3_cam02_drunken01_place03_night_summer_6040_8183_totter_(621_1521).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam03_drunken04_place02_night_spring_3372_5824_totter_(776_1676).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam03_drunken04_place02_night_spring_3372_5824_totter_(776_1676).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam03_drunken01_place03_night_winter_2260_3383_totter_(111_1011).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam03_drunken01_place03_night_winter_2260_3383_totter_(111_1011).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/233-6_cam02_drunken03_place02_night_spring_3793_4965_totter_(136_1036).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/233-6_cam02_drunken03_place02_night_spring_3793_4965_totter_(136_1036).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/214-2_cam02_drunken04_place02_night_winter_6974_8224_totter_(175_1075).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/214-2_cam02_drunken04_place02_night_winter_6974_8224_totter_(175_1075).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-2_cam03_drunken01_place03_night_spring_1440_3441_totter_(550_1450).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-2_cam03_drunken01_place03_night_spring_1440_3441_totter_(550_1450).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam02_drunken03_place03_night_winter_1634_2751_totter_(108_1008).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-5_cam02_drunken03_place03_night_winter_1634_2751_totter_(108_1008).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/218-4_cam03_drunken01_place03_night_winter_261_2243_totter_(541_1441).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/218-4_cam03_drunken01_place03_night_winter_261_2243_totter_(541_1441).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-2_cam01_drunken03_place03_night_winter_473_4519_totter_(1573_2473).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-2_cam01_drunken03_place03_night_winter_473_4519_totter_(1573_2473).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam01_drunken04_place02_night_summer_3701_5505_totter_(452_1352).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/croki_30초/623-2_cam01_drunken04_place02_night_summer_3701_5505_totter_(452_1352).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/croki_30초/212-5_cam02_drunken03_place03_night_summer_1671_2626_totter_(27_927).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/croki_0넣기/212-5_cam02_drunken03_place03_night_summer_1671_2626_totter_(27_927).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/croki_30초'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/croki_0넣기'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 및 'frame' 열 확인\n",
    "    if 'label' not in df.columns or 'frame' not in df.columns:\n",
    "        print(f\"'label' 또는 'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 데이터프레임을 'label' 열로 그룹화\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        frames_to_add = []  # 새로운 프레임 데이터 저장 리스트\n",
    "\n",
    "        # 10프레임 간격으로 선택\n",
    "        for frame in range(min_frame, max_frame + 1, 10):\n",
    "            # 선택한 프레임이 존재하는 경우 그대로 추가\n",
    "            if frame in group['frame'].values:\n",
    "                frames_to_add.append(group[group['frame'] == frame].iloc[0].to_dict())\n",
    "            else:\n",
    "                # 대체 프레임 탐색: 우선적으로 다음 프레임, 그다음 이전 프레임\n",
    "                replacement_frame = None\n",
    "                if (frame + 1) in group['frame'].values:\n",
    "                    replacement_frame = frame + 1\n",
    "                elif (frame - 1) in group['frame'].values:\n",
    "                    replacement_frame = frame - 1\n",
    "\n",
    "                if replacement_frame:\n",
    "                    frames_to_add.append(group[group['frame'] == replacement_frame].iloc[0].to_dict())\n",
    "                else:\n",
    "                    # 대체 프레임도 없으면 0으로 채운 데이터 추가\n",
    "                    empty_row = {col: 0 for col in group.columns if col != 'label'}\n",
    "                    empty_row['label'] = label\n",
    "                    empty_row['frame'] = frame\n",
    "                    frames_to_add.append(empty_row)\n",
    "\n",
    "        # 생성된 프레임 리스트를 데이터프레임으로 변환\n",
    "        new_df = pd.DataFrame(frames_to_add)\n",
    "        processed_dfs.append(new_df)\n",
    "\n",
    "    # 그룹별 처리된 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>437</td>\n",
       "      <td>145</td>\n",
       "      <td>402</td>\n",
       "      <td>122</td>\n",
       "      <td>553</td>\n",
       "      <td>166</td>\n",
       "      <td>511</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>428</td>\n",
       "      <td>181</td>\n",
       "      <td>376</td>\n",
       "      <td>167</td>\n",
       "      <td>543</td>\n",
       "      <td>217</td>\n",
       "      <td>483</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>69</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>423</td>\n",
       "      <td>202</td>\n",
       "      <td>366</td>\n",
       "      <td>198</td>\n",
       "      <td>538</td>\n",
       "      <td>236</td>\n",
       "      <td>477</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>415</td>\n",
       "      <td>196</td>\n",
       "      <td>366</td>\n",
       "      <td>199</td>\n",
       "      <td>528</td>\n",
       "      <td>231</td>\n",
       "      <td>478</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>402</td>\n",
       "      <td>187</td>\n",
       "      <td>371</td>\n",
       "      <td>197</td>\n",
       "      <td>515</td>\n",
       "      <td>213</td>\n",
       "      <td>492</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34575</th>\n",
       "      <td>887</td>\n",
       "      <td>65</td>\n",
       "      <td>91</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>510</td>\n",
       "      <td>83</td>\n",
       "      <td>480</td>\n",
       "      <td>210</td>\n",
       "      <td>651</td>\n",
       "      <td>95</td>\n",
       "      <td>615</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34576</th>\n",
       "      <td>897</td>\n",
       "      <td>42</td>\n",
       "      <td>94</td>\n",
       "      <td>44</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>64</td>\n",
       "      <td>490</td>\n",
       "      <td>146</td>\n",
       "      <td>653</td>\n",
       "      <td>111</td>\n",
       "      <td>601</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34577</th>\n",
       "      <td>907</td>\n",
       "      <td>41</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>501</td>\n",
       "      <td>104</td>\n",
       "      <td>454</td>\n",
       "      <td>77</td>\n",
       "      <td>681</td>\n",
       "      <td>144</td>\n",
       "      <td>567</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34578</th>\n",
       "      <td>917</td>\n",
       "      <td>26</td>\n",
       "      <td>99</td>\n",
       "      <td>32</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>509</td>\n",
       "      <td>72</td>\n",
       "      <td>466</td>\n",
       "      <td>95</td>\n",
       "      <td>657</td>\n",
       "      <td>121</td>\n",
       "      <td>590</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>927</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>16</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>503</td>\n",
       "      <td>61</td>\n",
       "      <td>480</td>\n",
       "      <td>138</td>\n",
       "      <td>635</td>\n",
       "      <td>110</td>\n",
       "      <td>601</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34580 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame  x1  y1  x2  y2  x3  y3   x4  y4   x5  ...  y14  x15  y15  x16  \\\n",
       "0        356   0   0   0   0   0   0   57  59    0  ...  437  145  402  122   \n",
       "1        366   0   0   0   0   0   0   79  65  135  ...  428  181  376  167   \n",
       "2        376   0   0   0   0   0   0   84  69  151  ...  423  202  366  198   \n",
       "3        386   0   0   0   0   0   0   73  71  142  ...  415  196  366  199   \n",
       "4        396   0   0   0   0   0   0   66  67  134  ...  402  187  371  197   \n",
       "...      ...  ..  ..  ..  ..  ..  ..  ...  ..  ...  ...  ...  ...  ...  ...   \n",
       "34575    887  65  91  69  76   0   0  114  74    0  ...  510   83  480  210   \n",
       "34576    897  42  94  44  79   0   0   89  73    0  ...  512   64  490  146   \n",
       "34577    907  41  97  45  82   0   0   94  81    0  ...  501  104  454   77   \n",
       "34578    917  26  99  32  85   0   0   83  88    0  ...  509   72  466   95   \n",
       "34579    927   8  98  16  86   0   0   69  92    0  ...  503   61  480  138   \n",
       "\n",
       "       y16  x17  y17       label  y  \\\n",
       "0      553  166  511  ID: 2613.0  1   \n",
       "1      543  217  483  ID: 2613.0  1   \n",
       "2      538  236  477  ID: 2613.0  1   \n",
       "3      528  231  478  ID: 2613.0  1   \n",
       "4      515  213  492  ID: 2613.0  1   \n",
       "...    ...  ...  ...         ... ..   \n",
       "34575  651   95  615  ID: 1416.0  1   \n",
       "34576  653  111  601  ID: 1416.0  1   \n",
       "34577  681  144  567  ID: 1416.0  1   \n",
       "34578  657  121  590  ID: 1416.0  1   \n",
       "34579  635  110  601  ID: 1416.0  1   \n",
       "\n",
       "                                                FILENAME  \n",
       "0      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "1      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "2      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "3      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "4      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "...                                                  ...  \n",
       "34575  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34576  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34577  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34578  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34579  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "\n",
       "[34580 rows x 38 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "croki_data = pd.DataFrame()\n",
    "\n",
    "for vid in os.listdir('/home/alpaco/project/drunk_prj/data/croki_0넣기'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/croki_0넣기',vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 1\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    croki_data = pd.concat([croki_data,tmp_csv],ignore_index=True)\n",
    "croki_data\n",
    "#34580줄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102370</th>\n",
       "      <td>142</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>19394.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102371</th>\n",
       "      <td>143</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>142.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>19394.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102372</th>\n",
       "      <td>144</td>\n",
       "      <td>48.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>144.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19394.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102373</th>\n",
       "      <td>145</td>\n",
       "      <td>55.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>19394.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102374</th>\n",
       "      <td>146</td>\n",
       "      <td>61.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>145.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19394.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102375 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frame    x1    y1    x2    y2    x3    y3   x4   y4    x5  ...    y14  \\\n",
       "0          53   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0  ...    0.0   \n",
       "1          54   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0  ...    0.0   \n",
       "2          55   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0  ...    0.0   \n",
       "3          56   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0  ...    0.0   \n",
       "4          57   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0  ...    0.0   \n",
       "...       ...   ...   ...   ...   ...   ...   ...  ...  ...   ...  ...    ...   \n",
       "102370    142  38.0  27.0  40.0  23.0  34.0  24.0  0.0  0.0  26.0  ...  127.0   \n",
       "102371    143  46.0  28.0  48.0  24.0  41.0  25.0  0.0  0.0  33.0  ...  142.0   \n",
       "102372    144  48.0  30.0  50.0  26.0  43.0  26.0  0.0  0.0  33.0  ...  144.0   \n",
       "102373    145  55.0  29.0  57.0  26.0  50.0  26.0  0.0  0.0  39.0  ...  140.0   \n",
       "102374    146  61.0  28.0  62.0  24.0  55.0  25.0  0.0  0.0  44.0  ...  145.0   \n",
       "\n",
       "         x15    y15   x16    y16   x17    y17    label  \\\n",
       "0        0.0    0.0   0.0    0.0   0.0    0.0   1585.0   \n",
       "1        0.0    0.0   0.0    0.0   0.0    0.0   1585.0   \n",
       "2        0.0    0.0   0.0    0.0   0.0    0.0   1585.0   \n",
       "3        0.0    0.0   0.0    0.0   0.0    0.0   1585.0   \n",
       "4        0.0    0.0   0.0    0.0   0.0    0.0   1585.0   \n",
       "...      ...    ...   ...    ...   ...    ...      ...   \n",
       "102370  55.0  132.0  67.0  157.0  65.0  161.0  19394.0   \n",
       "102371  53.0  146.0   0.0    0.0  61.0  173.0  19394.0   \n",
       "102372  69.0  149.0   0.0    0.0   0.0    0.0  19394.0   \n",
       "102373  57.0  147.0  89.0  174.0  71.0  181.0  19394.0   \n",
       "102374  58.0  150.0  91.0  184.0  60.0  188.0  19394.0   \n",
       "\n",
       "                                               FILENAME  y  \n",
       "0       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "1       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "2       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "3       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "4       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "...                                                 ... ..  \n",
       "102370  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  0  \n",
       "102371  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  0  \n",
       "102372  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  0  \n",
       "102373  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  0  \n",
       "102374  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  0  \n",
       "\n",
       "[102375 rows x 38 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "normal_data = pd.DataFrame()\n",
    "\n",
    "for vid in os.listdir('/home/alpaco/project/drunk_prj/data/normal_0넣기'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/normal_0넣기',vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 0\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    normal_data = pd.concat([normal_data,tmp_csv],ignore_index=True)\n",
    "normal_data\n",
    "#102375 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136950</th>\n",
       "      <td>887</td>\n",
       "      <td>65.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>510.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136951</th>\n",
       "      <td>897</td>\n",
       "      <td>42.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>512.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136952</th>\n",
       "      <td>907</td>\n",
       "      <td>41.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>501.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>681.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136953</th>\n",
       "      <td>917</td>\n",
       "      <td>26.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>509.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136954</th>\n",
       "      <td>927</td>\n",
       "      <td>8.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>503.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136955 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frame    x1    y1    x2    y2   x3   y3     x4    y4   x5  ...    y14  \\\n",
       "0          53   0.0   0.0   0.0   0.0  0.0  0.0    0.0   0.0  0.0  ...    0.0   \n",
       "1          54   0.0   0.0   0.0   0.0  0.0  0.0    0.0   0.0  0.0  ...    0.0   \n",
       "2          55   0.0   0.0   0.0   0.0  0.0  0.0    0.0   0.0  0.0  ...    0.0   \n",
       "3          56   0.0   0.0   0.0   0.0  0.0  0.0    0.0   0.0  0.0  ...    0.0   \n",
       "4          57   0.0   0.0   0.0   0.0  0.0  0.0    0.0   0.0  0.0  ...    0.0   \n",
       "...       ...   ...   ...   ...   ...  ...  ...    ...   ...  ...  ...    ...   \n",
       "136950    887  65.0  91.0  69.0  76.0  0.0  0.0  114.0  74.0  0.0  ...  510.0   \n",
       "136951    897  42.0  94.0  44.0  79.0  0.0  0.0   89.0  73.0  0.0  ...  512.0   \n",
       "136952    907  41.0  97.0  45.0  82.0  0.0  0.0   94.0  81.0  0.0  ...  501.0   \n",
       "136953    917  26.0  99.0  32.0  85.0  0.0  0.0   83.0  88.0  0.0  ...  509.0   \n",
       "136954    927   8.0  98.0  16.0  86.0  0.0  0.0   69.0  92.0  0.0  ...  503.0   \n",
       "\n",
       "          x15    y15    x16    y16    x17    y17       label  \\\n",
       "0         0.0    0.0    0.0    0.0    0.0    0.0      1585.0   \n",
       "1         0.0    0.0    0.0    0.0    0.0    0.0      1585.0   \n",
       "2         0.0    0.0    0.0    0.0    0.0    0.0      1585.0   \n",
       "3         0.0    0.0    0.0    0.0    0.0    0.0      1585.0   \n",
       "4         0.0    0.0    0.0    0.0    0.0    0.0      1585.0   \n",
       "...       ...    ...    ...    ...    ...    ...         ...   \n",
       "136950   83.0  480.0  210.0  651.0   95.0  615.0  ID: 1416.0   \n",
       "136951   64.0  490.0  146.0  653.0  111.0  601.0  ID: 1416.0   \n",
       "136952  104.0  454.0   77.0  681.0  144.0  567.0  ID: 1416.0   \n",
       "136953   72.0  466.0   95.0  657.0  121.0  590.0  ID: 1416.0   \n",
       "136954   61.0  480.0  138.0  635.0  110.0  601.0  ID: 1416.0   \n",
       "\n",
       "                                                 FILENAME  y  \n",
       "0         C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "1         C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "2         C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "3         C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "4         C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  0  \n",
       "...                                                   ... ..  \n",
       "136950  212-5_cam02_drunken03_place03_night_summer_167...  1  \n",
       "136951  212-5_cam02_drunken03_place03_night_summer_167...  1  \n",
       "136952  212-5_cam02_drunken03_place03_night_summer_167...  1  \n",
       "136953  212-5_cam02_drunken03_place03_night_summer_167...  1  \n",
       "136954  212-5_cam02_drunken03_place03_night_summer_167...  1  \n",
       "\n",
       "[136955 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Combined = pd.concat([normal_data,croki_data],ignore_index=True)\n",
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = Combined.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(Combined, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178420, 90, 34)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 76/76 [00:00<00:00, 120.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.3750, F1 Score: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 76/76 [00:00<00:00, 188.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.3685, F1 Score: 0.7742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 76/76 [00:00<00:00, 289.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.2999, F1 Score: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 76/76 [00:00<00:00, 195.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.2928, F1 Score: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 76/76 [00:00<00:00, 272.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.2585, F1 Score: 0.7465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 76/76 [00:00<00:00, 217.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.2233, F1 Score: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 76/76 [00:00<00:00, 224.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.1949, F1 Score: 0.6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 76/76 [00:00<00:00, 218.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.1769, F1 Score: 0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 76/76 [00:00<00:00, 186.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.1563, F1 Score: 0.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 76/76 [00:00<00:00, 278.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.1268, F1 Score: 0.8358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 76/76 [00:00<00:00, 176.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.1227, F1 Score: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 76/76 [00:00<00:00, 264.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.1270, F1 Score: 0.8784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 76/76 [00:00<00:00, 189.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 0.1188, F1 Score: 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 76/76 [00:00<00:00, 209.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Loss: 0.1129, F1 Score: 0.8780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 76/76 [00:00<00:00, 216.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 0.1136, F1 Score: 0.8054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 76/76 [00:00<00:00, 171.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Loss: 0.1178, F1 Score: 0.8774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 76/76 [00:00<00:00, 182.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Loss: 0.1112, F1 Score: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 76/76 [00:00<00:00, 219.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Loss: 0.0999, F1 Score: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 76/76 [00:00<00:00, 175.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Loss: 0.0886, F1 Score: 0.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 76/76 [00:00<00:00, 279.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 0.0905, F1 Score: 0.9116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 76/76 [00:00<00:00, 177.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Loss: 0.0747, F1 Score: 0.9078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 76/76 [00:00<00:00, 277.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Loss: 0.0700, F1 Score: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 76/76 [00:00<00:00, 190.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Loss: 0.0642, F1 Score: 0.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 76/76 [00:00<00:00, 216.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Loss: 0.1512, F1 Score: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 76/76 [00:00<00:00, 239.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 0.1431, F1 Score: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 76/76 [00:00<00:00, 179.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Loss: 0.1076, F1 Score: 0.9067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 76/76 [00:00<00:00, 300.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Loss: 0.0974, F1 Score: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 76/76 [00:00<00:00, 184.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Loss: 0.0807, F1 Score: 0.8784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 76/76 [00:00<00:00, 304.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Loss: 0.0760, F1 Score: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 76/76 [00:00<00:00, 182.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Loss: 0.0797, F1 Score: 0.8444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 76/76 [00:00<00:00, 277.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Loss: 0.0806, F1 Score: 0.8859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 76/76 [00:00<00:00, 164.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Loss: 0.1148, F1 Score: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 76/76 [00:00<00:00, 281.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Loss: 0.1147, F1 Score: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 76/76 [00:00<00:00, 185.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Loss: 0.1013, F1 Score: 0.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 76/76 [00:00<00:00, 301.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Loss: 0.0809, F1 Score: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 76/76 [00:00<00:00, 174.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Loss: 0.0891, F1 Score: 0.8060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 76/76 [00:00<00:00, 213.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Loss: 0.0760, F1 Score: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 76/76 [00:00<00:00, 220.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Loss: 0.0661, F1 Score: 0.8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 76/76 [00:00<00:00, 184.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Loss: 0.1308, F1 Score: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 76/76 [00:00<00:00, 297.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Loss: 0.0842, F1 Score: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 76/76 [00:00<00:00, 182.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Loss: 0.0834, F1 Score: 0.8244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 76/76 [00:00<00:00, 277.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Loss: 0.0738, F1 Score: 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 76/76 [00:00<00:00, 194.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Loss: 0.0591, F1 Score: 0.9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 76/76 [00:00<00:00, 216.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Loss: 0.0581, F1 Score: 0.9014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 76/76 [00:00<00:00, 185.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Loss: 0.0562, F1 Score: 0.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 76/76 [00:00<00:00, 177.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Loss: 0.0490, F1 Score: 0.9189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 76/76 [00:00<00:00, 294.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Loss: 0.0517, F1 Score: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 76/76 [00:00<00:00, 196.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Loss: 0.0540, F1 Score: 0.9067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 76/76 [00:00<00:00, 168.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Loss: 0.0392, F1 Score: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 76/76 [00:00<00:00, 316.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Loss: 0.0381, F1 Score: 0.9028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 분류용\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 및 검증 함수\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = evaluate(model, valid_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'90frame000_LSTM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_900=[]\n",
    "\n",
    "for video in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1_1/Rel2'):\n",
    "    start,end = int(video.split('_')[-3]),int((video.split('_')[-2]))\n",
    "    total = end- start\n",
    "    if total >=900:\n",
    "        video_name = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1_1/Rel2',video)\n",
    "        test_900.append([video_name,total//2-450,total//2+450])         \n",
    "for video in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1/Rel2'):\n",
    "    start,end = int(video.split('_')[-3]),int((video.split('_')[-2]))\n",
    "    total = end- start\n",
    "    if total >=900:\n",
    "        video_name = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1/Rel2',video)\n",
    "        test_900.append([video_name,total//2-450,total//2+450])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/test_30초2\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in test_900:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 'y' 열 값 확인 및 제외 처리\n",
    "    if 'y' in filtered_df.columns and filtered_df['y'].isnull().all():\n",
    "        filtered_df = filtered_df.drop(columns=['y'])\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기3/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/test_30초2'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/test_0넣기3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 및 'frame' 열 확인\n",
    "    if 'label' not in df.columns or 'frame' not in df.columns:\n",
    "        print(f\"'label' 또는 'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 데이터프레임을 'label' 열로 그룹화\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        frames_to_add = []  # 새로운 프레임 데이터 저장 리스트\n",
    "\n",
    "        # 10프레임 간격으로 선택\n",
    "        for frame in range(min_frame, max_frame + 1, 10):\n",
    "            # 선택한 프레임이 존재하는 경우 그대로 추가\n",
    "            if frame in group['frame'].values:\n",
    "                frames_to_add.append(group[group['frame'] == frame].iloc[0].to_dict())\n",
    "            else:\n",
    "                # 대체 프레임 탐색: 우선적으로 다음 프레임, 그다음 이전 프레임\n",
    "                replacement_frame = None\n",
    "                if (frame + 1) in group['frame'].values:\n",
    "                    replacement_frame = frame + 1\n",
    "                elif (frame - 1) in group['frame'].values:\n",
    "                    replacement_frame = frame - 1\n",
    "\n",
    "                if replacement_frame:\n",
    "                    frames_to_add.append(group[group['frame'] == replacement_frame].iloc[0].to_dict())\n",
    "                else:\n",
    "                    # 대체 프레임도 없으면 0으로 채운 데이터 추가\n",
    "                    empty_row = {col: 0 for col in group.columns if col != 'label'}\n",
    "                    empty_row['label'] = label\n",
    "                    empty_row['frame'] = frame\n",
    "                    frames_to_add.append(empty_row)\n",
    "\n",
    "        # 생성된 프레임 리스트를 데이터프레임으로 변환\n",
    "        new_df = pd.DataFrame(frames_to_add)\n",
    "        processed_dfs.append(new_df)\n",
    "\n",
    "    # 그룹별 처리된 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/test_30초2/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/test_30초2/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/test_0넣기2/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/test_30초2'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/test_0넣기2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue  # 중단하지 않고 다음 파일로 이동\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열 확인\n",
    "    if 'label' not in df.columns:\n",
    "        print(f\"'label' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열 확인\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열을 기준으로 그룹화 및 프레임 보정\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        # 전체 프레임 범위 생성\n",
    "        full_frames = pd.DataFrame({'frame': range(min_frame, max_frame + 1)})\n",
    "\n",
    "        # 기존 데이터와 병합하여 누락된 프레임 추가\n",
    "        merged = pd.merge(full_frames, group, on='frame', how='left')\n",
    "\n",
    "        # 누락된 값 채우기\n",
    "        for col in merged.columns:\n",
    "            if col != 'frame' and col != 'label':  # frame과 label 열은 유지\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "        merged['label'] = merged['label'].fillna(label)\n",
    "\n",
    "        # 처리된 데이터 저장\n",
    "        processed_dfs.append(merged)\n",
    "\n",
    "    # 그룹 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>1131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13737</th>\n",
       "      <td>1141</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>133</td>\n",
       "      <td>28</td>\n",
       "      <td>133</td>\n",
       "      <td>46</td>\n",
       "      <td>162</td>\n",
       "      <td>30</td>\n",
       "      <td>162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>1151</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>40</td>\n",
       "      <td>128</td>\n",
       "      <td>46</td>\n",
       "      <td>161</td>\n",
       "      <td>43</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13739</th>\n",
       "      <td>1161</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>134</td>\n",
       "      <td>35</td>\n",
       "      <td>133</td>\n",
       "      <td>55</td>\n",
       "      <td>167</td>\n",
       "      <td>39</td>\n",
       "      <td>167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13740</th>\n",
       "      <td>1171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13741 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame  x1  y1  x2  y2  x3  y3  x4  y4  x5  ...  y14  x15  y15  x16  \\\n",
       "0        640   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "1        650   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "2        660   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "3        670   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "4        680   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "...      ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...   \n",
       "13736   1131   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "13737   1141  41  14  45  11  38  11  50  14  32  ...  133   28  133   46   \n",
       "13738   1151  47  14  51  11  44  11  57  14  40  ...  129   40  128   46   \n",
       "13739   1161  43  15  46  12  39  12  52  14  34  ...  134   35  133   55   \n",
       "13740   1171   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "\n",
       "       y16  x17  y17    y      label  \\\n",
       "0        0    0    0  0.0  ID: 110.0   \n",
       "1        0    0    0  0.0  ID: 110.0   \n",
       "2        0    0    0  0.0  ID: 110.0   \n",
       "3        0    0    0  0.0  ID: 110.0   \n",
       "4        0    0    0  0.0  ID: 110.0   \n",
       "...    ...  ...  ...  ...        ...   \n",
       "13736    0    0    0  0.0  ID: 530.0   \n",
       "13737  162   30  162  0.0  ID: 530.0   \n",
       "13738  161   43  160  0.0  ID: 530.0   \n",
       "13739  167   39  167  0.0  ID: 530.0   \n",
       "13740    0    0    0  0.0  ID: 530.0   \n",
       "\n",
       "                                                FILENAME  \n",
       "0      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "1      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "2      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "3      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "4      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "...                                                  ...  \n",
       "13736  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "13737  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "13738  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "13739  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "13740  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "\n",
       "[13741 rows x 38 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for vid in os.listdir('/home/alpaco/project/drunk_prj/data/test_0넣기3'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/test_0넣기3',vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    test_data = pd.concat([test_data,tmp_csv],ignore_index=True)\n",
    "test_data\n",
    "#13741    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = test_data[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_data[coordinate_cols] = X_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = test_data.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "test_data[columns_to_convert] = test_data[columns_to_convert].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(test_data, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1172931/1278838449.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLSTMModel(\n",
       "  (lstm): LSTM(34, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(X_seq)\n",
    "test_y_tensor = torch.LongTensor(Y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "loaded_model = BinaryLSTMModel(X_seq.shape[2],50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 59.60%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.42\n",
      "Recall: 0.83\n",
      "F1 Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 87.84%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.69\n",
      "Recall: 1.00\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds,pos_label=1)\n",
    "recall = recall_score(all_labels, all_preds,pos_label=1)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "over_90=[]\n",
    "import os\n",
    "for folders in os.listdir('/home/alpaco/project/drunk_prj/data/videofile/32.이동 행위'):\n",
    "    vid_path = os.path.join('/home/alpaco/project/drunk_prj/data/videofile/32.이동 행위',folders)\n",
    "    for video in os.listdir(vid_path):\n",
    "        video_path = os.path.join(vid_path,video)\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"동영상을 열 수 없습니다.\")\n",
    "        else:\n",
    "            # 총 프레임 수 확인\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames >=90:\n",
    "            csv_s = video.split('.')[0]+'.csv'\n",
    "            new_path = os.path.join('/home/alpaco/project/drunk_prj/data/normal_ver2',folders)\n",
    "            new_path = os.path.join(new_path,'abs')\n",
    "            new_path = os.path.join(new_path,csv_s)\n",
    "            over_90.append([new_path,total_frames//2-45,total_frames//2+45])\n",
    "        # 자원 해제\n",
    "        cap.release()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_b_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_b_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_a_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_a_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_a_aft_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_b_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_c_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_c_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_c_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_a_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_b_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_c_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_c_aft_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_b_aft_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_a_aft_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_a_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_a_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_c_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_b_aft_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_b_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_c_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_a_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_c_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_b_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_a_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_b_aft_DF2_(46_136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_c_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_b_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_c_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_a_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_b_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_c_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_a_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_a_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_c_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_a_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_a_aft_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_b_aft_DF2_(51_141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_b_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_c_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_b_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_c_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_b_for_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_a_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_c_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_c_for_DF2_(116_206).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_b_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_a_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_a_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_b_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_a_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_c_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_b_for_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_c_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_c_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_a_for_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_a_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_b_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_a_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_a_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_b_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_b_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_b_aft_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_a_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_c_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_a_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_b_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_b_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_c_aft_DF2_(48_138).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_c_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_c_aft_DF2_(52_142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_(49_139).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_b_aft_DF2_(50_140).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_b_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_c_for_DF2_(53_143).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_a_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_c_for_DF2_(59_149).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_c_for_DF2_(59_149).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_a_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_b_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_b_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_b_for_DF2_(55_145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_a_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_a_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_c_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_c_for_DF2_(54_144).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_b_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_b_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_c_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_b_for_DF2_(57_147).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_a_for_DF2_(58_148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_a_for_DF2_(56_146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_b_for_DF2_(54_144).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/abs_normal_30초\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in over_90:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_c_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-47-00_c_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_c_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-17-00_c_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_b_aft_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-48-00_b_aft_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_c_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-41-00_c_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_a_for_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-44-00_a_for_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_a_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-43-00_a_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_b_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-47-00_b_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_a_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-48-00_a_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_b_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-59-00_b_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_b_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-25-00_b_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_a_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-55-00_a_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_c_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-56-00_c_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_c_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-47-00_c_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_b_for_DF2_(58_148).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-41-00_b_for_DF2_(58_148).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_c_for_DF2_(116_206).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-50-00_c_for_DF2_(116_206).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_c_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-06-00_c_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_a_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-01-00_a_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_b_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-01-00_b_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_c_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-27-00_c_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-03-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-06-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_c_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-50-00_c_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_c_for_DF2_(59_149).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-41-00_c_for_DF2_(59_149).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_c_aft_DF2_(46_136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_12_smp_su_09-11_15-10-00_c_aft_DF2_(46_136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_c_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-52-00_c_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_a_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-56-00_a_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_c_aft_DF2_(46_136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_12_smp_su_09-11_15-08-00_c_aft_DF2_(46_136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_b_for_DF2_(53_143).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_b_for_DF2_(53_143).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-21-00_a_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_b_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-27-00_b_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_c_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-08-00_c_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_a_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_11-01-00_a_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_a_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-54-00_a_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-47-00_a_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-47-00_a_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_a_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-59-00_a_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_c_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-44-00_c_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_c_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-21-00_c_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_b_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-23-00_b_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_b_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-19-00_b_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_b_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-01-00_b_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_c_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-56-00_c_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_b_for_DF2_(58_148).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-54-00_b_for_DF2_(58_148).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_c_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-11-00_c_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_c_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-59-00_c_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_b_aft_DF2_(46_136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_12_smp_su_09-11_15-10-00_b_aft_DF2_(46_136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-29-00_a_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_a_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-50-00_a_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_c_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-50-00_c_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_a_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-21-00_a_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_a_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-25-00_a_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_b_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-11-00_b_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-52-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_a_aft_DF2_(46_136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_12_smp_su_09-11_15-08-00_a_aft_DF2_(46_136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_c_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-23-00_c_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_a_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-59-00_a_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_b_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-41-00_b_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-57-00_a_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_c_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-21-00_c_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_b_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-57-00_b_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_b_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-08-00_b_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_b_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-49-00_b_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-47-00_a_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-47-00_a_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_b_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-43-00_b_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-21-00_b_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-21-00_b_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_11-01-00_b_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_11-01-00_b_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_c_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-06-00_c_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_b_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-56-00_b_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_b_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-03-00_b_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-06-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-41-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-41-00_a_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-08-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-08-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_a_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-33-00_a_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_a_for_DF2_(58_148).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-43-00_a_for_DF2_(58_148).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-41-00_a_for_DF2_(58_148).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-41-00_a_for_DF2_(58_148).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_a_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-23-00_a_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_b_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-53-00_b_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_c_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-11-00_c_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_a_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-09-00_a_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-17-00_a_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_b_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-59-00_b_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-50-00_b_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-50-00_b_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_b_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-43-00_b_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_a_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-25-00_a_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_a_aft_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-50-00_a_aft_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_c_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-53-00_c_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-52-00_b_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-52-00_b_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-50-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_b_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-00-00_b_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_a_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-19-00_a_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_c_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-00-00_c_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-11-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-11-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-06-00_b_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-06-00_b_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_b_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-09-00_b_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-08-00_b_aft_DF2_(46_136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_12_smp_su_09-11_15-08-00_b_aft_DF2_(46_136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_c_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-48-00_c_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_a_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-49-00_a_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-21-00_b_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-21-00_b_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_b_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-33-00_b_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-54-00_c_for_DF2_(59_149).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-54-00_c_for_DF2_(59_149).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_b_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-29-00_b_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_a_aft_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-48-00_a_aft_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-59-00_b_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-59-00_b_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_12_smp_su_09-11_15-10-00_a_aft_DF2_(46_136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_12_smp_su_09-11_15-10-00_a_aft_DF2_(46_136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-49-00_c_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-49-00_c_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-43-00_c_for_DF2_(57_147).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-43-00_c_for_DF2_(57_147).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-56-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-56-00_a_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-53-00_a_aft_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-53-00_a_aft_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_b_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-55-00_b_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-48-00_c_aft_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-48-00_c_aft_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-19-00_c_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-19-00_c_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-43-00_c_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-43-00_c_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_a_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-59-00_a_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_13-50-00_b_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_13-50-00_b_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-09-00_c_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-09-00_c_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-06-00_b_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-06-00_b_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-17-00_b_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-17-00_b_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_a_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-42-00_a_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-44-00_b_for_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-44-00_b_for_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-29-00_c_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-29-00_c_for_DF2_(56_146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-01-00_c_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-01-00_c_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-50-00_b_aft_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-50-00_b_aft_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-55-00_c_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-55-00_c_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-23-00_c_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-23-00_c_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-03-00_c_aft_DF2_(49_139).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-03-00_c_aft_DF2_(49_139).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_13-25-00_b_aft_DF2_(51_141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_13-25-00_b_aft_DF2_(51_141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_a_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-23-00_a_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-25-00_c_for_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-25-00_c_for_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_b_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-11-00_b_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-23-00_b_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-23-00_b_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_c_for_DF2_(54_144).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-42-00_c_for_DF2_(54_144).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_15-01-00_c_aft_DF2_(48_138).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_15-01-00_c_aft_DF2_(48_138).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-48-00_b_for_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-48-00_b_for_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-42-00_b_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-42-00_b_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-59-00_c_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-59-00_c_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-56-00_b_aft_DF2_(53_143).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-56-00_b_aft_DF2_(53_143).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_9_smp_su_09-11_12-59-00_c_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_9_smp_su_09-11_12-59-00_c_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_7_smp_su_09-11_10-57-00_c_for_DF2_(55_145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_7_smp_su_09-11_10-57-00_c_for_DF2_(55_145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-00-00_a_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-00-00_a_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_10_smp_su_09-11_14-11-00_a_aft_DF2_(50_140).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_10_smp_su_09-11_14-11-00_a_aft_DF2_(50_140).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_11_smp_su_09-11_14-33-00_c_aft_DF2_(52_142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_11_smp_su_09-11_14-33-00_c_aft_DF2_(52_142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_normal_30초/C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_normal_0넣기/C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/abs_normal_30초'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/abs_normal_0넣기'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue  # 중단하지 않고 다음 파일로 이동\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열 확인\n",
    "    if 'label' not in df.columns:\n",
    "        print(f\"'label' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열 확인\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열을 기준으로 그룹화 및 프레임 보정\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        # 전체 프레임 범위 생성\n",
    "        full_frames = pd.DataFrame({'frame': range(min_frame, max_frame + 1)})\n",
    "\n",
    "        # 기존 데이터와 병합하여 누락된 프레임 추가\n",
    "        merged = pd.merge(full_frames, group, on='frame', how='left')\n",
    "\n",
    "        # 누락된 값 채우기\n",
    "        for col in merged.columns:\n",
    "            if col != 'frame' and col != 'label':  # frame과 label 열은 유지\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "        merged['label'] = merged['label'].fillna(label)\n",
    "\n",
    "        # 처리된 데이터 저장\n",
    "        processed_dfs.append(merged)\n",
    "\n",
    "    # 그룹 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam03_drunken04_place03_night_spring_174_1098_totter_(12_912).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam03_drunken02_place01_night_summer_107_1221_totter_(107_1007).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-5_cam02_drunken04_place03_night_winter_222_1402_totter_(140_1040).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam01_drunken04_place03_night_winter_194_2250_totter_(578_1478).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-5_cam03_drunken04_place03_night_spring_2140_3078_totter_(19_919).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam03_drunken04_place03_night_winter_248_1762_totter_(307_1207).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam03_drunken02_place01_night_winter_122_1113_totter_(45_945).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam03_drunken02_place01_night_winter_1713_2835_totter_(111_1011).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam01_drunken02_place01_night_winter_1935_2973_totter_(69_969).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam02_drunken02_place01_night_winter_227_1241_totter_(57_957).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam01_drunken04_place03_night_summer_147_1942_totter_(447_1347).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam01_drunken02_place01_night_summer_264_1413_totter_(124_1024).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-4_cam01_drunken02_place01_night_spring_117_1033_totter_(8_908).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam02_drunken04_place03_night_summer_153_1969_totter_(458_1358).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam02_drunken04_place03_night_winter_168_2251_totter_(591_1491).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam02_drunken02_place01_night_summer_270_1447_totter_(138_1038).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam03_drunken04_place03_night_spring_4523_5987_totter_(282_1182).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam02_drunken04_place03_night_winter_297_1793_totter_(298_1198).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam01_drunken04_place03_night_winter_265_1768_totter_(301_1201).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam02_drunken02_place01_night_winter_1813_2977_totter_(132_1032).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam01_drunken02_place01_night_winter_321_1389_totter_(84_984).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam03_drunken04_place03_night_summer_155_1916_totter_(430_1330).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam02_drunken04_place03_night_spring_4527_5936_totter_(254_1154).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam03_drunken04_place03_night_summer_400_1810_totter_(255_1155).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam03_drunken01_place03_night_winter_381_2313_totter_(516_1416).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam01_drunken01_place03_night_spring_4990_6291_totter_(200_1100).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_spring_4992_6262_totter_(185_1085).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam01_drunken01_place03_night_spring_6823_8222_totter_(249_1149).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_summer_397_2910_totter_(806_1706).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_spring_5038_6650_totter_(356_1256).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_spring_6807_8169_totter_(231_1131).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_summer_406_1537_totter_(115_1015).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_spring_6819_8189_totter_(235_1135).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_spring_455_3546_totter_(1095_1995).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_summer_6579_8095_totter_(308_1208).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_winter_5718_6654_totter_(18_918).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_winter_5752_6663_totter_(5_905).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_winter_382_4237_totter_(1477_2377).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_winter_5791_7513_totter_(411_1311).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_spring_394_3564_totter_(1135_2035).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_spring_2261_3878_totter_(358_1258).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_winter_7296_8365_totter_(84_984).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_summer_576_4476_totter_(1500_2400).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-2_cam03_drunken01_place03_night_spring_2266_3496_totter_(165_1065).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-2_cam02_drunken01_place03_night_spring_2247_3165_totter_(9_909).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_spring_5106_6243_totter_(118_1018).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_winter_5808_7585_totter_(438_1338).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_spring_2279_3416_totter_(118_1018).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_winter_7281_8371_totter_(95_995).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-2_cam01_drunken01_place03_night_spring_2255_3161_totter_(3_903).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam03_drunken01_place03_night_spring_4998_6499_totter_(300_1200).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_spring_526_3559_totter_(1066_1966).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_summer_6040_8183_totter_(621_1521).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_summer_462_4454_totter_(1546_2446).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_winter_421_4278_totter_(1478_2378).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_winter_413_2350_totter_(518_1418).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_winter_2378_3736_totter_(229_1129).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_winter_3994_5252_totter_(179_1079).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_summer_1841_2949_totter_(104_1004).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_spring_5000_6280_totter_(190_1090).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam01_drunken03_place03_night_winter_1645_2747_totter_(101_1001).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam01_drunken04_place02_night_spring_3310_4503_totter_(146_1046).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_spring_6703_7742_totter_(69_969).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam01_drunken03_place03_night_winter_492_1623_totter_(115_1015).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_spring_3372_5824_totter_(776_1676).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_winter_6973_8183_totter_(155_1055).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_spring_3377_5818_totter_(770_1670).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam03_drunken03_place03_night_summer_1644_2608_totter_(32_932).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam03_drunken03_place03_night_spring_96_1488_totter_(246_1146).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam01_drunken03_place03_night_winter_473_4519_totter_(1573_2473).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_spring_3440_5836_totter_(748_1648).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_winter_3801_6423_totter_(861_1761).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_winter_7033_8201_totter_(134_1034).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam02_drunken03_place03_night_winter_472_4543_totter_(1585_2485).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam03_drunken03_place03_night_winter_481_4532_totter_(1575_2475).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_winter_3778_6383_totter_(852_1752).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam01_drunken03_place03_night_summer_1639_2579_totter_(20_920).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam02_drunken03_place03_night_summer_1671_2626_totter_(27_927).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_summer_4043_6527_totter_(792_1692).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam02_drunken03_place03_night_winter_142_1602_totter_(280_1180).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam02_drunken04_place02_night_summer_3827_5457_totter_(365_1265).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_summer_7333_8646_totter_(206_1106).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam02_drunken03_place03_night_winter_1634_2751_totter_(108_1008).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_spring_6701_7707_totter_(53_953).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_summer_7269_8601_totter_(216_1116).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam03_drunken03_place03_night_summer_117_1029_totter_(6_906).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam03_drunken03_place03_night_winter_121_1588_totter_(283_1183).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_spring_6577_7747_totter_(135_1035).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_summer_7295_8596_totter_(200_1100).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_winter_6974_8224_totter_(175_1075).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam02_drunken03_place03_night_spring_141_1493_totter_(226_1126).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam02_drunken03_place03_night_spring_1378_2664_totter_(193_1093).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam01_drunken03_place03_night_spring_131_1922_totter_(445_1345).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_summer_4001_6485_totter_(792_1692).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam03_drunken03_place03_night_winter_1622_2757_totter_(117_1017).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_winter_3804_6397_totter_(846_1746).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_summer_3995_6473_totter_(789_1689).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam01_drunken04_place02_night_summer_3701_5505_totter_(452_1352).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam03_drunken03_place03_night_summer_151_3017_totter_(983_1883).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam03_drunken03_place03_night_winter_2733_3932_totter_(149_1049).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam02_drunken03_place03_night_winter_1597_3116_totter_(309_1209).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam01_drunken03_place03_night_winter_1478_3040_totter_(331_1231).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam01_drunken03_place03_night_summer_1136_4740_totter_(1352_2252).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam03_drunken03_place03_night_winter_162_1576_totter_(257_1157).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam01_drunken03_place03_night_summer_133_1529_totter_(248_1148).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam03_drunken03_place03_night_winter_140_2996_totter_(978_1878).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam01_drunken03_place03_night_winter_479_1427_totter_(24_924).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam02_drunken03_place03_night_winter_213_1876_totter_(381_1281).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam02_drunken03_place03_night_summer_1995_3379_totter_(242_1142).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam03_drunken03_place03_night_spring_165_1183_totter_(59_959).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam01_drunken03_place03_night_winter_198_1608_totter_(255_1155).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam03_drunken03_place03_night_winter_4195_5643_totter_(274_1174).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_winter_4185_5658_totter_(286_1186).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_spring_332_3141_totter_(954_1854).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam03_drunken03_place03_night_summer_1957_3339_totter_(241_1141).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_spring_173_3282_totter_(1104_2004).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_spring_4528_5885_totter_(228_1128).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_winter_436_2936_totter_(800_1700).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_summer_189_3078_totter_(994_1894).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam02_drunken03_place03_night_winter_176_1512_totter_(218_1118).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_summer_442_2906_totter_(782_1682).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_summer_4111_5095_totter_(42_942).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_winter_141_3004_totter_(981_1881).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam03_drunken03_place03_night_summer_156_1626_totter_(285_1185).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam02_drunken03_place03_night_winter_3063_4354_totter_(195_1095).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_summer_4248_5174_totter_(13_913).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam01_drunken03_place03_night_winter_2716_3841_totter_(112_1012).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_spring_4613_6061_totter_(274_1174).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_winter_4115_5522_totter_(253_1153).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_summer_4693_5799_totter_(103_1003).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam02_drunken03_place02_night_spring_7694_8961_totter_(183_1083).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam02_drunken03_place02_night_winter_1594_3038_totter_(272_1172).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam01_drunken03_place02_night_winter_4408_5592_totter_(142_1042).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam01_drunken03_place02_night_winter_8030_8939_totter_(4_904).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam02_drunken03_place02_night_spring_7390_8709_totter_(209_1109).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam01_drunken03_place02_night_winter_1681_3116_totter_(267_1167).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam02_drunken03_place02_night_winter_7950_8870_totter_(10_910).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_winter_1589_3053_totter_(282_1182).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-3_cam01_drunken03_place02_night_winter_7568_8475_totter_(3_903).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_spring_3793_4965_totter_(136_1036).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_winter_8296_9225_totter_(14_914).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam01_drunken03_place02_night_spring_7460_8736_totter_(188_1088).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_spring_7371_8417_totter_(73_973).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam03_drunken03_place02_night_summer_4668_5697_totter_(64_964).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-3_cam01_drunken03_place02_night_summer_1205_2292_totter_(93_993).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam03_drunken03_place02_night_winter_4353_5577_totter_(162_1062).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam03_drunken03_place02_night_spring_7702_8943_totter_(170_1070).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam02_drunken03_place02_night_summer_7436_8740_totter_(202_1102).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam03_drunken03_place02_night_spring_3786_4969_totter_(141_1041).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam01_drunken03_place02_night_summer_1227_2149_totter_(11_911).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_summer_7625_8630_totter_(52_952).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_spring_2435_3361_totter_(13_913).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam01_drunken03_place02_night_summer_4699_5755_totter_(78_978).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam01_drunken03_place02_night_spring_7893_9008_totter_(107_1007).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_winter_4418_5531_totter_(106_1006).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam03_drunken01_place03_night_summer_289_2180_totter_(495_1395).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam01_drunken04_place03_night_summer_173_1204_totter_(65_965).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam01_drunken01_place03_night_spring_256_1162_totter_(3_903).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam03_drunken01_place03_night_spring_1440_3441_totter_(550_1450).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam03_drunken01_place03_night_winter_261_2243_totter_(541_1441).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam03_drunken01_place03_night_winter_2260_3383_totter_(111_1011).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam01_drunken01_place03_night_summer_495_2457_totter_(531_1431).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam02_drunken01_place03_night_winter_248_2231_totter_(541_1441).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam02_drunken04_place03_night_summer_122_1208_totter_(93_993).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-2_cam03_drunken04_place03_night_summer_112_1036_totter_(12_912).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam01_drunken01_place03_night_winter_1611_2920_totter_(204_1104).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam01_drunken01_place03_night_winter_234_2164_totter_(515_1415).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam01_drunken01_place03_night_spring_1333_3315_totter_(541_1441).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam01_drunken01_place03_night_spring_388_1488_totter_(100_1000).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam01_drunken01_place03_night_spring_451_1524_totter_(86_986).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam03_drunken01_place03_night_spring_293_1245_totter_(26_926).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam01_drunken01_place03_night_summer_1842_3078_totter_(168_1068).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam02_drunken01_place03_night_summer_151_1472_totter_(210_1110).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam02_drunken01_place03_night_summer_1704_3061_totter_(228_1128).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam02_drunken01_place03_night_winter_1776_2994_totter_(159_1059).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam02_drunken01_place03_night_spring_541_1523_totter_(41_941).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam03_drunken04_place03_night_summer_127_1218_totter_(95_995).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam03_drunken04_place03_night_winter_171_1180_totter_(54_954).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam02_drunken01_place03_night_spring_141_1497_totter_(228_1128).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam02_drunken01_place03_night_summer_204_2607_totter_(751_1651).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam02_drunken01_place03_night_summer_178_1550_totter_(236_1136).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam01_drunken04_place03_night_winter_156_1119_totter_(31_931).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam01_drunken01_place03_night_summer_336_1535_totter_(149_1049).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-2_cam03_drunken04_place03_night_spring_93_1021_totter_(14_914).csv\n"
     ]
    }
   ],
   "source": [
    "over_900=[]\n",
    "for folders in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_video1/totter/Abs'):\n",
    "    vid_path = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_video1/totter/Abs',folders)\n",
    "    for video in os.listdir(vid_path):\n",
    "        start,end = int(video.split('_')[-3]),int((video.split('_')[-2]).split('.')[0])\n",
    "        total = end- start\n",
    "        if total >=900:\n",
    "            video_name = os.path.join(vid_path,video)\n",
    "            over_900.append([video_name,total//2-450,total//2+450])            \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/abs_croki_30초\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in over_900:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_spring_5038_6650_totter_(356_1256).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam02_drunken01_place03_night_spring_5038_6650_totter_(356_1256).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam01_drunken03_place03_night_winter_479_1427_totter_(24_924).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-4_cam01_drunken03_place03_night_winter_479_1427_totter_(24_924).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam01_drunken03_place02_night_spring_7893_9008_totter_(107_1007).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-2_cam01_drunken03_place02_night_spring_7893_9008_totter_(107_1007).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam03_drunken04_place03_night_summer_155_1916_totter_(430_1330).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-2_cam03_drunken04_place03_night_summer_155_1916_totter_(430_1330).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam02_drunken03_place03_night_winter_1597_3116_totter_(309_1209).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-4_cam02_drunken03_place03_night_winter_1597_3116_totter_(309_1209).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_summer_462_4454_totter_(1546_2446).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam02_drunken01_place03_night_summer_462_4454_totter_(1546_2446).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam02_drunken03_place03_night_spring_141_1493_totter_(226_1126).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam02_drunken03_place03_night_spring_141_1493_totter_(226_1126).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam02_drunken03_place02_night_spring_7390_8709_totter_(209_1109).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam02_drunken03_place02_night_spring_7390_8709_totter_(209_1109).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam03_drunken03_place03_night_spring_165_1183_totter_(59_959).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-2_cam03_drunken03_place03_night_spring_165_1183_totter_(59_959).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam01_drunken03_place03_night_summer_1639_2579_totter_(20_920).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-5_cam01_drunken03_place03_night_summer_1639_2579_totter_(20_920).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam01_drunken03_place02_night_winter_8030_8939_totter_(4_904).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-2_cam01_drunken03_place02_night_winter_8030_8939_totter_(4_904).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-2_cam02_drunken01_place03_night_spring_2247_3165_totter_(9_909).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-2_cam02_drunken01_place03_night_spring_2247_3165_totter_(9_909).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-4_cam01_drunken02_place01_night_spring_117_1033_totter_(8_908).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-4_cam01_drunken02_place01_night_spring_117_1033_totter_(8_908).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam01_drunken03_place03_night_winter_198_1608_totter_(255_1155).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-1_cam01_drunken03_place03_night_winter_198_1608_totter_(255_1155).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam03_drunken03_place02_night_winter_4353_5577_totter_(162_1062).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam03_drunken03_place02_night_winter_4353_5577_totter_(162_1062).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam03_drunken01_place03_night_winter_381_2313_totter_(516_1416).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam03_drunken01_place03_night_winter_381_2313_totter_(516_1416).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam03_drunken03_place03_night_winter_4195_5643_totter_(274_1174).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam03_drunken03_place03_night_winter_4195_5643_totter_(274_1174).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_winter_5718_6654_totter_(18_918).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam01_drunken01_place03_night_winter_5718_6654_totter_(18_918).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam02_drunken03_place03_night_winter_176_1512_totter_(218_1118).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-4_cam02_drunken03_place03_night_winter_176_1512_totter_(218_1118).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam02_drunken03_place03_night_winter_213_1876_totter_(381_1281).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-1_cam02_drunken03_place03_night_winter_213_1876_totter_(381_1281).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_summer_397_2910_totter_(806_1706).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam02_drunken01_place03_night_summer_397_2910_totter_(806_1706).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam02_drunken01_place03_night_summer_178_1550_totter_(236_1136).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam02_drunken01_place03_night_summer_178_1550_totter_(236_1136).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam01_drunken01_place03_night_winter_1611_2920_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam01_drunken01_place03_night_winter_1611_2920_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam01_drunken03_place02_night_winter_4408_5592_totter_(142_1042).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam01_drunken03_place02_night_winter_4408_5592_totter_(142_1042).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam03_drunken04_place03_night_spring_4523_5987_totter_(282_1182).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-6_cam03_drunken04_place03_night_spring_4523_5987_totter_(282_1182).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_spring_6703_7742_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam01_drunken04_place02_night_spring_6703_7742_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam02_drunken02_place01_night_winter_227_1241_totter_(57_957).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam02_drunken02_place01_night_winter_227_1241_totter_(57_957).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam03_drunken03_place03_night_winter_121_1588_totter_(283_1183).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam03_drunken03_place03_night_winter_121_1588_totter_(283_1183).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam01_drunken03_place03_night_winter_1645_2747_totter_(101_1001).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-5_cam01_drunken03_place03_night_winter_1645_2747_totter_(101_1001).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_winter_6973_8183_totter_(155_1055).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam03_drunken04_place02_night_winter_6973_8183_totter_(155_1055).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam03_drunken04_place03_night_summer_127_1218_totter_(95_995).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-5_cam03_drunken04_place03_night_summer_127_1218_totter_(95_995).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_summer_6579_8095_totter_(308_1208).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam02_drunken01_place03_night_summer_6579_8095_totter_(308_1208).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam03_drunken03_place03_night_summer_151_3017_totter_(983_1883).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam03_drunken03_place03_night_summer_151_3017_totter_(983_1883).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_spring_3440_5836_totter_(748_1648).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam02_drunken04_place02_night_spring_3440_5836_totter_(748_1648).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam01_drunken01_place03_night_summer_336_1535_totter_(149_1049).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam01_drunken01_place03_night_summer_336_1535_totter_(149_1049).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_winter_382_4237_totter_(1477_2377).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam01_drunken01_place03_night_winter_382_4237_totter_(1477_2377).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam02_drunken02_place01_night_summer_270_1447_totter_(138_1038).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam02_drunken02_place01_night_summer_270_1447_totter_(138_1038).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam02_drunken03_place03_night_winter_142_1602_totter_(280_1180).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam02_drunken03_place03_night_winter_142_1602_totter_(280_1180).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam01_drunken03_place03_night_summer_133_1529_totter_(248_1148).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-4_cam01_drunken03_place03_night_summer_133_1529_totter_(248_1148).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam01_drunken03_place02_night_summer_4699_5755_totter_(78_978).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam01_drunken03_place02_night_summer_4699_5755_totter_(78_978).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam01_drunken04_place03_night_summer_173_1204_totter_(65_965).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-5_cam01_drunken04_place03_night_summer_173_1204_totter_(65_965).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_winter_7033_8201_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam01_drunken04_place02_night_winter_7033_8201_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam02_drunken03_place02_night_winter_7950_8870_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-2_cam02_drunken03_place02_night_winter_7950_8870_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_summer_406_1537_totter_(115_1015).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam01_drunken01_place03_night_summer_406_1537_totter_(115_1015).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_winter_1589_3053_totter_(282_1182).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam03_drunken03_place02_night_winter_1589_3053_totter_(282_1182).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam03_drunken02_place01_night_winter_1713_2835_totter_(111_1011).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam03_drunken02_place01_night_winter_1713_2835_totter_(111_1011).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_winter_4115_5522_totter_(253_1153).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam01_drunken03_place03_night_winter_4115_5522_totter_(253_1153).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_summer_4043_6527_totter_(792_1692).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam01_drunken04_place02_night_summer_4043_6527_totter_(792_1692).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam02_drunken04_place03_night_winter_297_1793_totter_(298_1198).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-2_cam02_drunken04_place03_night_winter_297_1793_totter_(298_1198).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_winter_7281_8371_totter_(95_995).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam01_drunken01_place03_night_winter_7281_8371_totter_(95_995).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_winter_8296_9225_totter_(14_914).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam02_drunken03_place02_night_winter_8296_9225_totter_(14_914).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-2_cam01_drunken01_place03_night_spring_2255_3161_totter_(3_903).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-2_cam01_drunken01_place03_night_spring_2255_3161_totter_(3_903).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_winter_3778_6383_totter_(852_1752).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam02_drunken04_place02_night_winter_3778_6383_totter_(852_1752).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam01_drunken01_place03_night_spring_388_1488_totter_(100_1000).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam01_drunken01_place03_night_spring_388_1488_totter_(100_1000).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam02_drunken01_place03_night_spring_141_1497_totter_(228_1128).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam02_drunken01_place03_night_spring_141_1497_totter_(228_1128).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_winter_4418_5531_totter_(106_1006).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam02_drunken03_place02_night_winter_4418_5531_totter_(106_1006).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_winter_141_3004_totter_(981_1881).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam02_drunken03_place03_night_winter_141_3004_totter_(981_1881).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam01_drunken03_place02_night_spring_7460_8736_totter_(188_1088).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam01_drunken03_place02_night_spring_7460_8736_totter_(188_1088).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam02_drunken01_place03_night_summer_204_2607_totter_(751_1651).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam02_drunken01_place03_night_summer_204_2607_totter_(751_1651).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam03_drunken03_place03_night_summer_117_1029_totter_(6_906).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam03_drunken03_place03_night_summer_117_1029_totter_(6_906).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam02_drunken03_place03_night_summer_1995_3379_totter_(242_1142).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-2_cam02_drunken03_place03_night_summer_1995_3379_totter_(242_1142).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam03_drunken03_place03_night_summer_156_1626_totter_(285_1185).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-4_cam03_drunken03_place03_night_summer_156_1626_totter_(285_1185).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_spring_3377_5818_totter_(770_1670).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam01_drunken04_place02_night_spring_3377_5818_totter_(770_1670).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam02_drunken01_place03_night_winter_248_2231_totter_(541_1441).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam02_drunken01_place03_night_winter_248_2231_totter_(541_1441).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_winter_421_4278_totter_(1478_2378).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam02_drunken01_place03_night_winter_421_4278_totter_(1478_2378).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_winter_436_2936_totter_(800_1700).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam01_drunken03_place03_night_winter_436_2936_totter_(800_1700).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_spring_6819_8189_totter_(235_1135).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam02_drunken01_place03_night_spring_6819_8189_totter_(235_1135).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam01_drunken03_place03_night_spring_131_1922_totter_(445_1345).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam01_drunken03_place03_night_spring_131_1922_totter_(445_1345).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam01_drunken01_place03_night_winter_234_2164_totter_(515_1415).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam01_drunken01_place03_night_winter_234_2164_totter_(515_1415).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam01_drunken01_place03_night_spring_6823_8222_totter_(249_1149).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam01_drunken01_place03_night_spring_6823_8222_totter_(249_1149).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_winter_5808_7585_totter_(438_1338).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam03_drunken01_place03_night_winter_5808_7585_totter_(438_1338).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-3_cam01_drunken03_place02_night_summer_1205_2292_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-3_cam01_drunken03_place02_night_summer_1205_2292_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam03_drunken03_place03_night_summer_1957_3339_totter_(241_1141).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-2_cam03_drunken03_place03_night_summer_1957_3339_totter_(241_1141).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam03_drunken03_place03_night_winter_162_1576_totter_(257_1157).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-1_cam03_drunken03_place03_night_winter_162_1576_totter_(257_1157).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam03_drunken03_place03_night_winter_481_4532_totter_(1575_2475).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-2_cam03_drunken03_place03_night_winter_481_4532_totter_(1575_2475).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-2_cam03_drunken04_place03_night_spring_93_1021_totter_(14_914).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-2_cam03_drunken04_place03_night_spring_93_1021_totter_(14_914).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_summer_1841_2949_totter_(104_1004).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam01_drunken01_place03_night_summer_1841_2949_totter_(104_1004).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam02_drunken04_place03_night_summer_122_1208_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-5_cam02_drunken04_place03_night_summer_122_1208_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam01_drunken01_place03_night_spring_256_1162_totter_(3_903).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam01_drunken01_place03_night_spring_256_1162_totter_(3_903).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_spring_5000_6280_totter_(190_1090).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam02_drunken01_place03_night_spring_5000_6280_totter_(190_1090).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_summer_4111_5095_totter_(42_942).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam01_drunken03_place03_night_summer_4111_5095_totter_(42_942).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-5_cam02_drunken04_place03_night_winter_222_1402_totter_(140_1040).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-5_cam02_drunken04_place03_night_winter_222_1402_totter_(140_1040).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam01_drunken04_place03_night_winter_265_1768_totter_(301_1201).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-2_cam01_drunken04_place03_night_winter_265_1768_totter_(301_1201).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_spring_4992_6262_totter_(185_1085).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam03_drunken01_place03_night_spring_4992_6262_totter_(185_1085).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam02_drunken01_place03_night_summer_151_1472_totter_(210_1110).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam02_drunken01_place03_night_summer_151_1472_totter_(210_1110).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_summer_7625_8630_totter_(52_952).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam03_drunken03_place02_night_summer_7625_8630_totter_(52_952).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam02_drunken01_place03_night_winter_5791_7513_totter_(411_1311).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam02_drunken01_place03_night_winter_5791_7513_totter_(411_1311).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam02_drunken04_place03_night_winter_168_2251_totter_(591_1491).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-6_cam02_drunken04_place03_night_winter_168_2251_totter_(591_1491).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam03_drunken03_place02_night_summer_4668_5697_totter_(64_964).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam03_drunken03_place02_night_summer_4668_5697_totter_(64_964).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam03_drunken04_place03_night_winter_171_1180_totter_(54_954).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-5_cam03_drunken04_place03_night_winter_171_1180_totter_(54_954).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-2_cam03_drunken01_place03_night_spring_2266_3496_totter_(165_1065).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-2_cam03_drunken01_place03_night_spring_2266_3496_totter_(165_1065).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_spring_2261_3878_totter_(358_1258).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam02_drunken01_place03_night_spring_2261_3878_totter_(358_1258).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_spring_4613_6061_totter_(274_1174).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam02_drunken03_place03_night_spring_4613_6061_totter_(274_1174).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam03_drunken02_place01_night_summer_107_1221_totter_(107_1007).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam03_drunken02_place01_night_summer_107_1221_totter_(107_1007).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam02_drunken03_place02_night_spring_7694_8961_totter_(183_1083).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-2_cam02_drunken03_place02_night_spring_7694_8961_totter_(183_1083).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam03_drunken02_place01_night_winter_122_1113_totter_(45_945).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam03_drunken02_place01_night_winter_122_1113_totter_(45_945).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_summer_4693_5799_totter_(103_1003).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam02_drunken03_place02_night_summer_4693_5799_totter_(103_1003).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam01_drunken03_place02_night_winter_1681_3116_totter_(267_1167).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam01_drunken03_place02_night_winter_1681_3116_totter_(267_1167).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-5_cam01_drunken04_place03_night_winter_156_1119_totter_(31_931).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-5_cam01_drunken04_place03_night_winter_156_1119_totter_(31_931).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam02_drunken04_place02_night_summer_3827_5457_totter_(365_1265).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/623-2_cam02_drunken04_place02_night_summer_3827_5457_totter_(365_1265).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_spring_332_3141_totter_(954_1854).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam01_drunken03_place03_night_spring_332_3141_totter_(954_1854).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_summer_7269_8601_totter_(216_1116).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam03_drunken04_place02_night_summer_7269_8601_totter_(216_1116).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-5_cam03_drunken04_place03_night_spring_2140_3078_totter_(19_919).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-5_cam03_drunken04_place03_night_spring_2140_3078_totter_(19_919).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_summer_442_2906_totter_(782_1682).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam01_drunken03_place03_night_summer_442_2906_totter_(782_1682).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_summer_4001_6485_totter_(792_1692).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam02_drunken04_place02_night_summer_4001_6485_totter_(792_1692).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-3_cam01_drunken03_place02_night_winter_7568_8475_totter_(3_903).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-3_cam01_drunken03_place02_night_winter_7568_8475_totter_(3_903).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam02_drunken03_place02_night_summer_7436_8740_totter_(202_1102).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam02_drunken03_place02_night_summer_7436_8740_totter_(202_1102).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-1_cam02_drunken03_place03_night_winter_3063_4354_totter_(195_1095).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-1_cam02_drunken03_place03_night_winter_3063_4354_totter_(195_1095).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam03_drunken04_place03_night_summer_400_1810_totter_(255_1155).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-6_cam03_drunken04_place03_night_summer_400_1810_totter_(255_1155).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_winter_7296_8365_totter_(84_984).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam03_drunken01_place03_night_winter_7296_8365_totter_(84_984).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam03_drunken03_place03_night_winter_140_2996_totter_(978_1878).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam03_drunken03_place03_night_winter_140_2996_totter_(978_1878).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam02_drunken01_place03_night_winter_413_2350_totter_(518_1418).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam02_drunken01_place03_night_winter_413_2350_totter_(518_1418).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_spring_173_3282_totter_(1104_2004).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam02_drunken03_place03_night_spring_173_3282_totter_(1104_2004).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam03_drunken03_place03_night_spring_96_1488_totter_(246_1146).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam03_drunken03_place03_night_spring_96_1488_totter_(246_1146).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam01_drunken01_place03_night_spring_451_1524_totter_(86_986).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam01_drunken01_place03_night_spring_451_1524_totter_(86_986).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_summer_3995_6473_totter_(789_1689).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam03_drunken04_place02_night_summer_3995_6473_totter_(789_1689).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_spring_2279_3416_totter_(118_1018).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam01_drunken01_place03_night_spring_2279_3416_totter_(118_1018).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam01_drunken04_place03_night_summer_147_1942_totter_(447_1347).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-2_cam01_drunken04_place03_night_summer_147_1942_totter_(447_1347).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam01_drunken01_place03_night_winter_2378_3736_totter_(229_1129).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam01_drunken01_place03_night_winter_2378_3736_totter_(229_1129).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam02_drunken01_place03_night_winter_1776_2994_totter_(159_1059).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam02_drunken01_place03_night_winter_1776_2994_totter_(159_1059).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam03_drunken04_place03_night_spring_174_1098_totter_(12_912).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-6_cam03_drunken04_place03_night_spring_174_1098_totter_(12_912).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_summer_189_3078_totter_(994_1894).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam02_drunken03_place03_night_summer_189_3078_totter_(994_1894).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-5_cam03_drunken01_place03_night_spring_4998_6499_totter_(300_1200).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-5_cam03_drunken01_place03_night_spring_4998_6499_totter_(300_1200).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam01_drunken04_place03_night_winter_194_2250_totter_(578_1478).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-6_cam01_drunken04_place03_night_winter_194_2250_totter_(578_1478).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam02_drunken04_place03_night_summer_153_1969_totter_(458_1358).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-2_cam02_drunken04_place03_night_summer_153_1969_totter_(458_1358).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam03_drunken03_place02_night_spring_3786_4969_totter_(141_1041).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam03_drunken03_place02_night_spring_3786_4969_totter_(141_1041).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_spring_6701_7707_totter_(53_953).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam03_drunken04_place02_night_spring_6701_7707_totter_(53_953).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam01_drunken03_place02_night_summer_1227_2149_totter_(11_911).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam01_drunken03_place02_night_summer_1227_2149_totter_(11_911).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam02_drunken02_place01_night_winter_1813_2977_totter_(132_1032).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam02_drunken02_place01_night_winter_1813_2977_totter_(132_1032).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam01_drunken01_place03_night_spring_4990_6291_totter_(200_1100).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam01_drunken01_place03_night_spring_4990_6291_totter_(200_1100).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_winter_4185_5658_totter_(286_1186).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam02_drunken03_place03_night_winter_4185_5658_totter_(286_1186).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam02_drunken01_place03_night_spring_541_1523_totter_(41_941).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam02_drunken01_place03_night_spring_541_1523_totter_(41_941).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_winter_5752_6663_totter_(5_905).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam03_drunken01_place03_night_winter_5752_6663_totter_(5_905).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_spring_455_3546_totter_(1095_1995).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam02_drunken01_place03_night_spring_455_3546_totter_(1095_1995).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam01_drunken01_place03_night_spring_1333_3315_totter_(541_1441).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam01_drunken01_place03_night_spring_1333_3315_totter_(541_1441).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam01_drunken03_place03_night_winter_2716_3841_totter_(112_1012).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-2_cam01_drunken03_place03_night_winter_2716_3841_totter_(112_1012).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_spring_6807_8169_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam03_drunken01_place03_night_spring_6807_8169_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/229-2_cam03_drunken04_place03_night_summer_112_1036_totter_(12_912).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/229-2_cam03_drunken04_place03_night_summer_112_1036_totter_(12_912).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_winter_3804_6397_totter_(846_1746).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam03_drunken04_place02_night_winter_3804_6397_totter_(846_1746).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam01_drunken01_place03_night_summer_1842_3078_totter_(168_1068).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam01_drunken01_place03_night_summer_1842_3078_totter_(168_1068).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_spring_6577_7747_totter_(135_1035).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam02_drunken04_place02_night_spring_6577_7747_totter_(135_1035).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam03_drunken03_place03_night_winter_2733_3932_totter_(149_1049).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-2_cam03_drunken03_place03_night_winter_2733_3932_totter_(149_1049).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam02_drunken03_place02_night_winter_1594_3038_totter_(272_1172).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam02_drunken03_place02_night_winter_1594_3038_totter_(272_1172).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam02_drunken01_place03_night_summer_1704_3061_totter_(228_1128).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam02_drunken01_place03_night_summer_1704_3061_totter_(228_1128).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-5_cam03_drunken01_place03_night_summer_289_2180_totter_(495_1395).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-5_cam03_drunken01_place03_night_summer_289_2180_totter_(495_1395).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-2_cam03_drunken04_place03_night_winter_248_1762_totter_(307_1207).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-2_cam03_drunken04_place03_night_winter_248_1762_totter_(307_1207).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_summer_7295_8596_totter_(200_1100).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam02_drunken04_place02_night_summer_7295_8596_totter_(200_1100).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_spring_5106_6243_totter_(118_1018).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam02_drunken01_place03_night_spring_5106_6243_totter_(118_1018).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_spring_2435_3361_totter_(13_913).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam03_drunken03_place02_night_spring_2435_3361_totter_(13_913).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-4_cam03_drunken03_place02_night_spring_7371_8417_totter_(73_973).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-4_cam03_drunken03_place02_night_spring_7371_8417_totter_(73_973).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_spring_526_3559_totter_(1066_1966).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam03_drunken01_place03_night_spring_526_3559_totter_(1066_1966).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam03_drunken03_place03_night_summer_1644_2608_totter_(32_932).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-5_cam03_drunken03_place03_night_summer_1644_2608_totter_(32_932).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam01_drunken01_place03_night_spring_394_3564_totter_(1135_2035).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam01_drunken01_place03_night_spring_394_3564_totter_(1135_2035).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam01_drunken02_place01_night_winter_321_1389_totter_(84_984).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam01_drunken02_place01_night_winter_321_1389_totter_(84_984).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam01_drunken02_place01_night_winter_1935_2973_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam01_drunken02_place01_night_winter_1935_2973_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-6_cam01_drunken03_place03_night_winter_492_1623_totter_(115_1015).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-6_cam01_drunken03_place03_night_winter_492_1623_totter_(115_1015).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-2_cam01_drunken03_place03_night_summer_1136_4740_totter_(1352_2252).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-2_cam01_drunken03_place03_night_summer_1136_4740_totter_(1352_2252).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-1_cam03_drunken01_place03_night_winter_3994_5252_totter_(179_1079).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-1_cam03_drunken01_place03_night_winter_3994_5252_totter_(179_1079).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam02_drunken03_place03_night_summer_4248_5174_totter_(13_913).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam02_drunken03_place03_night_summer_4248_5174_totter_(13_913).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam03_drunken01_place03_night_spring_293_1245_totter_(26_926).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam03_drunken01_place03_night_spring_293_1245_totter_(26_926).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam03_drunken01_place03_night_summer_576_4476_totter_(1500_2400).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam03_drunken01_place03_night_summer_576_4476_totter_(1500_2400).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam03_drunken03_place03_night_winter_1622_2757_totter_(117_1017).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-5_cam03_drunken03_place03_night_winter_1622_2757_totter_(117_1017).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam01_drunken01_place03_night_summer_495_2457_totter_(531_1431).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam01_drunken01_place03_night_summer_495_2457_totter_(531_1431).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_summer_7333_8646_totter_(206_1106).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam01_drunken04_place02_night_summer_7333_8646_totter_(206_1106).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/217-6_cam02_drunken04_place03_night_spring_4527_5936_totter_(254_1154).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/217-6_cam02_drunken04_place03_night_spring_4527_5936_totter_(254_1154).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/216-1_cam01_drunken02_place01_night_summer_264_1413_totter_(124_1024).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/216-1_cam01_drunken02_place01_night_summer_264_1413_totter_(124_1024).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-3_cam01_drunken03_place03_night_spring_4528_5885_totter_(228_1128).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-3_cam01_drunken03_place03_night_spring_4528_5885_totter_(228_1128).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam01_drunken04_place02_night_spring_3310_4503_totter_(146_1046).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/623-2_cam01_drunken04_place02_night_spring_3310_4503_totter_(146_1046).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam01_drunken04_place02_night_winter_3801_6423_totter_(861_1761).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam01_drunken04_place02_night_winter_3801_6423_totter_(861_1761).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam02_drunken03_place03_night_winter_472_4543_totter_(1585_2485).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-2_cam02_drunken03_place03_night_winter_472_4543_totter_(1585_2485).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/278-4_cam01_drunken03_place03_night_winter_1478_3040_totter_(331_1231).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/278-4_cam01_drunken03_place03_night_winter_1478_3040_totter_(331_1231).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-2_cam03_drunken03_place02_night_spring_7702_8943_totter_(170_1070).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-2_cam03_drunken03_place02_night_spring_7702_8943_totter_(170_1070).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam02_drunken03_place03_night_spring_1378_2664_totter_(193_1093).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-2_cam02_drunken03_place03_night_spring_1378_2664_totter_(193_1093).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/209-3_cam02_drunken01_place03_night_summer_6040_8183_totter_(621_1521).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/209-3_cam02_drunken01_place03_night_summer_6040_8183_totter_(621_1521).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam03_drunken04_place02_night_spring_3372_5824_totter_(776_1676).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam03_drunken04_place02_night_spring_3372_5824_totter_(776_1676).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam03_drunken01_place03_night_winter_2260_3383_totter_(111_1011).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam03_drunken01_place03_night_winter_2260_3383_totter_(111_1011).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/233-6_cam02_drunken03_place02_night_spring_3793_4965_totter_(136_1036).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/233-6_cam02_drunken03_place02_night_spring_3793_4965_totter_(136_1036).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/214-2_cam02_drunken04_place02_night_winter_6974_8224_totter_(175_1075).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/214-2_cam02_drunken04_place02_night_winter_6974_8224_totter_(175_1075).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-2_cam03_drunken01_place03_night_spring_1440_3441_totter_(550_1450).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-2_cam03_drunken01_place03_night_spring_1440_3441_totter_(550_1450).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam02_drunken03_place03_night_winter_1634_2751_totter_(108_1008).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-5_cam02_drunken03_place03_night_winter_1634_2751_totter_(108_1008).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/218-4_cam03_drunken01_place03_night_winter_261_2243_totter_(541_1441).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/218-4_cam03_drunken01_place03_night_winter_261_2243_totter_(541_1441).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-2_cam01_drunken03_place03_night_winter_473_4519_totter_(1573_2473).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-2_cam01_drunken03_place03_night_winter_473_4519_totter_(1573_2473).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam01_drunken04_place02_night_summer_3701_5505_totter_(452_1352).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_croki_30초/623-2_cam01_drunken04_place02_night_summer_3701_5505_totter_(452_1352).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_croki_30초/212-5_cam02_drunken03_place03_night_summer_1671_2626_totter_(27_927).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_croki_0넣기/212-5_cam02_drunken03_place03_night_summer_1671_2626_totter_(27_927).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/abs_croki_30초'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/abs_croki_0넣기'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 및 'frame' 열 확인\n",
    "    if 'label' not in df.columns or 'frame' not in df.columns:\n",
    "        print(f\"'label' 또는 'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 데이터프레임을 'label' 열로 그룹화\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        frames_to_add = []  # 새로운 프레임 데이터 저장 리스트\n",
    "\n",
    "        # 10프레임 간격으로 선택\n",
    "        for frame in range(min_frame, max_frame + 1, 10):\n",
    "            # 선택한 프레임이 존재하는 경우 그대로 추가\n",
    "            if frame in group['frame'].values:\n",
    "                frames_to_add.append(group[group['frame'] == frame].iloc[0].to_dict())\n",
    "            else:\n",
    "                # 대체 프레임 탐색: 우선적으로 다음 프레임, 그다음 이전 프레임\n",
    "                replacement_frame = None\n",
    "                if (frame + 1) in group['frame'].values:\n",
    "                    replacement_frame = frame + 1\n",
    "                elif (frame - 1) in group['frame'].values:\n",
    "                    replacement_frame = frame - 1\n",
    "\n",
    "                if replacement_frame:\n",
    "                    frames_to_add.append(group[group['frame'] == replacement_frame].iloc[0].to_dict())\n",
    "                else:\n",
    "                    # 대체 프레임도 없으면 0으로 채운 데이터 추가\n",
    "                    empty_row = {col: 0 for col in group.columns if col != 'label'}\n",
    "                    empty_row['label'] = label\n",
    "                    empty_row['frame'] = frame\n",
    "                    frames_to_add.append(empty_row)\n",
    "\n",
    "        # 생성된 프레임 리스트를 데이터프레임으로 변환\n",
    "        new_df = pd.DataFrame(frames_to_add)\n",
    "        processed_dfs.append(new_df)\n",
    "\n",
    "    # 그룹별 처리된 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1009</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1076</td>\n",
       "      <td>1097</td>\n",
       "      <td>1041</td>\n",
       "      <td>1074</td>\n",
       "      <td>1192</td>\n",
       "      <td>1118</td>\n",
       "      <td>1150</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>981</td>\n",
       "      <td>718</td>\n",
       "      <td>1037</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>1083</td>\n",
       "      <td>1029</td>\n",
       "      <td>1069</td>\n",
       "      <td>1196</td>\n",
       "      <td>1119</td>\n",
       "      <td>1136</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>959</td>\n",
       "      <td>728</td>\n",
       "      <td>1026</td>\n",
       "      <td>...</td>\n",
       "      <td>1082</td>\n",
       "      <td>1077</td>\n",
       "      <td>1025</td>\n",
       "      <td>1073</td>\n",
       "      <td>1197</td>\n",
       "      <td>1111</td>\n",
       "      <td>1136</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>947</td>\n",
       "      <td>737</td>\n",
       "      <td>1016</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>1070</td>\n",
       "      <td>1032</td>\n",
       "      <td>1073</td>\n",
       "      <td>1194</td>\n",
       "      <td>1105</td>\n",
       "      <td>1144</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>943</td>\n",
       "      <td>739</td>\n",
       "      <td>1011</td>\n",
       "      <td>...</td>\n",
       "      <td>1074</td>\n",
       "      <td>1064</td>\n",
       "      <td>1043</td>\n",
       "      <td>1074</td>\n",
       "      <td>1187</td>\n",
       "      <td>1090</td>\n",
       "      <td>1164</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34575</th>\n",
       "      <td>887</td>\n",
       "      <td>2329</td>\n",
       "      <td>908</td>\n",
       "      <td>2333</td>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2378</td>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1327</td>\n",
       "      <td>2347</td>\n",
       "      <td>1297</td>\n",
       "      <td>2474</td>\n",
       "      <td>1468</td>\n",
       "      <td>2359</td>\n",
       "      <td>1432</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34576</th>\n",
       "      <td>897</td>\n",
       "      <td>2288</td>\n",
       "      <td>914</td>\n",
       "      <td>2290</td>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2335</td>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1332</td>\n",
       "      <td>2310</td>\n",
       "      <td>1310</td>\n",
       "      <td>2392</td>\n",
       "      <td>1473</td>\n",
       "      <td>2357</td>\n",
       "      <td>1421</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34577</th>\n",
       "      <td>907</td>\n",
       "      <td>2254</td>\n",
       "      <td>947</td>\n",
       "      <td>2258</td>\n",
       "      <td>932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2307</td>\n",
       "      <td>931</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1351</td>\n",
       "      <td>2317</td>\n",
       "      <td>1304</td>\n",
       "      <td>2290</td>\n",
       "      <td>1531</td>\n",
       "      <td>2357</td>\n",
       "      <td>1417</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34578</th>\n",
       "      <td>917</td>\n",
       "      <td>2228</td>\n",
       "      <td>967</td>\n",
       "      <td>2234</td>\n",
       "      <td>953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2285</td>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1377</td>\n",
       "      <td>2274</td>\n",
       "      <td>1334</td>\n",
       "      <td>2297</td>\n",
       "      <td>1525</td>\n",
       "      <td>2323</td>\n",
       "      <td>1458</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>927</td>\n",
       "      <td>2170</td>\n",
       "      <td>984</td>\n",
       "      <td>2178</td>\n",
       "      <td>972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2231</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1389</td>\n",
       "      <td>2223</td>\n",
       "      <td>1366</td>\n",
       "      <td>2300</td>\n",
       "      <td>1521</td>\n",
       "      <td>2272</td>\n",
       "      <td>1487</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34580 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame    x1   y1    x2   y2  x3  y3    x4   y4    x5  ...   y14   x15  \\\n",
       "0        356     0    0     0    0   0   0  1009  698     0  ...  1076  1097   \n",
       "1        366     0    0     0    0   0   0   981  718  1037  ...  1081  1083   \n",
       "2        376     0    0     0    0   0   0   959  728  1026  ...  1082  1077   \n",
       "3        386     0    0     0    0   0   0   947  737  1016  ...  1081  1070   \n",
       "4        396     0    0     0    0   0   0   943  739  1011  ...  1074  1064   \n",
       "...      ...   ...  ...   ...  ...  ..  ..   ...  ...   ...  ...   ...   ...   \n",
       "34575    887  2329  908  2333  893   0   0  2378  891     0  ...  1327  2347   \n",
       "34576    897  2288  914  2290  899   0   0  2335  893     0  ...  1332  2310   \n",
       "34577    907  2254  947  2258  932   0   0  2307  931     0  ...  1351  2317   \n",
       "34578    917  2228  967  2234  953   0   0  2285  956     0  ...  1377  2274   \n",
       "34579    927  2170  984  2178  972   0   0  2231  978     0  ...  1389  2223   \n",
       "\n",
       "        y15   x16   y16   x17   y17       label  y  \\\n",
       "0      1041  1074  1192  1118  1150  ID: 2613.0  1   \n",
       "1      1029  1069  1196  1119  1136  ID: 2613.0  1   \n",
       "2      1025  1073  1197  1111  1136  ID: 2613.0  1   \n",
       "3      1032  1073  1194  1105  1144  ID: 2613.0  1   \n",
       "4      1043  1074  1187  1090  1164  ID: 2613.0  1   \n",
       "...     ...   ...   ...   ...   ...         ... ..   \n",
       "34575  1297  2474  1468  2359  1432  ID: 1416.0  1   \n",
       "34576  1310  2392  1473  2357  1421  ID: 1416.0  1   \n",
       "34577  1304  2290  1531  2357  1417  ID: 1416.0  1   \n",
       "34578  1334  2297  1525  2323  1458  ID: 1416.0  1   \n",
       "34579  1366  2300  1521  2272  1487  ID: 1416.0  1   \n",
       "\n",
       "                                                FILENAME  \n",
       "0      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "1      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "2      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "3      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "4      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "...                                                  ...  \n",
       "34575  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34576  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34577  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34578  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34579  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "\n",
       "[34580 rows x 38 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "croki_data = pd.DataFrame()\n",
    "croki_path = '/home/alpaco/project/drunk_prj/data/abs_croki_0넣기'\n",
    "for vid in os.listdir(croki_path):\n",
    "    csv_path = os.path.join(croki_path,vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 1\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    croki_data = pd.concat([croki_data,tmp_csv],ignore_index=True)\n",
    "croki_data\n",
    "#34580줄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102370</th>\n",
       "      <td>142</td>\n",
       "      <td>403.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>...</td>\n",
       "      <td>351.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102371</th>\n",
       "      <td>143</td>\n",
       "      <td>401.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>...</td>\n",
       "      <td>368.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102372</th>\n",
       "      <td>144</td>\n",
       "      <td>401.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>...</td>\n",
       "      <td>378.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102373</th>\n",
       "      <td>145</td>\n",
       "      <td>410.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>...</td>\n",
       "      <td>377.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102374</th>\n",
       "      <td>146</td>\n",
       "      <td>428.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>...</td>\n",
       "      <td>386.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102375 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frame     x1     y1     x2     y2     x3     y3   x4   y4     x5  ...  \\\n",
       "0          53    0.0    0.0    0.0    0.0    0.0    0.0  0.0  0.0    0.0  ...   \n",
       "1          54    0.0    0.0    0.0    0.0    0.0    0.0  0.0  0.0    0.0  ...   \n",
       "2          55    0.0    0.0    0.0    0.0    0.0    0.0  0.0  0.0    0.0  ...   \n",
       "3          56    0.0    0.0    0.0    0.0    0.0    0.0  0.0  0.0    0.0  ...   \n",
       "4          57    0.0    0.0    0.0    0.0    0.0    0.0  0.0  0.0    0.0  ...   \n",
       "...       ...    ...    ...    ...    ...    ...    ...  ...  ...    ...  ...   \n",
       "102370    142  403.0  251.0  405.0  247.0  399.0  248.0  0.0  0.0  391.0  ...   \n",
       "102371    143  401.0  254.0  403.0  250.0  396.0  251.0  0.0  0.0  388.0  ...   \n",
       "102372    144  401.0  264.0  403.0  260.0  396.0  260.0  0.0  0.0  386.0  ...   \n",
       "102373    145  410.0  266.0  412.0  263.0  405.0  263.0  0.0  0.0  394.0  ...   \n",
       "102374    146  428.0  269.0  429.0  265.0  422.0  266.0  0.0  0.0  411.0  ...   \n",
       "\n",
       "          y14    x15    y15    x16    y16    x17    y17        label  y  \\\n",
       "0         0.0    0.0    0.0    0.0    0.0    0.0    0.0   ID: 1585.0  0   \n",
       "1         0.0    0.0    0.0    0.0    0.0    0.0    0.0   ID: 1585.0  0   \n",
       "2         0.0    0.0    0.0    0.0    0.0    0.0    0.0   ID: 1585.0  0   \n",
       "3         0.0    0.0    0.0    0.0    0.0    0.0    0.0   ID: 1585.0  0   \n",
       "4         0.0    0.0    0.0    0.0    0.0    0.0    0.0   ID: 1585.0  0   \n",
       "...       ...    ...    ...    ...    ...    ...    ...          ... ..   \n",
       "102370  351.0  420.0  356.0  432.0  381.0  430.0  385.0  ID: 19394.0  0   \n",
       "102371  368.0  408.0  372.0    0.0    0.0  416.0  399.0  ID: 19394.0  0   \n",
       "102372  378.0  422.0  383.0    0.0    0.0    0.0    0.0  ID: 19394.0  0   \n",
       "102373  377.0  412.0  384.0  444.0  411.0  426.0  418.0  ID: 19394.0  0   \n",
       "102374  386.0  425.0  391.0  458.0  425.0  427.0  429.0  ID: 19394.0  0   \n",
       "\n",
       "                                               FILENAME  \n",
       "0       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "1       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "2       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "3       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "4       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "...                                                 ...  \n",
       "102370  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102371  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102372  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102373  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102374  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "\n",
       "[102375 rows x 38 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "normal_data = pd.DataFrame()\n",
    "normal_path= '/home/alpaco/project/drunk_prj/data/abs_normal_0넣기'\n",
    "for vid in os.listdir(normal_path):\n",
    "    csv_path = os.path.join(normal_path,vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 0\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    normal_data = pd.concat([normal_data,tmp_csv],ignore_index=True)\n",
    "normal_data\n",
    "#102375 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Combined = pd.concat([normal_data,croki_data],ignore_index=True)\n",
    "Combined\n",
    "\n",
    "columns_to_convert = Combined.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID: 1585.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102370</th>\n",
       "      <td>142</td>\n",
       "      <td>403.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>...</td>\n",
       "      <td>351.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102371</th>\n",
       "      <td>143</td>\n",
       "      <td>401.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>...</td>\n",
       "      <td>368.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102372</th>\n",
       "      <td>144</td>\n",
       "      <td>401.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>...</td>\n",
       "      <td>378.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102373</th>\n",
       "      <td>145</td>\n",
       "      <td>410.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>...</td>\n",
       "      <td>377.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102374</th>\n",
       "      <td>146</td>\n",
       "      <td>428.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>...</td>\n",
       "      <td>386.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>ID: 19394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102375 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frame     x1     y1     x2     y2     x3     y3   x4   y4     x5  ...  \\\n",
       "0          53    NaN    NaN    NaN    NaN    NaN    NaN  NaN  NaN    NaN  ...   \n",
       "1          54    NaN    NaN    NaN    NaN    NaN    NaN  NaN  NaN    NaN  ...   \n",
       "2          55    NaN    NaN    NaN    NaN    NaN    NaN  NaN  NaN    NaN  ...   \n",
       "3          56    NaN    NaN    NaN    NaN    NaN    NaN  NaN  NaN    NaN  ...   \n",
       "4          57    NaN    NaN    NaN    NaN    NaN    NaN  NaN  NaN    NaN  ...   \n",
       "...       ...    ...    ...    ...    ...    ...    ...  ...  ...    ...  ...   \n",
       "102370    142  403.0  251.0  405.0  247.0  399.0  248.0  0.0  0.0  391.0  ...   \n",
       "102371    143  401.0  254.0  403.0  250.0  396.0  251.0  0.0  0.0  388.0  ...   \n",
       "102372    144  401.0  264.0  403.0  260.0  396.0  260.0  0.0  0.0  386.0  ...   \n",
       "102373    145  410.0  266.0  412.0  263.0  405.0  263.0  0.0  0.0  394.0  ...   \n",
       "102374    146  428.0  269.0  429.0  265.0  422.0  266.0  0.0  0.0  411.0  ...   \n",
       "\n",
       "          y14    x15    y15    x16    y16    x17    y17        label  y  \\\n",
       "0         NaN    NaN    NaN    NaN    NaN    NaN    NaN   ID: 1585.0  0   \n",
       "1         NaN    NaN    NaN    NaN    NaN    NaN    NaN   ID: 1585.0  0   \n",
       "2         NaN    NaN    NaN    NaN    NaN    NaN    NaN   ID: 1585.0  0   \n",
       "3         NaN    NaN    NaN    NaN    NaN    NaN    NaN   ID: 1585.0  0   \n",
       "4         NaN    NaN    NaN    NaN    NaN    NaN    NaN   ID: 1585.0  0   \n",
       "...       ...    ...    ...    ...    ...    ...    ...          ... ..   \n",
       "102370  351.0  420.0  356.0  432.0  381.0  430.0  385.0  ID: 19394.0  0   \n",
       "102371  368.0  408.0  372.0    0.0    0.0  416.0  399.0  ID: 19394.0  0   \n",
       "102372  378.0  422.0  383.0    0.0    0.0    0.0    0.0  ID: 19394.0  0   \n",
       "102373  377.0  412.0  384.0  444.0  411.0  426.0  418.0  ID: 19394.0  0   \n",
       "102374  386.0  425.0  391.0  458.0  425.0  427.0  429.0  ID: 19394.0  0   \n",
       "\n",
       "                                               FILENAME  \n",
       "0       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "1       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "2       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "3       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "4       C_32_7_smp_su_09-11_11-01-00_c_for_DF2_(53_143)  \n",
       "...                                                 ...  \n",
       "102370  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102371  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102372  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102373  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "102374  C_32_8_smp_su_09-11_11-27-00_a_for_DF2_(56_146)  \n",
       "\n",
       "[102375 rows x 38 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Combined = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_interpol.csv')\n",
    "Combined.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(Combined, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  47%|████▋     | 5646/12085 [00:18<00:20, 312.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m--> 100\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    103\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 분류용\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 및 검증 함수\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = evaluate(model, valid_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'abs90frame000_LSTM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_900=[]\n",
    "\n",
    "for video in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1_1/Abs'):\n",
    "    start,end = int(video.split('_')[-3]),int((video.split('_')[-2]))\n",
    "    total = end- start\n",
    "    if total >=900:\n",
    "        video_name = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1_1/Abs',video)\n",
    "        test_900.append([video_name,total//2-450,total//2+450])         \n",
    "for video in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1/Abs'):\n",
    "    start,end = int(video.split('_')[-3]),int((video.split('_')[-2]))\n",
    "    total = end- start\n",
    "    if total >=900:\n",
    "        video_name = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1/Abs',video)\n",
    "        test_900.append([video_name,total//2-450,total//2+450])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "저장 완료: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/abs_test_30초\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in test_900:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 'y' 열 값 확인 및 제외 처리\n",
    "    if 'y' in filtered_df.columns and filtered_df['y'].isnull().all():\n",
    "        filtered_df = filtered_df.drop(columns=['y'])\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-2_cam02_drunken04_place03_night_spring_717_2898_totter_(640_1540).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-6_cam01_drunken04_place03_night_summer_4909_6240_totter_(215_1115).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-3_cam01_drunken04_place03_night_spring_1930_3718_totter_(444_1344).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-1_cam01_drunken04_place03_night_spring_2463_3550_totter_(93_993).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/616-6_cam02_drunken03_place01_day_spring_135_1644_totter_(304_1204).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam01_drunken04_place03_night_summer_1638_5279_totter_(1370_2270).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-7_cam01_drunken01_place03_night_spring_4584_5996_totter_(256_1156).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-12_cam01_drunken04_place03_night_summer_3249_4640_totter_(245_1145).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam02_drunken04_place03_night_spring_1145_3151_totter_(553_1453).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-1_cam01_drunken04_place03_night_summer_4687_6296_totter_(354_1254).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-3_cam01_drunken04_place03_night_spring_991_4884_totter_(1496_2396).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-1_cam02_drunken04_place03_night_spring_2435_3438_totter_(51_951).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/616-6_cam01_drunken03_place01_day_summer_1545_2711_totter_(133_1033).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-4_cam02_drunken04_place03_night_summer_1189_2302_totter_(106_1006).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/616-5_cam02_drunken03_place01_day_spring_1110_2168_totter_(79_979).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-5_cam01_drunken04_place03_night_summer_1941_4245_totter_(702_1602).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam02_drunken04_place03_night_spring_1893_5864_totter_(1535_2435).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-1_cam02_drunken04_place03_night_summer_4695_6314_totter_(359_1259).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-3_cam02_drunken04_place03_night_summer_1075_3748_totter_(886_1786).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-9_cam01_drunken01_place03_night_spring_3807_4969_totter_(131_1031).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-4_cam02_drunken04_place03_night_summer_1749_5289_totter_(1320_2220).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-12_cam02_drunken04_place03_night_summer_3183_4682_totter_(299_1199).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-8_cam01_drunken01_place03_night_spring_7528_8836_totter_(204_1104).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-7_cam02_drunken01_place03_night_spring_4421_5879_totter_(279_1179).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-5_cam01_drunken04_place03_night_summer_1739_2983_totter_(172_1072).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-5_cam02_drunken04_place03_night_summer_1756_2979_totter_(161_1061).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-5_cam02_drunken04_place03_night_summer_1888_4195_totter_(703_1603).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-3_cam01_drunken04_place03_night_summer_2038_3068_totter_(65_965).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-6_cam01_drunken04_place03_night_summer_2401_3763_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-12_cam01_drunken04_place03_night_spring_2686_3878_totter_(146_1046).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-3_cam02_drunken04_place03_night_summer_2157_3111_totter_(27_927).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-4_cam01_drunken04_place03_night_summer_4583_5622_totter_(69_969).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/616-6_cam02_drunken03_place01_day_summer_396_2687_totter_(695_1595).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-9_cam02_drunken01_place03_night_spring_4006_5174_totter_(134_1034).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv\n",
      "처리된 데이터가 없습니다: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-1_cam01_drunken04_place03_night_summer_796_3118_totter_(711_1611).csv. 파일을 건너뜁니다.\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-8_cam01_drunken01_place03_night_spring_3115_4019_totter_(2_902).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-9_cam02_drunken01_place03_night_summer_7673_8654_totter_(40_940).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-3_cam01_drunken04_place03_night_summer_1082_3744_totter_(881_1781).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-2_cam01_drunken04_place03_night_summer_2175_3808_totter_(366_1266).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-3_cam02_drunken04_place03_night_spring_1021_4952_totter_(1515_2415).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-7_cam01_drunken01_place03_night_summer_5890_7544_totter_(377_1277).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-6_cam02_drunken04_place03_night_summer_1850_3133_totter_(191_1091).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-2_cam01_drunken04_place03_night_spring_786_2898_totter_(606_1506).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-10_cam02_drunken04_place03_night_spring_4675_6386_totter_(405_1305).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-12_cam02_drunken04_place03_night_spring_2688_3973_totter_(192_1092).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-1_cam02_drunken04_place03_night_summer_954_3092_totter_(619_1519).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-6_cam02_drunken04_place03_night_summer_2403_3765_totter_(231_1131).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-4_cam01_drunken04_place03_night_summer_1190_2284_totter_(97_997).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-2_cam02_drunken04_place03_night_summer_2209_3792_totter_(341_1241).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-8_cam02_drunken01_place03_night_summer_3382_5565_totter_(641_1541).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-7_cam02_drunken01_place03_night_summer_5929_7621_totter_(396_1296).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-8_cam02_drunken01_place03_night_spring_7555_8790_totter_(167_1067).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-8_cam02_drunken01_place03_night_spring_3083_4004_totter_(10_910).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-10_cam01_drunken04_place03_night_summer_4708_6432_totter_(412_1312).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-4_cam01_drunken04_place03_night_spring_4677_5595_totter_(9_909).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-8_cam01_drunken01_place03_night_summer_3170_5510_totter_(720_1620).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-6_cam01_drunken04_place03_night_summer_1877_3167_totter_(195_1095).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/617-4_cam02_drunken04_place03_night_summer_4590_5637_totter_(73_973).csv\n",
      "CSV 파일 읽기 성공: /home/alpaco/project/drunk_prj/data/abs_test_30초/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n",
      "처리 완료: /home/alpaco/project/drunk_prj/data/abs_test_0넣기3/618-2_cam02_drunken04_place03_night_spring_1808_3251_totter_(271_1171).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/abs_test_30초'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/abs_test_0넣기3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 및 'frame' 열 확인\n",
    "    if 'label' not in df.columns or 'frame' not in df.columns:\n",
    "        print(f\"'label' 또는 'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 데이터프레임을 'label' 열로 그룹화\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        frames_to_add = []  # 새로운 프레임 데이터 저장 리스트\n",
    "\n",
    "        # 10프레임 간격으로 선택\n",
    "        for frame in range(min_frame, max_frame + 1, 10):\n",
    "            # 선택한 프레임이 존재하는 경우 그대로 추가\n",
    "            if frame in group['frame'].values:\n",
    "                frames_to_add.append(group[group['frame'] == frame].iloc[0].to_dict())\n",
    "            else:\n",
    "                # 대체 프레임 탐색: 우선적으로 다음 프레임, 그다음 이전 프레임\n",
    "                replacement_frame = None\n",
    "                if (frame + 1) in group['frame'].values:\n",
    "                    replacement_frame = frame + 1\n",
    "                elif (frame - 1) in group['frame'].values:\n",
    "                    replacement_frame = frame - 1\n",
    "\n",
    "                if replacement_frame:\n",
    "                    frames_to_add.append(group[group['frame'] == replacement_frame].iloc[0].to_dict())\n",
    "                else:\n",
    "                    # 대체 프레임도 없으면 0으로 채운 데이터 추가\n",
    "                    empty_row = {col: 0 for col in group.columns if col != 'label'}\n",
    "                    empty_row['label'] = label\n",
    "                    empty_row['frame'] = frame\n",
    "                    frames_to_add.append(empty_row)\n",
    "\n",
    "        # 생성된 프레임 리스트를 데이터프레임으로 변환\n",
    "        new_df = pd.DataFrame(frames_to_add)\n",
    "        processed_dfs.append(new_df)\n",
    "\n",
    "    # 그룹별 처리된 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for vid in os.listdir('/home/alpaco/project/drunk_prj/data/abs_test_0넣기3'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/abs_test_0넣기3',vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    test_data = pd.concat([test_data,tmp_csv],ignore_index=True)\n",
    "    \n",
    "test_data = test_data.drop(['Unnamed: 37'],axis=1)\n",
    "#136051   \n",
    "\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = test_data[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_data[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "columns_to_convert = test_data.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "test_data[columns_to_convert] = test_data[columns_to_convert].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "test_x_seq,test_y_seq = create_sequences(test_data,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 90, 34)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1196996/3287355153.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/11_27alignremodel_LSTM.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLSTMModel(\n",
       "  (lstm): LSTM(34, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_x_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "loaded_model = BinaryLSTMModel(X_seq.shape[2],50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/11_27alignremodel_LSTM.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 32.47%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.31\n",
      "Recall: 0.98\n",
      "F1 Score: 0.47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "croki_data = pd.DataFrame()\n",
    "croki_path = '/home/alpaco/project/drunk_prj/data/abs_croki_0넣기'\n",
    "for vid in os.listdir(croki_path):\n",
    "    csv_path = os.path.join(croki_path,vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 1\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    croki_data = pd.concat([croki_data,tmp_csv],ignore_index=True)\n",
    "croki_data\n",
    "#34580줄\n",
    "\n",
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "normal_data = pd.DataFrame()\n",
    "normal_path= '/home/alpaco/project/drunk_prj/data/normal_0넣기'\n",
    "for vid in os.listdir(normal_path):\n",
    "    csv_path = os.path.join(normal_path,vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 0\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    normal_data = pd.concat([normal_data,tmp_csv],ignore_index=True)\n",
    "normal_data\n",
    "#102375 \n",
    "\n",
    "Combined = pd.concat([normal_data,croki_data],ignore_index=True)\n",
    "Combined\n",
    "\n",
    "columns_to_convert = Combined.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(Combined, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 분류용\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 및 검증 함수\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = evaluate(model, valid_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}')\n",
    "torch.save(model.state_dict(),'abs90frame000_LSTM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_900=[]\n",
    "\n",
    "for video in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1_1/Abs'):\n",
    "    start,end = int(video.split('_')[-3]),int((video.split('_')[-2]))\n",
    "    total = end- start\n",
    "    if total >=900:\n",
    "        video_name = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1_1/Abs',video)\n",
    "        test_900.append([video_name,total//2-450,total//2+450])         \n",
    "for video in os.listdir('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1/Abs'):\n",
    "    start,end = int(video.split('_')[-3]),int((video.split('_')[-2]))\n",
    "    total = end- start\n",
    "    if total >=900:\n",
    "        video_name = os.path.join('/home/alpaco/project/drunk_prj/data/comfirm_labeling/video1/Abs',video)\n",
    "        test_900.append([video_name,total//2-450,total//2+450])       \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 잘린 CSV 파일을 저장할 폴더 경로\n",
    "cutting_folder = \"/home/alpaco/project/drunk_prj/data/abs_test_30초\"\n",
    "os.makedirs(cutting_folder, exist_ok=True)\n",
    "\n",
    "# 작업 수행\n",
    "for entry in test_900:\n",
    "    csv_path, start_frame, end_frame = entry\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 열 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열에서 특정 범위의 데이터만 필터링\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    filtered_df = df[(df['frame'] >= start_frame) & (df['frame'] <= end_frame)]\n",
    "    \n",
    "    # 'y' 열 값 확인 및 제외 처리\n",
    "    if 'y' in filtered_df.columns and filtered_df['y'].isnull().all():\n",
    "        filtered_df = filtered_df.drop(columns=['y'])\n",
    "    \n",
    "    # 저장할 파일 이름 및 경로 생성\n",
    "    file_name = os.path.basename(csv_path).replace(\".csv\", f\"_({start_frame}_{end_frame}).csv\")\n",
    "    output_path = os.path.join(cutting_folder, file_name)\n",
    "\n",
    "    # 필터링된 데이터 저장\n",
    "    try:\n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        print(f\"저장 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {output_path}. 오류: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/abs_test_30초'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/abs_test_0넣기3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에서 최소, 최대 값 추출\n",
    "    file_name = csv_file.split(\".\")[0]\n",
    "    match = re.search(r'\\((\\d+)_(\\d+)\\)', file_name)\n",
    "    if not match:\n",
    "        print(f\"파일 이름에서 최소/최대 값을 찾을 수 없습니다: {file_name}\")\n",
    "        continue\n",
    "\n",
    "    min_frame, max_frame = map(int, match.groups())\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 및 'frame' 열 확인\n",
    "    if 'label' not in df.columns or 'frame' not in df.columns:\n",
    "        print(f\"'label' 또는 'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 데이터프레임을 'label' 열로 그룹화\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('label'):\n",
    "        frames_to_add = []  # 새로운 프레임 데이터 저장 리스트\n",
    "\n",
    "        # 10프레임 간격으로 선택\n",
    "        for frame in range(min_frame, max_frame + 1, 10):\n",
    "            # 선택한 프레임이 존재하는 경우 그대로 추가\n",
    "            if frame in group['frame'].values:\n",
    "                frames_to_add.append(group[group['frame'] == frame].iloc[0].to_dict())\n",
    "            else:\n",
    "                # 대체 프레임 탐색: 우선적으로 다음 프레임, 그다음 이전 프레임\n",
    "                replacement_frame = None\n",
    "                if (frame + 1) in group['frame'].values:\n",
    "                    replacement_frame = frame + 1\n",
    "                elif (frame - 1) in group['frame'].values:\n",
    "                    replacement_frame = frame - 1\n",
    "\n",
    "                if replacement_frame:\n",
    "                    frames_to_add.append(group[group['frame'] == replacement_frame].iloc[0].to_dict())\n",
    "                else:\n",
    "                    # 대체 프레임도 없으면 0으로 채운 데이터 추가\n",
    "                    empty_row = {col: 0 for col in group.columns if col != 'label'}\n",
    "                    empty_row['label'] = label\n",
    "                    empty_row['frame'] = frame\n",
    "                    frames_to_add.append(empty_row)\n",
    "\n",
    "        # 생성된 프레임 리스트를 데이터프레임으로 변환\n",
    "        new_df = pd.DataFrame(frames_to_add)\n",
    "        processed_dfs.append(new_df)\n",
    "\n",
    "    # 그룹별 처리된 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n",
    "\n",
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for vid in os.listdir('/home/alpaco/project/drunk_prj/data/abs_test_0넣기3'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/abs_test_0넣기3',vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['FILENAME'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    test_data = pd.concat([test_data,tmp_csv],ignore_index=True)\n",
    "    \n",
    "test_data = test_data.drop(['Unnamed: 37'],axis=1)\n",
    "#136051   \n",
    "\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = test_data[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_data[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "columns_to_convert = test_data.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "test_data[columns_to_convert] = test_data[columns_to_convert].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "test_x_seq,test_y_seq = create_sequences(test_data,sequence_length)\n",
    "test_x_seq.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_x_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "loaded_model = BinaryLSTMModel(X_seq.shape[2],50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초당 3프레임으로 변환된 비디오 저장 경로: /home/alpaco/project/drunk_prj/data/일반인/PXL_20241123_063503606.mp4/PXL_20241123_063503606_3fps.mp4\n"
     ]
    }
   ],
   "source": [
    "def convert_to_3fps(input_video_path, output_dir):\n",
    "    \"\"\"동영상을 3FPS로 변환하여 저장\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    output_fps = 3\n",
    "    frame_skip = int(input_fps / output_fps)\n",
    "    base_name = os.path.splitext(os.path.basename(input_video_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_3fps.mp4\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, output_fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % frame_skip == 0:\n",
    "            out.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    return output_path\n",
    "import os\n",
    "import cv2\n",
    "input_video_path = '/home/alpaco/project/drunk_prj/data/rsj_거리보행자/PXL_20241123_063503606.mp4'\n",
    "output_dir = '/home/alpaco/project/drunk_prj/data/일반인/PXL_20241123_063503606.mp4'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 초당 3프레임으로 변환\n",
    "converted_video_path = convert_to_3fps(input_video_path, output_dir)\n",
    "print(f\"초당 3프레임으로 변환된 비디오 저장 경로: {converted_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3FPS에서 30FPS로 변환된 비디오 저장 경로: /home/alpaco/project/drunk_prj/data/일반인/C_32_9_smp_su_09-11_12-50-00_a_aft_DF2_30fps.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def convert_3fps_to_30fps(input_video_path, output_dir):\n",
    "    \"\"\"3FPS 비디오를 30FPS로 확장\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # 출력 비디오 파라미터 설정\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    base_name = os.path.splitext(os.path.basename(input_video_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_30fps.mp4\")\n",
    "    \n",
    "    # 비디오 라이터 생성 (30FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30, (width, height))\n",
    "    \n",
    "    frames = []\n",
    "    # 모든 프레임 읽기\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # 각 원본 프레임 사이에 보간 프레임 추가\n",
    "    for i in range(len(frames) - 1):\n",
    "        out.write(frames[i])  # 원본 프레임\n",
    "        \n",
    "        # 9개의 보간 프레임 생성 (3FPS -> 30FPS)\n",
    "        for j in range(1, 10):\n",
    "            # 선형 보간\n",
    "            alpha = j / 10\n",
    "            interpolated_frame = cv2.addWeighted(frames[i], 1 - alpha, frames[i+1], alpha, 0)\n",
    "            out.write(interpolated_frame)\n",
    "    \n",
    "    # 마지막 프레임 추가\n",
    "    out.write(frames[-1])\n",
    "    \n",
    "    out.release()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# 사용 예시\n",
    "input_video_path = '/home/alpaco/project/drunk_prj/data/videofile/32.이동 행위/32-9 세 명이 걸어가는 행위/C_32_9_smp_su_09-11_12-50-00_a_aft_DF2.mp4'\n",
    "output_dir = '/home/alpaco/project/drunk_prj/data/일반인'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "converted_video_path = convert_3fps_to_30fps(input_video_path, output_dir)\n",
    "print(f\"3FPS에서 30FPS로 변환된 비디오 저장 경로: {converted_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1802 프레임, 2 개의 라벨 그룹 처리 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path = '/home/alpaco/project/drunk_prj/data/일반인/abs/PXL_20241123_063503606.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 라벨 그룹별 처리를 위한 함수\n",
    "def process_label_group(label_group_df):\n",
    "    first_frame = label_group_df['frame'].min()\n",
    "    frames_to_include = list(range(first_frame, first_frame + 901))\n",
    "    \n",
    "    # 새로운 데이터프레임 생성\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['frame'] = frames_to_include\n",
    "    new_df['label'] = label_group_df['label'].iloc[0]  # 해당 그룹의 고유 label\n",
    "    \n",
    "    # 다른 칼럼들 0으로 초기화\n",
    "    other_columns = [col for col in label_group_df.columns if col not in ['frame', 'label']]\n",
    "    for col in other_columns:\n",
    "        new_df[col] = 0\n",
    "    \n",
    "    # 원본 데이터에서 matching되는 행 복사\n",
    "    for _, row in label_group_df.iterrows():\n",
    "        if row['frame'] in frames_to_include:\n",
    "            mask = new_df['frame'] == row['frame']\n",
    "            \n",
    "            for col in other_columns:\n",
    "                new_df.loc[mask, col] = row[col]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# 라벨 그룹별로 분리\n",
    "df_with_label = df[df['label'] != 0]\n",
    "label_groups = df_with_label.groupby('label')\n",
    "\n",
    "# 최종 결과를 저장할 리스트\n",
    "final_dfs = []\n",
    "\n",
    "# 각 라벨 그룹 처리\n",
    "for label, label_group_df in label_groups:\n",
    "    processed_df = process_label_group(label_group_df)\n",
    "    final_dfs.append(processed_df)\n",
    "\n",
    "# 모든 처리된 데이터프레임 병합\n",
    "result_df = pd.concat(final_dfs, ignore_index=True)\n",
    "\n",
    "# 정렬 및 저장\n",
    "result_df = result_df.sort_values('frame')\n",
    "result_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"총 {len(result_df)} 프레임, {len(label_groups)} 개의 라벨 그룹 처리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 7개의 CSV 파일 생성 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path = '/home/alpaco/project/drunk_prj/data/일반인/abs_PXL_20241123_063503606.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 라벨 그룹별 처리를 위한 함수\n",
    "def process_label_group(label_group_df):\n",
    "    first_frame = label_group_df['frame'].min()\n",
    "    label = label_group_df['label'].iloc[0]\n",
    "    \n",
    "    # 10프레임 간격으로 총 90개의 프레임 생성\n",
    "    frames_to_include = [first_frame + i for i in range(90)]\n",
    "    \n",
    "    # 새로운 데이터프레임 생성\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['frame'] = frames_to_include\n",
    "    new_df['label'] = label\n",
    "    \n",
    "    # 다른 칼럼들 0으로 초기화\n",
    "    other_columns = [col for col in label_group_df.columns if col not in ['frame', 'label']]\n",
    "    for col in other_columns:\n",
    "        new_df[col] = 0\n",
    "    \n",
    "    # 원본 데이터에서 matching되는 행 복사\n",
    "    for _, row in label_group_df.iterrows():\n",
    "        if row['frame'] in frames_to_include:\n",
    "            mask = new_df['frame'] == row['frame']\n",
    "            \n",
    "            for col in other_columns:\n",
    "                new_df.loc[mask, col] = row[col]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# 라벨 그룹별로 분리\n",
    "df_with_label = df[df['label'] != 0]\n",
    "label_groups = df_with_label.groupby('label')\n",
    "\n",
    "# 최종 결과를 저장할 리스트\n",
    "final_dfs = []\n",
    "\n",
    "# 각 라벨 그룹 처리\n",
    "for label, label_group_df in label_groups:\n",
    "    processed_df = process_label_group(label_group_df)\n",
    "    final_dfs.append(processed_df)\n",
    "\n",
    "# 모든 처리된 데이터프레임 병합\n",
    "result_df = pd.concat(final_dfs, ignore_index=True)\n",
    "\n",
    "# 결과 저장을 위한 디렉토리 생성\n",
    "output_dir = '/home/alpaco/project/drunk_prj/data/일반인/abs'\n",
    "\n",
    "# 각 새로운 CSV 파일 저장\n",
    "for i, group_df in enumerate(final_dfs, 1):\n",
    "    output_filename = f'processed_{group_df[\"label\"].iloc[0]}_{group_df[\"frame\"].iloc[0]}.csv'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    group_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"총 {len(final_dfs)}개의 CSV 파일 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>...</th>\n",
       "      <th>y13</th>\n",
       "      <th>x14</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>ID: 123.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206</td>\n",
       "      <td>ID: 123.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1868</td>\n",
       "      <td>482</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1884</td>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>ID: 123.0</td>\n",
       "      <td>1876</td>\n",
       "      <td>262</td>\n",
       "      <td>1886</td>\n",
       "      <td>256</td>\n",
       "      <td>1875</td>\n",
       "      <td>253</td>\n",
       "      <td>1911</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>432</td>\n",
       "      <td>1916</td>\n",
       "      <td>509</td>\n",
       "      <td>1829</td>\n",
       "      <td>502</td>\n",
       "      <td>1920</td>\n",
       "      <td>580</td>\n",
       "      <td>1795</td>\n",
       "      <td>574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>226</td>\n",
       "      <td>ID: 123.0</td>\n",
       "      <td>1837</td>\n",
       "      <td>264</td>\n",
       "      <td>1847</td>\n",
       "      <td>258</td>\n",
       "      <td>1833</td>\n",
       "      <td>255</td>\n",
       "      <td>1865</td>\n",
       "      <td>266</td>\n",
       "      <td>...</td>\n",
       "      <td>431</td>\n",
       "      <td>1838</td>\n",
       "      <td>508</td>\n",
       "      <td>1792</td>\n",
       "      <td>511</td>\n",
       "      <td>1895</td>\n",
       "      <td>559</td>\n",
       "      <td>1792</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>236</td>\n",
       "      <td>ID: 123.0</td>\n",
       "      <td>1796</td>\n",
       "      <td>258</td>\n",
       "      <td>1805</td>\n",
       "      <td>253</td>\n",
       "      <td>1790</td>\n",
       "      <td>251</td>\n",
       "      <td>1820</td>\n",
       "      <td>260</td>\n",
       "      <td>...</td>\n",
       "      <td>421</td>\n",
       "      <td>1765</td>\n",
       "      <td>491</td>\n",
       "      <td>1773</td>\n",
       "      <td>501</td>\n",
       "      <td>1779</td>\n",
       "      <td>577</td>\n",
       "      <td>1806</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>875</td>\n",
       "      <td>ID: 7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>885</td>\n",
       "      <td>ID: 7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>895</td>\n",
       "      <td>ID: 7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>905</td>\n",
       "      <td>ID: 7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>915</td>\n",
       "      <td>ID: 7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame      label    x1   y1    x2   y2    x3   y3    x4   y4  ...  y13  \\\n",
       "0      196  ID: 123.0     0    0     0    0     0    0     0    0  ...    0   \n",
       "1      206  ID: 123.0     0    0     0    0     0    0     0    0  ...  416   \n",
       "2      216  ID: 123.0  1876  262  1886  256  1875  253  1911  265  ...  432   \n",
       "3      226  ID: 123.0  1837  264  1847  258  1833  255  1865  266  ...  431   \n",
       "4      236  ID: 123.0  1796  258  1805  253  1790  251  1820  260  ...  421   \n",
       "..     ...        ...   ...  ...   ...  ...   ...  ...   ...  ...  ...  ...   \n",
       "175    875    ID: 7.0     0    0     0    0     0    0     0    0  ...    0   \n",
       "176    885    ID: 7.0     0    0     0    0     0    0     0    0  ...    0   \n",
       "177    895    ID: 7.0     0    0     0    0     0    0     0    0  ...    0   \n",
       "178    905    ID: 7.0     0    0     0    0     0    0     0    0  ...    0   \n",
       "179    915    ID: 7.0     0    0     0    0     0    0     0    0  ...    0   \n",
       "\n",
       "      x14  y14   x15  y15   x16  y16   x17  y17  y  \n",
       "0       0    0     0    0     0    0     0    0  0  \n",
       "1       0    0  1868  482     0    0  1884  553  0  \n",
       "2    1916  509  1829  502  1920  580  1795  574  0  \n",
       "3    1838  508  1792  511  1895  559  1792  582  0  \n",
       "4    1765  491  1773  501  1779  577  1806  580  0  \n",
       "..    ...  ...   ...  ...   ...  ...   ...  ... ..  \n",
       "175     0    0     0    0     0    0     0    0  0  \n",
       "176     0    0     0    0     0    0     0    0  0  \n",
       "177     0    0     0    0     0    0     0    0  0  \n",
       "178     0    0     0    0     0    0     0    0  0  \n",
       "179     0    0     0    0     0    0     0    0  0  \n",
       "\n",
       "[180 rows x 37 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 크로마키 영상\n",
    "import os\n",
    "import pandas as pd\n",
    "normal_test_data = pd.DataFrame()\n",
    "\n",
    "for vid in os.listdir('/home/alpaco/project/drunk_prj/data/일반인/abs/processed'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/일반인/abs/processed',vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y']=0\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    normal_test_data = pd.concat([normal_test_data,tmp_csv],ignore_index=True)\n",
    "normal_test_data\n",
    "#136051   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = normal_test_data[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "normal_test_data[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "columns_to_convert = normal_test_data.columns.difference(['label'])\n",
    "\n",
    "# float으로 변환\n",
    "normal_test_data[columns_to_convert] = normal_test_data[columns_to_convert].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame','label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(0,len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "test_x_seq,test_y_seq = create_sequences(normal_test_data,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1172931/1480221517.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_x_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "loaded_model = BinaryLSTMModel(X_seq.shape[2],50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_percen=[]\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        percen = torch.sigmoid(outputs).cpu()\n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        all_percen.extend(percen)\n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.8951]), tensor([0.9451])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_percen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 받아서 내가 해야하는 일은\n",
    "1. abs 기반으로 진행했을 때, 명치 기반으로 centering진행해서 scaling 진행후 모델 만들어 볼 것 x11,x12 에서 진행\n",
    "2. 지금 90프레임 기반으로 진행하고 있는데, 30 or 45프레임으로 sequence만들어서 그 중에서 몇 개 이상이 0일 경우 sequence안만들게 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/normal_hip_align'\n",
    "os.makedirs(output_folder,exist_ok=True)\n",
    "for name in os.listdir('/home/alpaco/project/drunk_prj/data/normal_processed_csv'):\n",
    "    csvpath =os.path.join('/home/alpaco/project/drunk_prj/data/normal_processed_csv',name)\n",
    "    data = pd.read_csv(csvpath)\n",
    "    \n",
    "    keypoint_columns = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "\n",
    "    # 정렬된 데이터를 저장할 리스트\n",
    "    aligned_data = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        # 중심점 계산\n",
    "        center_x = (row['x11'] + row['x12']) // 2\n",
    "        center_y = (row['y11'] + row['y12']) // 2\n",
    "        \n",
    "        # 기존 데이터에서 중심점 빼기\n",
    "        aligned_row = row.copy()\n",
    "        for i in range(1, 18):\n",
    "            aligned_row[f'x{i}'] = row[f'x{i}'] - center_x\n",
    "            aligned_row[f'y{i}'] = row[f'y{i}'] - center_y\n",
    "\n",
    "        aligned_data.append(aligned_row)\n",
    "\n",
    "    # 정렬된 데이터 프레임 생성\n",
    "    aligned_df = pd.DataFrame(aligned_data)\n",
    "    \n",
    "    output_csv_path = os.path.join(output_folder, name)\n",
    "\n",
    "    # CSV 저장\n",
    "    aligned_df.to_csv(output_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences_with_filter(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "\n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['filename', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'filename', 'label', 'y'], errors='ignore').values  \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "\n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            \n",
    "            # 34개의 값이 모두 0인 프레임의 개수 계산\n",
    "            zero_frame_count = np.sum(np.all(x == 0, axis=1))\n",
    "            \n",
    "            # 조건: 0으로만 이루어진 프레임이 30개 이상이면 해당 시퀀스를 제외\n",
    "            if zero_frame_count < 30:\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 2265.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 2265.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 2265.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 2265.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 2265.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40468</th>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40469</th>\n",
       "      <td>87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40470</th>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40471</th>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40472</th>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40473 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame   x1   y1   x2   y2   x3   y3   x4   y4   x5  ...  y14  x15  y15  \\\n",
       "0          1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1          2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2          3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3          4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4          5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "40468     86  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "40469     87  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "40470     88  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "40471     89  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "40472     90  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       x16  y16  x17  y17       label  y  \\\n",
       "0      0.0  0.0  0.0  0.0  ID: 2265.0  0   \n",
       "1      0.0  0.0  0.0  0.0  ID: 2265.0  0   \n",
       "2      0.0  0.0  0.0  0.0  ID: 2265.0  0   \n",
       "3      0.0  0.0  0.0  0.0  ID: 2265.0  0   \n",
       "4      0.0  0.0  0.0  0.0  ID: 2265.0  0   \n",
       "...    ...  ...  ...  ...         ... ..   \n",
       "40468  0.0  0.0  0.0  0.0           4  0   \n",
       "40469  0.0  0.0  0.0  0.0           4  0   \n",
       "40470  0.0  0.0  0.0  0.0           4  0   \n",
       "40471  0.0  0.0  0.0  0.0           4  0   \n",
       "40472  0.0  0.0  0.0  0.0           4  0   \n",
       "\n",
       "                                          filename  \n",
       "0      C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv  \n",
       "1      C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv  \n",
       "2      C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv  \n",
       "3      C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv  \n",
       "4      C_32_10_smp_su_09-11_14-06-00_a_aft_DF2.csv  \n",
       "...                                            ...  \n",
       "40468   C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv  \n",
       "40469   C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv  \n",
       "40470   C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv  \n",
       "40471   C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv  \n",
       "40472   C_32_7_smp_su_09-11_10-47-00_a_for_DF2.csv  \n",
       "\n",
       "[40473 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반인 /home/alpaco/project/drunk_prj/data/normal_processed_csv\n",
    "import os \n",
    "import pandas as pd\n",
    "Normal_Frame = pd.DataFrame()\n",
    "for name in os.listdir('/home/alpaco/project/drunk_prj/data/normal_hip_align'):\n",
    "    file_path = os.path.join('/home/alpaco/project/drunk_prj/data/normal_hip_align',name)\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['y']=0\n",
    "    num_cols = data.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    data[num_cols] = data[num_cols].clip(lower=0)\n",
    "    Normal_Frame = pd.concat([Normal_Frame,data],ignore_index=True)\n",
    "Normal_Frame\n",
    "#40473  줄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>482</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>606</td>\n",
       "      <td>570</td>\n",
       "      <td>571</td>\n",
       "      <td>547</td>\n",
       "      <td>722</td>\n",
       "      <td>591</td>\n",
       "      <td>680</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>242</td>\n",
       "      <td>49</td>\n",
       "      <td>190</td>\n",
       "      <td>35</td>\n",
       "      <td>357</td>\n",
       "      <td>85</td>\n",
       "      <td>297</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>248</td>\n",
       "      <td>63</td>\n",
       "      <td>191</td>\n",
       "      <td>59</td>\n",
       "      <td>363</td>\n",
       "      <td>97</td>\n",
       "      <td>302</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>247</td>\n",
       "      <td>54</td>\n",
       "      <td>198</td>\n",
       "      <td>57</td>\n",
       "      <td>360</td>\n",
       "      <td>89</td>\n",
       "      <td>310</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>241</td>\n",
       "      <td>53</td>\n",
       "      <td>210</td>\n",
       "      <td>63</td>\n",
       "      <td>354</td>\n",
       "      <td>79</td>\n",
       "      <td>331</td>\n",
       "      <td>ID: 2613.0</td>\n",
       "      <td>1</td>\n",
       "      <td>209-5_cam02_drunken01_place03_night_spring_503...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34575</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>101</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34576</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>61</td>\n",
       "      <td>309</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34577</th>\n",
       "      <td>907</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>41</td>\n",
       "      <td>221</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34578</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>311</td>\n",
       "      <td>41</td>\n",
       "      <td>244</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>62</td>\n",
       "      <td>297</td>\n",
       "      <td>34</td>\n",
       "      <td>263</td>\n",
       "      <td>ID: 1416.0</td>\n",
       "      <td>1</td>\n",
       "      <td>212-5_cam02_drunken03_place03_night_summer_167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34580 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame  x1  y1  x2  y2  x3  y3   x4   y4  x5  ...  y14  x15  y15  x16  \\\n",
       "0        356   0   0   0   0   0   0  482  228   0  ...  606  570  571  547   \n",
       "1        366   0   0   0   0   0   0    0    0   3  ...  242   49  190   35   \n",
       "2        376   0   0   0   0   0   0    0    0  12  ...  248   63  191   59   \n",
       "3        386   0   0   0   0   0   0    0    0   0  ...  247   54  198   57   \n",
       "4        396   0   0   0   0   0   0    0    0   0  ...  241   53  210   63   \n",
       "...      ...  ..  ..  ..  ..  ..  ..  ...  ...  ..  ...  ...  ...  ...  ...   \n",
       "34575    887   0   0   0   0   0   0    5    0   0  ...  171    0  141  101   \n",
       "34576    897   0   0   0   0   0   0    4    0   0  ...  168    0  146   61   \n",
       "34577    907   0   0   0   0   0   0    0    0   0  ...  155    1  108    0   \n",
       "34578    917   0   0   0   0   0   0    3    0   0  ...  163    0  120   15   \n",
       "34579    927   0   0   0   0   0   0    0    0   0  ...  165    0  142   62   \n",
       "\n",
       "       y16  x17  y17       label  y  \\\n",
       "0      722  591  680  ID: 2613.0  1   \n",
       "1      357   85  297  ID: 2613.0  1   \n",
       "2      363   97  302  ID: 2613.0  1   \n",
       "3      360   89  310  ID: 2613.0  1   \n",
       "4      354   79  331  ID: 2613.0  1   \n",
       "...    ...  ...  ...         ... ..   \n",
       "34575  312    0  276  ID: 1416.0  1   \n",
       "34576  309   26  257  ID: 1416.0  1   \n",
       "34577  335   41  221  ID: 1416.0  1   \n",
       "34578  311   41  244  ID: 1416.0  1   \n",
       "34579  297   34  263  ID: 1416.0  1   \n",
       "\n",
       "                                                filename  \n",
       "0      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "1      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "2      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "3      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "4      209-5_cam02_drunken01_place03_night_spring_503...  \n",
       "...                                                  ...  \n",
       "34575  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34576  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34577  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34578  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "34579  212-5_cam02_drunken03_place03_night_summer_167...  \n",
       "\n",
       "[34580 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반인 /home/alpaco/project/drunk_prj/data/normal_processed_csv\n",
    "import os \n",
    "import pandas as pd\n",
    "Normal_Frame = pd.DataFrame()\n",
    "for name in os.listdir('/home/alpaco/project/drunk_prj/data/normal_hip_align'):\n",
    "    file_path = os.path.join('/home/alpaco/project/drunk_prj/data/normal_hip_align',name)\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['y']=0\n",
    "    num_cols = data.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    data[num_cols] = data[num_cols].clip(lower=0)\n",
    "    Normal_Frame = pd.concat([Normal_Frame,data],ignore_index=True)\n",
    "    \n",
    "Croki_Frame = pd.DataFrame()\n",
    "\n",
    "\n",
    "croki_path = '/home/alpaco/project/drunk_prj/data/crokihip_align'\n",
    "for vid in os.listdir(croki_path):\n",
    "    csv_path = os.path.join(croki_path,vid)\n",
    "    tmp_csv = pd.read_csv(csv_path)\n",
    "    tmp_csv['y'] = 1\n",
    "    tmp_csv['filename'] = (vid.split('/')[-1]).split('.')[0]\n",
    "    num_cols = tmp_csv.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    tmp_csv[num_cols] = tmp_csv[num_cols].clip(lower=0)\n",
    "    Croki_Frame = pd.concat([Croki_Frame,tmp_csv],ignore_index=True)\n",
    "Croki_Frame\n",
    "#34580줄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "Combined = pd.concat([Normal_Frame,Croki_Frame],ignore_index=True)\n",
    "\n",
    "\n",
    "columns_to_convert = Combined.columns.difference(['filename','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)\n",
    "#스케일링 진행 후\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['filename', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'filename', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 45\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences_with_filter(Combined, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 1888/1888 [00:05<00:00, 322.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.2599, F1 Score: 0.9015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 1888/1888 [00:05<00:00, 372.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.1695, F1 Score: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 1888/1888 [00:05<00:00, 363.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.1593, F1 Score: 0.9094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 1888/1888 [00:05<00:00, 357.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.1588, F1 Score: 0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 1888/1888 [00:05<00:00, 347.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.1569, F1 Score: 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 1888/1888 [00:05<00:00, 370.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.1568, F1 Score: 0.9066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 1888/1888 [00:04<00:00, 381.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.1503, F1 Score: 0.8570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 1888/1888 [00:04<00:00, 391.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.1457, F1 Score: 0.9086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 1888/1888 [00:04<00:00, 389.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.1500, F1 Score: 0.9140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 1888/1888 [00:04<00:00, 379.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.1527, F1 Score: 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 1888/1888 [00:05<00:00, 346.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.1444, F1 Score: 0.9178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 1888/1888 [00:04<00:00, 378.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.1413, F1 Score: 0.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 1888/1888 [00:05<00:00, 377.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 0.1388, F1 Score: 0.9185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 1888/1888 [00:04<00:00, 380.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Loss: 0.1422, F1 Score: 0.9096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 0.1445, F1 Score: 0.9169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 1888/1888 [00:05<00:00, 374.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Loss: 0.1548, F1 Score: 0.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 1888/1888 [00:04<00:00, 380.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Loss: 0.1424, F1 Score: 0.9171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Loss: 0.1395, F1 Score: 0.9186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 1888/1888 [00:05<00:00, 362.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Loss: 0.1465, F1 Score: 0.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 0.1522, F1 Score: 0.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Loss: 0.1607, F1 Score: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Loss: 0.2018, F1 Score: 0.9038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 1888/1888 [00:05<00:00, 364.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Loss: 0.1665, F1 Score: 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 1888/1888 [00:05<00:00, 347.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Loss: 0.1802, F1 Score: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 0.1764, F1 Score: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 1888/1888 [00:05<00:00, 366.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Loss: 0.1754, F1 Score: 0.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 1888/1888 [00:05<00:00, 372.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Loss: 0.1778, F1 Score: 0.9088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 1888/1888 [00:05<00:00, 370.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Loss: 0.1638, F1 Score: 0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 1888/1888 [00:04<00:00, 378.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Loss: 0.1555, F1 Score: 0.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 1888/1888 [00:05<00:00, 376.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Loss: 0.1434, F1 Score: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 1888/1888 [00:04<00:00, 380.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Loss: 0.1470, F1 Score: 0.9153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 1888/1888 [00:05<00:00, 371.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Loss: 0.1510, F1 Score: 0.9153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 1888/1888 [00:05<00:00, 368.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Loss: 0.1494, F1 Score: 0.9085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Loss: 0.1965, F1 Score: 0.8653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 1888/1888 [00:04<00:00, 381.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Loss: 0.2230, F1 Score: 0.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Loss: 0.1875, F1 Score: 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 1888/1888 [00:05<00:00, 367.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Loss: 0.1765, F1 Score: 0.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 1888/1888 [00:05<00:00, 373.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Loss: 0.1702, F1 Score: 0.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 1888/1888 [00:05<00:00, 374.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Loss: 0.1857, F1 Score: 0.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 1888/1888 [00:05<00:00, 371.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Loss: 0.1606, F1 Score: 0.9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Loss: 0.1970, F1 Score: 0.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 1888/1888 [00:05<00:00, 377.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Loss: 0.1661, F1 Score: 0.9101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Loss: 0.1667, F1 Score: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Loss: 0.1827, F1 Score: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 1888/1888 [00:05<00:00, 368.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Loss: 0.1885, F1 Score: 0.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 1888/1888 [00:05<00:00, 375.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Loss: 0.1760, F1 Score: 0.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 1888/1888 [00:05<00:00, 373.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Loss: 0.1723, F1 Score: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 1888/1888 [00:05<00:00, 371.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Loss: 0.1658, F1 Score: 0.9078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Loss: 0.1934, F1 Score: 0.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 1888/1888 [00:05<00:00, 369.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Loss: 0.1830, F1 Score: 0.8530\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 분류용\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 및 검증 함수\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = evaluate(model, valid_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'11_27alignremodel_45LSTM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_winter_688_873.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.0ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 3.6ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.3ms\n",
      "Speed: 4.0ms preprocess, 20.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 4.2ms preprocess, 21.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.7ms preprocess, 17.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.7ms\n",
      "Speed: 2.6ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.5ms\n",
      "Speed: 5.1ms preprocess, 22.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.1ms\n",
      "Speed: 2.5ms preprocess, 14.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.7ms preprocess, 15.0ms inference, 11.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.6ms preprocess, 13.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 5.2ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 3.5ms preprocess, 24.2ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 5.0ms preprocess, 13.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.6ms preprocess, 13.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.0ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.7ms preprocess, 13.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 4.4ms preprocess, 19.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 3.6ms preprocess, 18.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.7ms\n",
      "Speed: 2.8ms preprocess, 16.7ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.1ms preprocess, 17.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.5ms\n",
      "Speed: 2.8ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.7ms\n",
      "Speed: 3.9ms preprocess, 20.7ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 2.8ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 3.9ms preprocess, 16.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.8ms\n",
      "Speed: 2.8ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.0ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 2.9ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.6ms\n",
      "Speed: 5.1ms preprocess, 20.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.7ms\n",
      "Speed: 3.0ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 4.1ms preprocess, 17.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.4ms\n",
      "Speed: 2.8ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.7ms preprocess, 17.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.6ms\n",
      "Speed: 2.9ms preprocess, 16.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 5.0ms preprocess, 24.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 2.9ms preprocess, 13.6ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.3ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.3ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.4ms\n",
      "Speed: 3.0ms preprocess, 15.4ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.0ms\n",
      "Speed: 2.8ms preprocess, 14.0ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.5ms preprocess, 17.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.6ms preprocess, 13.2ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.9ms\n",
      "Speed: 3.6ms preprocess, 20.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.5ms preprocess, 21.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 3.6ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.9ms\n",
      "Speed: 3.3ms preprocess, 13.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 3.8ms preprocess, 19.7ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 3.1ms preprocess, 12.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 5.2ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 5.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.8ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 5.8ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.6ms\n",
      "Speed: 2.6ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 3.6ms preprocess, 19.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 2.5ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 4.1ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.5ms preprocess, 17.2ms inference, 9.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.9ms\n",
      "Speed: 3.4ms preprocess, 21.9ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.5ms preprocess, 13.7ms inference, 9.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.5ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.1ms\n",
      "Speed: 2.5ms preprocess, 16.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.8ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.2ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 5.9ms preprocess, 15.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 3.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 5.1ms preprocess, 17.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.7ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.2ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_spring_173_3282_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_spring_173_3282_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 3.2ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.6ms\n",
      "Speed: 6.3ms preprocess, 18.6ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.5ms preprocess, 17.2ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.4ms\n",
      "Speed: 2.5ms preprocess, 15.4ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.1ms\n",
      "Speed: 4.1ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.9ms\n",
      "Speed: 2.5ms preprocess, 15.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.5ms\n",
      "Speed: 3.6ms preprocess, 19.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.3ms\n",
      "Speed: 3.9ms preprocess, 21.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 4.6ms preprocess, 19.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.5ms\n",
      "Speed: 5.6ms preprocess, 23.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 4.5ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 3.0ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.5ms preprocess, 14.8ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.2ms\n",
      "Speed: 3.4ms preprocess, 21.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.5ms preprocess, 13.2ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 2.5ms preprocess, 15.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.6ms preprocess, 13.4ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 3.9ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 2.9ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 4.5ms preprocess, 17.9ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 3.2ms preprocess, 14.0ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.4ms\n",
      "Speed: 4.7ms preprocess, 16.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.4ms preprocess, 18.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.0ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.8ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 4.1ms preprocess, 19.7ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.9ms\n",
      "Speed: 2.9ms preprocess, 13.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.3ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.2ms\n",
      "Speed: 4.0ms preprocess, 22.2ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.1ms preprocess, 15.2ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.3ms\n",
      "Speed: 3.6ms preprocess, 20.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 3.5ms preprocess, 15.2ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 4.4ms preprocess, 15.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 3.0ms preprocess, 15.3ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.8ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 2.8ms preprocess, 14.4ms inference, 9.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 3.5ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.6ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.5ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.9ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.4ms preprocess, 15.3ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.5ms\n",
      "Speed: 4.1ms preprocess, 21.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 2.4ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 3.5ms preprocess, 19.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.7ms preprocess, 13.3ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.3ms\n",
      "Speed: 5.2ms preprocess, 22.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.5ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 5.5ms preprocess, 18.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.3ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.1ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.3ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.2ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.3ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_spring_173_3282_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_spring_173_3282_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.4ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 2.5ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.2ms\n",
      "Speed: 5.0ms preprocess, 21.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 3.0ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.6ms\n",
      "Speed: 2.4ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.5ms preprocess, 17.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 3.0ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 3.0ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.0ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 5.6ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 2 backpacks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.4ms\n",
      "Speed: 2.5ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_spring_173_3282_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_spring_173_3282_part2_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_778_932.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_1025_1155.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_winter_913_1103.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 16.2ms\n",
      "Speed: 19.1ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.5ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.5ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.0ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.2ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.4ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.4ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 14.9ms\n",
      "Speed: 3.0ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.5ms preprocess, 15.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.9ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.4ms\n",
      "Speed: 3.6ms preprocess, 18.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.4ms\n",
      "Speed: 5.4ms preprocess, 21.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.6ms\n",
      "Speed: 3.8ms preprocess, 16.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 19.1ms\n",
      "Speed: 3.7ms preprocess, 19.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.9ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 2.9ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 3.4ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.6ms\n",
      "Speed: 3.5ms preprocess, 16.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.0ms\n",
      "Speed: 2.5ms preprocess, 17.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 3.6ms preprocess, 18.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 2.4ms preprocess, 15.1ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 3.7ms preprocess, 17.3ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 4.4ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 2.5ms preprocess, 13.8ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.7ms\n",
      "Speed: 3.6ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.5ms preprocess, 15.0ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.8ms preprocess, 17.2ms inference, 10.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.5ms preprocess, 13.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 20.7ms\n",
      "Speed: 3.5ms preprocess, 20.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.5ms\n",
      "Speed: 3.6ms preprocess, 19.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.8ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.8ms\n",
      "Speed: 2.5ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.6ms preprocess, 13.1ms inference, 9.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 3.6ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.6ms preprocess, 17.4ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.6ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 2.4ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.9ms\n",
      "Speed: 3.5ms preprocess, 21.9ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 2.5ms preprocess, 15.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam02_drunken03_place03_night_summer_1995_3379_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam02_drunken03_place03_night_summer_1995_3379_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_summer_3507_3811.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 25.3ms\n",
      "Speed: 34.4ms preprocess, 25.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 3.0ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.5ms preprocess, 16.7ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 2.9ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 3.3ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.5ms\n",
      "Speed: 4.1ms preprocess, 20.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.1ms\n",
      "Speed: 2.9ms preprocess, 17.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.5ms\n",
      "Speed: 5.2ms preprocess, 18.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 3.0ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.2ms\n",
      "Speed: 4.9ms preprocess, 19.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.3ms\n",
      "Speed: 2.9ms preprocess, 17.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.9ms\n",
      "Speed: 4.3ms preprocess, 20.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.2ms\n",
      "Speed: 2.9ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.8ms\n",
      "Speed: 4.0ms preprocess, 19.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.5ms\n",
      "Speed: 2.9ms preprocess, 17.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.6ms\n",
      "Speed: 4.2ms preprocess, 21.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 2.9ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 4.4ms preprocess, 17.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 20.5ms\n",
      "Speed: 3.7ms preprocess, 20.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 4.3ms preprocess, 18.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 23.7ms\n",
      "Speed: 3.1ms preprocess, 23.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.6ms\n",
      "Speed: 5.1ms preprocess, 17.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.2ms\n",
      "Speed: 2.9ms preprocess, 18.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.1ms\n",
      "Speed: 5.2ms preprocess, 20.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 4.0ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 3.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 4.1ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 5.7ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.9ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 3.0ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.3ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.7ms\n",
      "Speed: 2.9ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.0ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.2ms preprocess, 16.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.2ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.0ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.1ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.2ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.1ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.8ms\n",
      "Speed: 7.4ms preprocess, 21.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 3.1ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.1ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.2ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 3.2ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.5ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.8ms\n",
      "Speed: 5.2ms preprocess, 21.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.8ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.8ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_summer_4248_5174_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_summer_4248_5174_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_summer_3395_4284.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_3929_4507.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_summer_3654_4415.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 15.8ms\n",
      "Speed: 18.6ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.0ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 5.6ms preprocess, 17.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.6ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 4.1ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.9ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 5.8ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.6ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 3.0ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.2ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 3.1ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.8ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.2ms\n",
      "Speed: 2.9ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.8ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.3ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 26.6ms\n",
      "Speed: 3.6ms preprocess, 26.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 5.0ms preprocess, 15.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.4ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.1ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.4ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 16.7ms\n",
      "Speed: 3.8ms preprocess, 16.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.3ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 16.9ms\n",
      "Speed: 3.9ms preprocess, 16.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.5ms\n",
      "Speed: 5.1ms preprocess, 18.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 17.3ms\n",
      "Speed: 3.8ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.9ms\n",
      "Speed: 5.1ms preprocess, 18.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 14.2ms\n",
      "Speed: 3.6ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.7ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 16.1ms\n",
      "Speed: 4.2ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.9ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 17.0ms\n",
      "Speed: 3.4ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 5.2ms preprocess, 17.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.9ms\n",
      "Speed: 5.0ms preprocess, 18.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.2ms\n",
      "Speed: 3.0ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.3ms preprocess, 15.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 14.5ms\n",
      "Speed: 3.6ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.4ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.7ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 15.7ms\n",
      "Speed: 3.7ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.4ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.2ms\n",
      "Speed: 3.0ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.1ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.2ms\n",
      "Speed: 3.8ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 5.2ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 5.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.4ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.8ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.7ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.4ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.3ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.5ms preprocess, 14.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 5.7ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.5ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 3.5ms preprocess, 14.1ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.5ms preprocess, 15.0ms inference, 9.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 3.1ms preprocess, 12.5ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 3.4ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.0ms\n",
      "Speed: 3.8ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 2.8ms preprocess, 15.8ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.8ms\n",
      "Speed: 3.5ms preprocess, 19.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.6ms\n",
      "Speed: 5.6ms preprocess, 18.6ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.8ms\n",
      "Speed: 3.4ms preprocess, 19.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.7ms preprocess, 15.3ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.2ms\n",
      "Speed: 3.5ms preprocess, 20.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 3.9ms preprocess, 15.6ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 3.4ms preprocess, 15.8ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.8ms\n",
      "Speed: 5.5ms preprocess, 18.8ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.5ms\n",
      "Speed: 3.5ms preprocess, 19.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.5ms preprocess, 15.5ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.3ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.0ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.4ms preprocess, 15.0ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.0ms\n",
      "Speed: 2.5ms preprocess, 13.0ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 3.5ms preprocess, 19.7ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam03_drunken03_place03_night_spring_165_1183_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam03_drunken03_place03_night_spring_165_1183_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 24.9ms\n",
      "Speed: 5.6ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 25.5ms\n",
      "Speed: 2.9ms preprocess, 25.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 35.5ms\n",
      "Speed: 4.4ms preprocess, 35.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.5ms preprocess, 21.1ms inference, 13.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.8ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.3ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 3.7ms preprocess, 24.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 21.6ms\n",
      "Speed: 2.4ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.9ms\n",
      "Speed: 2.3ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.2ms\n",
      "Speed: 5.7ms preprocess, 22.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 2.5ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.8ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 4.3ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.4ms\n",
      "Speed: 3.8ms preprocess, 23.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 22.5ms\n",
      "Speed: 2.5ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.2ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 18.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 3.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.5ms preprocess, 13.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.7ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.7ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.7ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.6ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.7ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.6ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.9ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.8ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.4ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.2ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 3.2ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.3ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.0ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.1ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.0ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 4.0ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.3ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.3ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.2ms preprocess, 18.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.9ms\n",
      "Speed: 3.0ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 4.4ms preprocess, 18.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 26.5ms\n",
      "Speed: 3.7ms preprocess, 26.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 6.0ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 18.0ms\n",
      "Speed: 2.9ms preprocess, 18.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.1ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_spring_4613_6061_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_spring_4613_6061_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.8ms\n",
      "Speed: 6.5ms preprocess, 14.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.2ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.3ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.1ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.4ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.1ms\n",
      "Speed: 4.9ms preprocess, 24.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.2ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.4ms\n",
      "Speed: 5.6ms preprocess, 20.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.2ms preprocess, 15.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.2ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.0ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.1ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.2ms\n",
      "Speed: 3.0ms preprocess, 14.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.9ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.2ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 4.2ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.2ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.2ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.0ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.0ms\n",
      "Speed: 3.5ms preprocess, 17.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 3.1ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.6ms preprocess, 15.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.6ms\n",
      "Speed: 5.3ms preprocess, 18.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.1ms\n",
      "Speed: 5.3ms preprocess, 20.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.9ms\n",
      "Speed: 5.3ms preprocess, 18.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.1ms\n",
      "Speed: 4.4ms preprocess, 20.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.1ms\n",
      "Speed: 4.9ms preprocess, 20.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.2ms\n",
      "Speed: 3.9ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.1ms preprocess, 17.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.4ms\n",
      "Speed: 3.1ms preprocess, 16.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.8ms\n",
      "Speed: 4.0ms preprocess, 22.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 2.9ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.3ms preprocess, 18.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.3ms\n",
      "Speed: 3.4ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.0ms\n",
      "Speed: 5.5ms preprocess, 20.0ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 2.8ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.7ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.9ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.8ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 3.1ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.6ms preprocess, 14.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 4.0ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.9ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 25.0ms\n",
      "Speed: 3.6ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 3.4ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 4.8ms preprocess, 17.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 3.0ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.2ms preprocess, 16.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam01_drunken03_place03_night_winter_2716_3841_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam01_drunken03_place03_night_winter_2716_3841_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.0ms\n",
      "Speed: 5.1ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 26.9ms\n",
      "Speed: 4.0ms preprocess, 26.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.4ms\n",
      "Speed: 3.6ms preprocess, 16.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.1ms\n",
      "Speed: 3.4ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 4.8ms preprocess, 16.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.6ms\n",
      "Speed: 3.3ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.9ms\n",
      "Speed: 5.0ms preprocess, 18.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 17.1ms\n",
      "Speed: 3.2ms preprocess, 17.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 5.2ms preprocess, 21.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 15.4ms\n",
      "Speed: 3.4ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 38.2ms\n",
      "Speed: 5.4ms preprocess, 38.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 17.5ms\n",
      "Speed: 3.4ms preprocess, 17.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 5.9ms preprocess, 17.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.9ms\n",
      "Speed: 3.3ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.3ms\n",
      "Speed: 5.5ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 15.1ms\n",
      "Speed: 3.3ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 5.0ms preprocess, 18.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 15.6ms\n",
      "Speed: 3.2ms preprocess, 15.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 5.1ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 16.5ms\n",
      "Speed: 3.4ms preprocess, 16.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.4ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 17.4ms\n",
      "Speed: 3.7ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 5.5ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.0ms\n",
      "Speed: 4.3ms preprocess, 15.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 4.7ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.8ms\n",
      "Speed: 2.9ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.7ms preprocess, 18.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.7ms\n",
      "Speed: 4.3ms preprocess, 18.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.4ms\n",
      "Speed: 2.9ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.0ms preprocess, 17.4ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 12.9ms\n",
      "Speed: 3.0ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.5ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.7ms\n",
      "Speed: 5.4ms preprocess, 18.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.5ms\n",
      "Speed: 4.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 4.1ms preprocess, 24.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 43.8ms\n",
      "Speed: 16.4ms preprocess, 43.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 26.4ms\n",
      "Speed: 4.4ms preprocess, 26.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.7ms\n",
      "Speed: 3.1ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.2ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 16.1ms\n",
      "Speed: 3.2ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 5.6ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 32.0ms\n",
      "Speed: 3.6ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 3.4ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.6ms\n",
      "Speed: 3.1ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.6ms\n",
      "Speed: 4.2ms preprocess, 25.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.5ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.9ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 3.4ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.2ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 27.2ms\n",
      "Speed: 5.7ms preprocess, 27.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 3.7ms preprocess, 19.6ms inference, 13.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 20.5ms\n",
      "Speed: 2.4ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.7ms\n",
      "Speed: 3.5ms preprocess, 22.7ms inference, 10.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 20.8ms\n",
      "Speed: 2.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.4ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 23.2ms\n",
      "Speed: 4.1ms preprocess, 23.2ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.9ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 4.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 29.0ms\n",
      "Speed: 4.3ms preprocess, 29.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 29.1ms\n",
      "Speed: 2.6ms preprocess, 29.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 30.8ms\n",
      "Speed: 2.9ms preprocess, 30.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.6ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 24.7ms\n",
      "Speed: 2.9ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.4ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.0ms\n",
      "Speed: 4.4ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.5ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 24.4ms\n",
      "Speed: 3.1ms preprocess, 24.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.8ms\n",
      "Speed: 4.9ms preprocess, 18.8ms inference, 8.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 23.3ms\n",
      "Speed: 2.9ms preprocess, 23.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 4.1ms preprocess, 17.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.9ms\n",
      "Speed: 3.0ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 4.0ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.8ms\n",
      "Speed: 2.8ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 26.0ms\n",
      "Speed: 4.5ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.9ms\n",
      "Speed: 2.8ms preprocess, 13.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.3ms\n",
      "Speed: 4.3ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.6ms\n",
      "Speed: 3.2ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.3ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.1ms\n",
      "Speed: 5.7ms preprocess, 22.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.0ms\n",
      "Speed: 2.9ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.1ms preprocess, 17.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 19.6ms\n",
      "Speed: 2.9ms preprocess, 19.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.1ms preprocess, 17.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.7ms\n",
      "Speed: 2.9ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 4.3ms preprocess, 17.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.4ms\n",
      "Speed: 2.9ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.5ms\n",
      "Speed: 4.0ms preprocess, 22.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.8ms\n",
      "Speed: 3.2ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.0ms preprocess, 17.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.1ms\n",
      "Speed: 3.0ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.9ms\n",
      "Speed: 2.9ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 5.2ms preprocess, 17.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.8ms\n",
      "Speed: 2.9ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.6ms\n",
      "Speed: 4.1ms preprocess, 17.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.1ms\n",
      "Speed: 2.9ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.9ms preprocess, 17.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.9ms\n",
      "Speed: 3.2ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.5ms\n",
      "Speed: 2.8ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.6ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.2ms\n",
      "Speed: 3.2ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.1ms preprocess, 16.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 3.0ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.2ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-1_cam01_drunken03_place03_night_winter_198_1608_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-1_cam01_drunken03_place03_night_winter_198_1608_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 24.3ms\n",
      "Speed: 31.5ms preprocess, 24.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.8ms\n",
      "Speed: 3.0ms preprocess, 15.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.5ms\n",
      "Speed: 4.3ms preprocess, 18.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.2ms\n",
      "Speed: 2.8ms preprocess, 15.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 4.3ms preprocess, 17.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.1ms\n",
      "Speed: 3.5ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.6ms preprocess, 17.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 47.2ms\n",
      "Speed: 4.9ms preprocess, 47.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 5.0ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.7ms\n",
      "Speed: 3.2ms preprocess, 17.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.9ms\n",
      "Speed: 7.8ms preprocess, 25.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.5ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.7ms\n",
      "Speed: 3.5ms preprocess, 24.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.3ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 3.2ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.7ms\n",
      "Speed: 5.2ms preprocess, 18.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.2ms\n",
      "Speed: 3.6ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.2ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.1ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.2ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 2.9ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.5ms preprocess, 16.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 3.2ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.2ms\n",
      "Speed: 4.2ms preprocess, 21.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.4ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.4ms\n",
      "Speed: 2.9ms preprocess, 15.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.3ms preprocess, 16.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.3ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.2ms\n",
      "Speed: 3.6ms preprocess, 14.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 4.0ms preprocess, 15.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.5ms\n",
      "Speed: 2.9ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 26.1ms\n",
      "Speed: 4.0ms preprocess, 26.1ms inference, 10.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.8ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 27.7ms\n",
      "Speed: 2.6ms preprocess, 27.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.4ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 26.3ms\n",
      "Speed: 2.6ms preprocess, 26.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.6ms\n",
      "Speed: 3.9ms preprocess, 16.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.2ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 30.9ms\n",
      "Speed: 3.5ms preprocess, 30.9ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.4ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 11.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.6ms\n",
      "Speed: 6.0ms preprocess, 21.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 22.2ms\n",
      "Speed: 3.5ms preprocess, 22.2ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.6ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.2ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.1ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.2ms\n",
      "Speed: 4.0ms preprocess, 22.2ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.1ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.2ms\n",
      "Speed: 4.2ms preprocess, 22.2ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 4.1ms preprocess, 15.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 2.9ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.8ms\n",
      "Speed: 4.8ms preprocess, 18.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.8ms\n",
      "Speed: 4.1ms preprocess, 19.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.3ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.1ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 2.9ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.3ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 5.3ms preprocess, 21.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.4ms\n",
      "Speed: 4.1ms preprocess, 22.4ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.0ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.1ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 4.2ms preprocess, 15.6ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.0ms preprocess, 15.2ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.9ms\n",
      "Speed: 3.5ms preprocess, 15.9ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.3ms\n",
      "Speed: 3.0ms preprocess, 15.3ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.1ms\n",
      "Speed: 5.6ms preprocess, 23.1ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 3.6ms preprocess, 19.7ms inference, 7.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 2.5ms preprocess, 14.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 3.7ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 3.6ms preprocess, 17.3ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.3ms\n",
      "Speed: 3.7ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 4.5ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 4.2ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.0ms\n",
      "Speed: 3.1ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_summer_442_2906_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_summer_442_2906_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.2ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.7ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.1ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 3.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 3.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.9ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 3.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.1ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 3.2ms preprocess, 15.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.7ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.5ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.6ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.1ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.5ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.3ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 5.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.5ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.8ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.5ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 4.7ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 3.2ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.9ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.5ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 2.6ms preprocess, 15.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.9ms preprocess, 15.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.4ms\n",
      "Speed: 3.7ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.2ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.3ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.4ms preprocess, 17.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.6ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 3.9ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 5.1ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 2.8ms preprocess, 13.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.6ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_summer_442_2906_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_summer_442_2906_part1_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_winter_189_378.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 18.1ms\n",
      "Speed: 18.8ms preprocess, 18.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 3.9ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 4.0ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.9ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.8ms\n",
      "Speed: 2.4ms preprocess, 15.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 6.4ms preprocess, 15.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 3.1ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.8ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.0ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.8ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 3.2ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.8ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 5.2ms preprocess, 17.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 3.4ms preprocess, 16.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.3ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.8ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_winter_140_2996_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_winter_140_2996_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 4.6ms preprocess, 14.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 3.7ms preprocess, 18.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.7ms\n",
      "Speed: 2.6ms preprocess, 16.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.4ms\n",
      "Speed: 3.0ms preprocess, 15.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 3.1ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.1ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.0ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 4.4ms preprocess, 18.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 3.9ms preprocess, 15.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.4ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 4.6ms preprocess, 17.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.5ms preprocess, 14.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.6ms\n",
      "Speed: 3.1ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.0ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.2ms preprocess, 17.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.7ms\n",
      "Speed: 2.9ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 4.0ms preprocess, 16.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.2ms\n",
      "Speed: 3.0ms preprocess, 16.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.1ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.3ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.8ms\n",
      "Speed: 4.7ms preprocess, 19.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.1ms preprocess, 17.0ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 4.4ms preprocess, 17.8ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 4.6ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.4ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.8ms\n",
      "Speed: 3.4ms preprocess, 18.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 3.0ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 4.0ms preprocess, 17.7ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.3ms\n",
      "Speed: 3.5ms preprocess, 16.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.7ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 3.9ms preprocess, 15.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 2.5ms preprocess, 15.1ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.5ms preprocess, 14.8ms inference, 11.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 2.4ms preprocess, 14.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.8ms\n",
      "Speed: 5.4ms preprocess, 25.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.4ms\n",
      "Speed: 2.4ms preprocess, 16.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.4ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 3.4ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.5ms preprocess, 15.3ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.1ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 4.6ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.2ms\n",
      "Speed: 3.0ms preprocess, 14.2ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.6ms preprocess, 13.2ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.6ms preprocess, 13.1ms inference, 9.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 4.9ms preprocess, 19.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.5ms preprocess, 17.2ms inference, 9.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 23.9ms\n",
      "Speed: 4.4ms preprocess, 23.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.1ms preprocess, 17.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 3.0ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.9ms\n",
      "Speed: 4.3ms preprocess, 20.9ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.0ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.3ms\n",
      "Speed: 5.2ms preprocess, 21.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 3.6ms preprocess, 15.2ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.5ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 2.4ms preprocess, 14.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.6ms preprocess, 17.4ms inference, 10.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 2.5ms preprocess, 15.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.5ms preprocess, 21.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_winter_140_2996_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_winter_140_2996_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 3.0ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.2ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 6.1ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.6ms\n",
      "Speed: 2.6ms preprocess, 14.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 3.5ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 2.9ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 4.0ms preprocess, 17.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 20.1ms\n",
      "Speed: 3.8ms preprocess, 20.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.3ms\n",
      "Speed: 3.1ms preprocess, 14.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.4ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.4ms preprocess, 17.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 4.9ms preprocess, 18.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 3.3ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 4.0ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.8ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.1ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.3ms preprocess, 15.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 3.4ms preprocess, 16.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_winter_140_2996_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_winter_140_2996_part2_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_1305_1830.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_1221_1481.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_966_1052.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_summer_2610_3184.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_2064_2214.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_summer_1255_1461.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_spring_3935_4161.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_spring_240_1051.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_spring_3621_4345.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_spring_2109_2276.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_winter_2811_3240.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_2727_3222.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_summer_296_1098.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_winter_2468_2711.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_winter_1746_2125.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 5.1ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 2.9ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 2.9ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.3ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.2ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.9ms\n",
      "Speed: 4.1ms preprocess, 25.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 3.6ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.3ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.8ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 4.0ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 5.1ms preprocess, 17.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.6ms\n",
      "Speed: 3.3ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.8ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 3.2ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.8ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.9ms preprocess, 16.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.6ms\n",
      "Speed: 4.8ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.6ms\n",
      "Speed: 3.3ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.8ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.9ms\n",
      "Speed: 3.6ms preprocess, 18.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 3.3ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 20.0ms\n",
      "Speed: 3.1ms preprocess, 20.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.4ms\n",
      "Speed: 4.8ms preprocess, 16.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 3.3ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 3.6ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.9ms\n",
      "Speed: 4.8ms preprocess, 18.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.9ms preprocess, 16.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 3.4ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 3.3ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 5.0ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.3ms\n",
      "Speed: 4.9ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 3.2ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.3ms\n",
      "Speed: 3.9ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 3.1ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.2ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.5ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 2.9ms preprocess, 14.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 3.0ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.3ms\n",
      "Speed: 4.7ms preprocess, 19.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 3.0ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 3.1ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.9ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.9ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_winter_4195_5643_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_winter_4195_5643_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_3353_3593.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_1042_1141.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_428_788.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_spring_1639_2386.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.4ms\n",
      "Speed: 13.4ms preprocess, 16.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.6ms\n",
      "Speed: 2.9ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.3ms preprocess, 15.4ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.3ms\n",
      "Speed: 4.4ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 7.1ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.3ms\n",
      "Speed: 3.0ms preprocess, 15.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.2ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 3.0ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.1ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.2ms preprocess, 15.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.1ms\n",
      "Speed: 3.0ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.9ms\n",
      "Speed: 3.9ms preprocess, 22.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 5.2ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 4.5ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 3.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.2ms preprocess, 15.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.6ms\n",
      "Speed: 3.3ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.9ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.4ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 4.0ms preprocess, 16.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.7ms preprocess, 14.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.9ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.5ms\n",
      "Speed: 3.5ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 5.9ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 5.0ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.9ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 3.0ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.5ms preprocess, 15.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.3ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.9ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.7ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.9ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 2.8ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.8ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.9ms\n",
      "Speed: 3.3ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.1ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.9ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.0ms\n",
      "Speed: 2.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.3ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.2ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.3ms preprocess, 14.4ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.2ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 3.5ms preprocess, 15.4ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.0ms\n",
      "Speed: 3.5ms preprocess, 19.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.4ms preprocess, 21.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 2.5ms preprocess, 14.7ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 4.2ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.3ms\n",
      "Speed: 5.3ms preprocess, 19.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 2.4ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.4ms preprocess, 13.5ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 2.7ms preprocess, 13.8ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 4.2ms preprocess, 17.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 2.9ms preprocess, 16.0ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.7ms\n",
      "Speed: 2.9ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 5.3ms preprocess, 24.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_spring_332_3141_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_spring_332_3141_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 19.4ms\n",
      "Speed: 3.7ms preprocess, 19.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 4.0ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 4.1ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 3.2ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.7ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 3.2ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 3.3ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 4.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 5.4ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.9ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 3.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.8ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 5.0ms preprocess, 17.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.3ms preprocess, 13.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 3.7ms preprocess, 15.7ms inference, 11.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 2.5ms preprocess, 14.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.7ms\n",
      "Speed: 7.6ms preprocess, 23.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.7ms\n",
      "Speed: 2.6ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.8ms preprocess, 17.4ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 3.5ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.6ms\n",
      "Speed: 3.6ms preprocess, 18.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 4.7ms preprocess, 17.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.5ms preprocess, 21.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 4.0ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.8ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.9ms\n",
      "Speed: 3.6ms preprocess, 21.9ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 2.6ms preprocess, 14.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 2.7ms preprocess, 14.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.7ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.3ms\n",
      "Speed: 3.7ms preprocess, 20.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.6ms\n",
      "Speed: 5.2ms preprocess, 20.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_spring_332_3141_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_spring_332_3141_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.8ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.8ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.8ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 2.8ms preprocess, 13.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.7ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.1ms\n",
      "Speed: 4.3ms preprocess, 17.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.4ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.4ms\n",
      "Speed: 3.5ms preprocess, 16.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 5.0ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 3.4ms preprocess, 15.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.8ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.8ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 5.1ms preprocess, 17.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 3.6ms preprocess, 17.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.4ms preprocess, 17.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.9ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 5.3ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.9ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.2ms\n",
      "Speed: 3.4ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.5ms preprocess, 17.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 18.2ms\n",
      "Speed: 4.0ms preprocess, 18.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 5.5ms preprocess, 15.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.5ms\n",
      "Speed: 3.8ms preprocess, 17.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.9ms\n",
      "Speed: 2.5ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 5.0ms preprocess, 18.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_spring_332_3141_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_spring_332_3141_part2_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 1 surfboard, 15.9ms\n",
      "Speed: 5.9ms preprocess, 15.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.6ms\n",
      "Speed: 5.3ms preprocess, 18.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 5.0ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 24.2ms\n",
      "Speed: 2.5ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 12.0ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 3.0ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 21.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.5ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 3.1ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.9ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 5.6ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.3ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.3ms\n",
      "Speed: 2.9ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.5ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.4ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.2ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.8ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 14.1ms\n",
      "Speed: 3.1ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.3ms preprocess, 14.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.0ms preprocess, 15.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.8ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.2ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 16.5ms\n",
      "Speed: 2.7ms preprocess, 16.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.8ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.2ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 13.2ms\n",
      "Speed: 2.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.2ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.0ms\n",
      "Speed: 3.2ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.1ms\n",
      "Speed: 3.0ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.3ms\n",
      "Speed: 3.0ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.1ms\n",
      "Speed: 3.2ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.2ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.1ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.4ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.1ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam01_drunken03_place03_night_summer_133_1529_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam01_drunken03_place03_night_summer_133_1529_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_spring_1511_1622.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 18.9ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.1ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.8ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.2ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.6ms\n",
      "Speed: 2.8ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.5ms\n",
      "Speed: 4.2ms preprocess, 18.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 3.0ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.4ms preprocess, 18.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 3.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 5.0ms preprocess, 16.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 3.2ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.6ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.9ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.9ms\n",
      "Speed: 5.1ms preprocess, 21.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 3.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 5.2ms preprocess, 15.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 4.0ms preprocess, 17.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.0ms preprocess, 17.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.1ms preprocess, 17.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.4ms\n",
      "Speed: 3.1ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.9ms preprocess, 17.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.7ms\n",
      "Speed: 3.0ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.3ms preprocess, 18.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.8ms\n",
      "Speed: 3.2ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.3ms preprocess, 18.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 4.1ms preprocess, 17.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.1ms\n",
      "Speed: 3.3ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 4.4ms preprocess, 18.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 3.2ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.1ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.0ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.9ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 3.0ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.6ms preprocess, 18.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.4ms\n",
      "Speed: 3.4ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.3ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.1ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 5.2ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.1ms\n",
      "Speed: 4.9ms preprocess, 19.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.1ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.3ms\n",
      "Speed: 2.8ms preprocess, 16.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.9ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_winter_4185_5658_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_winter_4185_5658_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_1981_2116.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 8.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.7ms preprocess, 13.0ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 4.1ms preprocess, 13.4ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.5ms\n",
      "Speed: 5.1ms preprocess, 23.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.1ms\n",
      "Speed: 2.7ms preprocess, 17.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.3ms\n",
      "Speed: 3.2ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 5.7ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 5.0ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.1ms\n",
      "Speed: 3.4ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.9ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.9ms\n",
      "Speed: 4.0ms preprocess, 20.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.8ms preprocess, 14.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.7ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 5.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.7ms\n",
      "Speed: 3.4ms preprocess, 16.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.5ms\n",
      "Speed: 3.5ms preprocess, 19.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 5.7ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 2.6ms preprocess, 14.1ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.3ms\n",
      "Speed: 3.3ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.5ms\n",
      "Speed: 4.2ms preprocess, 18.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.2ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 5.6ms preprocess, 18.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.6ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 5.0ms preprocess, 18.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.7ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.4ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.1ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.9ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.2ms preprocess, 15.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.1ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.1ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.3ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 3.1ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.1ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.0ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.4ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.5ms preprocess, 17.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.1ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.7ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.5ms\n",
      "Speed: 5.1ms preprocess, 18.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 22.3ms\n",
      "Speed: 3.5ms preprocess, 22.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.8ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.6ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.9ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.1ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.9ms\n",
      "Speed: 2.8ms preprocess, 13.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam03_drunken03_place03_night_summer_1957_3339_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam03_drunken03_place03_night_summer_1957_3339_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_spring_2108_2691.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_summer_3916_4456.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_summer_1283_1381.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_spring_2657_3193.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_1526_1753.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-3_cam03_drunken03_place03_night_summer_4227_5123.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_winter_3162_3802.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_spring_1810_1926.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_summer_3571_3823.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_winter_616_1256.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 12.1ms\n",
      "Speed: 4.8ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 16.9ms\n",
      "Speed: 4.3ms preprocess, 16.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 3.9ms preprocess, 16.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.3ms\n",
      "Speed: 4.3ms preprocess, 21.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 4.0ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.6ms\n",
      "Speed: 2.9ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.3ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.1ms\n",
      "Speed: 3.1ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.6ms\n",
      "Speed: 7.6ms preprocess, 20.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.4ms preprocess, 14.9ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.5ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.9ms\n",
      "Speed: 3.4ms preprocess, 20.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.1ms\n",
      "Speed: 2.5ms preprocess, 13.1ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.4ms preprocess, 17.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 15.2ms\n",
      "Speed: 2.5ms preprocess, 15.2ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.4ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.7ms\n",
      "Speed: 3.5ms preprocess, 18.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.2ms\n",
      "Speed: 2.6ms preprocess, 14.2ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 3.5ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 3.5ms preprocess, 15.1ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.7ms\n",
      "Speed: 2.4ms preprocess, 13.7ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.7ms\n",
      "Speed: 3.6ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 3.5ms preprocess, 21.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.4ms\n",
      "Speed: 2.5ms preprocess, 13.4ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.9ms\n",
      "Speed: 3.0ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 5.2ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.6ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.8ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.8ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.5ms\n",
      "Speed: 5.4ms preprocess, 20.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.9ms\n",
      "Speed: 3.1ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 4.8ms preprocess, 18.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.5ms\n",
      "Speed: 3.1ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.3ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 4.0ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.4ms\n",
      "Speed: 4.9ms preprocess, 20.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.2ms\n",
      "Speed: 3.6ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.1ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.6ms preprocess, 17.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.0ms preprocess, 15.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.8ms\n",
      "Speed: 3.0ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.3ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.6ms\n",
      "Speed: 2.8ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.8ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.7ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.6ms\n",
      "Speed: 2.8ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.6ms\n",
      "Speed: 2.8ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.0ms\n",
      "Speed: 2.8ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 3.1ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-1_cam03_drunken03_place03_night_winter_162_1576_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-1_cam03_drunken03_place03_night_winter_162_1576_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_winter_240_565.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_summer_3199_3430.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_164_573.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_winter_701_830.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_1581_1832.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_spring_2844_3256.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_winter_581_823.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_spring_167_488.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 1 book, 12.1ms\n",
      "Speed: 9.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.8ms\n",
      "Speed: 3.5ms preprocess, 25.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 14.2ms\n",
      "Speed: 2.4ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 16.6ms\n",
      "Speed: 3.8ms preprocess, 16.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 book, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.0ms preprocess, 17.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.1ms\n",
      "Speed: 2.4ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 3.8ms preprocess, 19.7ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.9ms\n",
      "Speed: 2.5ms preprocess, 14.9ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.1ms\n",
      "Speed: 3.5ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.1ms\n",
      "Speed: 3.6ms preprocess, 19.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 2.4ms preprocess, 14.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 3.5ms preprocess, 24.2ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 3.8ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.5ms\n",
      "Speed: 3.4ms preprocess, 19.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.2ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.1ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 5.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 5.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 3.4ms preprocess, 16.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam02_drunken03_place03_night_winter_176_1512_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam02_drunken03_place03_night_winter_176_1512_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_winter_1284_1782.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_winter_3178_3823.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_1287_1464.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_winter_491_633.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_winter_1844_2262.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.8ms\n",
      "Speed: 5.7ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.6ms\n",
      "Speed: 2.9ms preprocess, 14.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.6ms preprocess, 17.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.1ms\n",
      "Speed: 2.5ms preprocess, 17.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 2.5ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.4ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 3.5ms preprocess, 19.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 4.1ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.0ms\n",
      "Speed: 4.8ms preprocess, 23.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 3.5ms preprocess, 16.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.0ms\n",
      "Speed: 2.5ms preprocess, 17.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 3.4ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.9ms\n",
      "Speed: 3.5ms preprocess, 20.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 5.0ms preprocess, 13.3ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.2ms\n",
      "Speed: 3.5ms preprocess, 24.2ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 2.5ms preprocess, 13.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.4ms preprocess, 15.5ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 2.5ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.4ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 3.4ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.1ms\n",
      "Speed: 4.6ms preprocess, 23.1ms inference, 9.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.5ms\n",
      "Speed: 2.5ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.6ms preprocess, 13.7ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.0ms\n",
      "Speed: 3.5ms preprocess, 22.0ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 3.4ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.8ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 4.9ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam01_drunken03_place03_night_winter_1478_3040_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam01_drunken03_place03_night_winter_1478_3040_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_763_914.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_2068_2245.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_spring_2146_2654.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_spring_558_1111.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_winter_4151_4736.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_summer_250_681.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_1978_2221.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_spring_161_457.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 9.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.5ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.2ms preprocess, 15.0ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.1ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.9ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 18.0ms\n",
      "Speed: 3.3ms preprocess, 18.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.6ms\n",
      "Speed: 2.4ms preprocess, 15.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.2ms\n",
      "Speed: 5.0ms preprocess, 21.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 3.5ms preprocess, 17.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 3.6ms preprocess, 15.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 3.5ms preprocess, 17.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 4.0ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.6ms\n",
      "Speed: 2.5ms preprocess, 15.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.1ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.3ms preprocess, 14.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.5ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.6ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_summer_151_3017_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_summer_151_3017_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.6ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 3.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.0ms\n",
      "Speed: 3.4ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.8ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.6ms\n",
      "Speed: 4.9ms preprocess, 17.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.7ms preprocess, 17.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.6ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 5.2ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 2.5ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.6ms\n",
      "Speed: 2.5ms preprocess, 14.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.8ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.8ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 21.4ms\n",
      "Speed: 2.4ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 37.2ms\n",
      "Speed: 4.1ms preprocess, 37.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 30.2ms\n",
      "Speed: 2.9ms preprocess, 30.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 4.4ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.0ms\n",
      "Speed: 4.1ms preprocess, 25.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.6ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.6ms\n",
      "Speed: 3.4ms preprocess, 24.6ms inference, 11.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 26.8ms\n",
      "Speed: 2.5ms preprocess, 26.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.7ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.1ms\n",
      "Speed: 2.5ms preprocess, 18.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 23.8ms\n",
      "Speed: 3.0ms preprocess, 23.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 3.6ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.1ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.6ms\n",
      "Speed: 4.1ms preprocess, 18.6ms inference, 10.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 21.5ms\n",
      "Speed: 2.8ms preprocess, 21.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.1ms preprocess, 15.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.8ms\n",
      "Speed: 3.5ms preprocess, 21.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.1ms\n",
      "Speed: 4.1ms preprocess, 25.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 3.9ms preprocess, 18.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 45.5ms\n",
      "Speed: 4.1ms preprocess, 45.5ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 23.2ms\n",
      "Speed: 3.7ms preprocess, 23.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 29.4ms\n",
      "Speed: 8.0ms preprocess, 29.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 21.5ms\n",
      "Speed: 2.9ms preprocess, 21.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 34.4ms\n",
      "Speed: 8.7ms preprocess, 34.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.5ms preprocess, 17.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 21.4ms\n",
      "Speed: 3.2ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.6ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 2.8ms preprocess, 15.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 26.0ms\n",
      "Speed: 4.2ms preprocess, 26.0ms inference, 10.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.6ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.5ms preprocess, 12.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.6ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 32.9ms\n",
      "Speed: 4.5ms preprocess, 32.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 22.5ms\n",
      "Speed: 2.8ms preprocess, 22.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.2ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 25.2ms\n",
      "Speed: 3.2ms preprocess, 25.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.3ms preprocess, 15.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 25.1ms\n",
      "Speed: 2.8ms preprocess, 25.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 4.5ms preprocess, 15.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.5ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.5ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.9ms\n",
      "Speed: 3.1ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.4ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 5.0ms preprocess, 15.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 3.3ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 5.6ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.2ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.3ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.2ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.0ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_summer_151_3017_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_summer_151_3017_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.5ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.0ms\n",
      "Speed: 2.5ms preprocess, 15.0ms inference, 7.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.4ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.8ms\n",
      "Speed: 3.5ms preprocess, 20.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.0ms preprocess, 17.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 2.7ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 2.8ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.0ms preprocess, 16.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 2.9ms preprocess, 15.2ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.4ms\n",
      "Speed: 4.0ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 4.6ms preprocess, 18.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.2ms\n",
      "Speed: 2.4ms preprocess, 17.2ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.4ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.5ms preprocess, 21.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 23.4ms\n",
      "Speed: 4.1ms preprocess, 23.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 3.4ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.3ms\n",
      "Speed: 4.2ms preprocess, 21.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.7ms\n",
      "Speed: 3.0ms preprocess, 15.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.2ms preprocess, 17.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 2.9ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.1ms preprocess, 17.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.8ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.7ms\n",
      "Speed: 2.9ms preprocess, 16.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.9ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.1ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.8ms\n",
      "Speed: 2.9ms preprocess, 16.8ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 2.8ms preprocess, 14.1ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.6ms preprocess, 12.9ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.8ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.8ms\n",
      "Speed: 2.7ms preprocess, 16.8ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.3ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.1ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 3.0ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.8ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 3.0ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.1ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.8ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.8ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.9ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.8ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.0ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.8ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.8ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.8ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.8ms preprocess, 14.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.8ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.9ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.9ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.7ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 3.0ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.1ms preprocess, 14.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.9ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 handbag, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 handbag, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 27.1ms\n",
      "Speed: 5.7ms preprocess, 27.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.6ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_summer_151_3017_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_summer_151_3017_part2_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.5ms\n",
      "Speed: 18.2ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.8ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.4ms\n",
      "Speed: 3.7ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 4.0ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.7ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 4.0ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 27.9ms\n",
      "Speed: 5.3ms preprocess, 27.9ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 29.0ms\n",
      "Speed: 5.9ms preprocess, 29.0ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 29.4ms\n",
      "Speed: 5.9ms preprocess, 29.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 3.5ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 3.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.4ms preprocess, 14.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.5ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.9ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 handbag, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.9ms\n",
      "Speed: 3.4ms preprocess, 17.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 3.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 5.7ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 3.7ms preprocess, 17.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.2ms\n",
      "Speed: 4.3ms preprocess, 19.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.9ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.0ms\n",
      "Speed: 4.9ms preprocess, 21.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.1ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 4.7ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 2.5ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.2ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.8ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.1ms\n",
      "Speed: 3.6ms preprocess, 21.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 3.8ms preprocess, 18.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 3.1ms preprocess, 16.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 3.5ms preprocess, 15.1ms inference, 11.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.6ms\n",
      "Speed: 2.6ms preprocess, 15.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.3ms preprocess, 15.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.5ms preprocess, 15.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.7ms\n",
      "Speed: 2.5ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 3.8ms preprocess, 19.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 3.4ms preprocess, 17.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.1ms\n",
      "Speed: 2.6ms preprocess, 14.1ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.0ms\n",
      "Speed: 2.6ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 4.4ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 2.5ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.5ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 3.4ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.6ms preprocess, 13.2ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 3.2ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.9ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 5.1ms preprocess, 17.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.6ms\n",
      "Speed: 4.1ms preprocess, 17.6ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.2ms\n",
      "Speed: 4.2ms preprocess, 21.2ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.1ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.3ms\n",
      "Speed: 3.7ms preprocess, 16.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.9ms\n",
      "Speed: 4.0ms preprocess, 19.9ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.6ms\n",
      "Speed: 2.6ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 3.5ms preprocess, 15.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 2.5ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.8ms\n",
      "Speed: 3.8ms preprocess, 20.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.8ms\n",
      "Speed: 3.5ms preprocess, 25.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.6ms\n",
      "Speed: 2.6ms preprocess, 14.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 3.7ms preprocess, 17.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 3.6ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 3.4ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 2.7ms preprocess, 13.6ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 4.0ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 3.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 2.6ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 3.6ms preprocess, 16.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.5ms\n",
      "Speed: 3.3ms preprocess, 17.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.7ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.4ms\n",
      "Speed: 3.3ms preprocess, 16.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.8ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 5.2ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.7ms preprocess, 17.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part2_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 2.6ms preprocess, 14.9ms inference, 9.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.9ms\n",
      "Speed: 3.5ms preprocess, 21.9ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 23.8ms\n",
      "Speed: 3.4ms preprocess, 23.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.3ms\n",
      "Speed: 3.5ms preprocess, 20.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.1ms\n",
      "Speed: 3.6ms preprocess, 22.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.7ms\n",
      "Speed: 3.5ms preprocess, 19.7ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 19.9ms\n",
      "Speed: 3.1ms preprocess, 19.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.0ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.9ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 24.3ms\n",
      "Speed: 4.2ms preprocess, 24.3ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.1ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 3.1ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 14.7ms preprocess, 18.3ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 3.0ms preprocess, 14.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 35.8ms\n",
      "Speed: 4.0ms preprocess, 35.8ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 23.6ms\n",
      "Speed: 2.9ms preprocess, 23.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 3.0ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 19.4ms\n",
      "Speed: 4.0ms preprocess, 19.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.2ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.1ms preprocess, 13.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.1ms\n",
      "Speed: 3.6ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.3ms preprocess, 16.7ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.0ms preprocess, 14.7ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 31.8ms\n",
      "Speed: 4.0ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.0ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.8ms\n",
      "Speed: 4.0ms preprocess, 25.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.7ms\n",
      "Speed: 3.2ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 3.9ms preprocess, 16.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.2ms\n",
      "Speed: 5.3ms preprocess, 19.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.3ms\n",
      "Speed: 3.1ms preprocess, 17.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.7ms\n",
      "Speed: 5.3ms preprocess, 22.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 18.1ms\n",
      "Speed: 3.7ms preprocess, 18.1ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 4.1ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.2ms\n",
      "Speed: 4.0ms preprocess, 21.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 4.0ms preprocess, 17.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 27.4ms\n",
      "Speed: 3.0ms preprocess, 27.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 2.9ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.2ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.3ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 2.9ms preprocess, 15.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.8ms preprocess, 15.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 3.0ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.4ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 3.1ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.9ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.8ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 4.5ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part3.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam01_drunken03_place03_night_summer_1136_4740_part3_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_summer_1283_1855.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_summer_1839_1981.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 18.8ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.4ms preprocess, 13.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.2ms\n",
      "Speed: 3.4ms preprocess, 14.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam03_drunken03_place03_night_winter_2733_3932_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam03_drunken03_place03_night_winter_2733_3932_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_winter_285_530.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 13.9ms\n",
      "Speed: 18.7ms preprocess, 13.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.8ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 3.6ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.4ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 13.3ms\n",
      "Speed: 2.7ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 4.0ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.1ms\n",
      "Speed: 3.6ms preprocess, 14.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 5.1ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 2.6ms preprocess, 14.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.8ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.5ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 2.9ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.9ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.7ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 3.8ms preprocess, 16.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 2.7ms preprocess, 14.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 3.5ms preprocess, 15.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 3.5ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 2.7ms preprocess, 16.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.2ms\n",
      "Speed: 2.6ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 2.6ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 3.3ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.5ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.6ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_winter_436_2936_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_winter_436_2936_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.3ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 3.4ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.5ms\n",
      "Speed: 4.5ms preprocess, 21.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 5.3ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 3.4ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.9ms preprocess, 16.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.8ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.7ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 3.3ms preprocess, 15.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 4.8ms preprocess, 16.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 3.3ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 5.0ms preprocess, 17.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.0ms\n",
      "Speed: 3.4ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 5.0ms preprocess, 16.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 3.5ms preprocess, 15.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 4.8ms preprocess, 16.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 3.5ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 5.2ms preprocess, 16.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.3ms\n",
      "Speed: 3.4ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.3ms\n",
      "Speed: 4.7ms preprocess, 16.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 3.3ms preprocess, 14.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.7ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.2ms\n",
      "Speed: 4.8ms preprocess, 25.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 3.4ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.4ms\n",
      "Speed: 4.8ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 3.5ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 5.0ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 31.9ms\n",
      "Speed: 3.4ms preprocess, 31.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.7ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 8.2ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.4ms\n",
      "Speed: 4.9ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 4.8ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.5ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.8ms preprocess, 16.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 5.3ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.8ms preprocess, 16.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 3.4ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 4.8ms preprocess, 16.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.7ms\n",
      "Speed: 3.3ms preprocess, 15.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.3ms\n",
      "Speed: 4.7ms preprocess, 16.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.4ms\n",
      "Speed: 3.5ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.8ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.3ms\n",
      "Speed: 3.3ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 4.7ms preprocess, 16.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.8ms\n",
      "Speed: 3.5ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.8ms preprocess, 16.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.2ms\n",
      "Speed: 3.4ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.8ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.9ms preprocess, 16.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 3.5ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.8ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.8ms preprocess, 15.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 4.3ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.2ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_winter_436_2936_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_winter_436_2936_part1_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_summer_270_1006.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 bench, 12.6ms\n",
      "Speed: 20.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.3ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.5ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.1ms\n",
      "Speed: 2.6ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.3ms\n",
      "Speed: 4.5ms preprocess, 19.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.7ms\n",
      "Speed: 2.5ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.9ms\n",
      "Speed: 3.6ms preprocess, 17.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.5ms\n",
      "Speed: 2.4ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-1_cam02_drunken03_place03_night_winter_213_1876_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-1_cam02_drunken03_place03_night_winter_213_1876_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_summer_414_640.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 19.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.5ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.6ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.5ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.2ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.9ms preprocess, 15.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.8ms\n",
      "Speed: 2.6ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.1ms\n",
      "Speed: 3.1ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.3ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.8ms\n",
      "Speed: 4.5ms preprocess, 17.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.8ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 4.7ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 4.5ms preprocess, 18.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_spring_4528_5885_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_spring_4528_5885_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_173_470.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_summer_2074_2553.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_spring_2479_2607.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.7ms\n",
      "Speed: 12.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 3.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 3.3ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.6ms preprocess, 18.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.3ms preprocess, 13.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.5ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 3.8ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.3ms\n",
      "Speed: 2.6ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 2.6ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.5ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 3.3ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_summer_189_3078_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_summer_189_3078_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.8ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.3ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.3ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.3ms\n",
      "Speed: 4.2ms preprocess, 16.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 3.1ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_summer_189_3078_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_summer_189_3078_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.6ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.3ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.8ms\n",
      "Speed: 3.3ms preprocess, 20.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.2ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 28.6ms\n",
      "Speed: 2.5ms preprocess, 28.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.4ms preprocess, 17.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.7ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.9ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 3.9ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 28.3ms\n",
      "Speed: 15.0ms preprocess, 28.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.7ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 2.9ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.8ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 3.0ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 3.8ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 2.9ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.0ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.8ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.7ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.0ms preprocess, 16.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.8ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 2.8ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 31.1ms\n",
      "Speed: 5.0ms preprocess, 31.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.2ms\n",
      "Speed: 4.7ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 19.0ms\n",
      "Speed: 2.4ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 21.8ms\n",
      "Speed: 2.5ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.6ms\n",
      "Speed: 3.5ms preprocess, 25.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.3ms preprocess, 16.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 3.5ms preprocess, 15.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.8ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 2.9ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 13.0ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.3ms\n",
      "Speed: 2.8ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.8ms preprocess, 15.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.9ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.1ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 5.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.9ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.4ms\n",
      "Speed: 2.8ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 4.1ms preprocess, 17.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 2.8ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 6.1ms preprocess, 18.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.3ms\n",
      "Speed: 2.9ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.0ms preprocess, 16.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 19.2ms\n",
      "Speed: 3.5ms preprocess, 19.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.1ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.1ms\n",
      "Speed: 3.0ms preprocess, 14.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.4ms\n",
      "Speed: 5.1ms preprocess, 20.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.2ms\n",
      "Speed: 3.5ms preprocess, 18.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.9ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.2ms\n",
      "Speed: 3.2ms preprocess, 15.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.8ms preprocess, 16.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.6ms\n",
      "Speed: 3.4ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.8ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 25.0ms\n",
      "Speed: 2.4ms preprocess, 25.0ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.4ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.4ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 handbag, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.2ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.4ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 handbag, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.3ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.4ms\n",
      "Speed: 3.3ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.8ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.6ms\n",
      "Speed: 2.8ms preprocess, 13.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 4.1ms preprocess, 15.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.4ms\n",
      "Speed: 3.2ms preprocess, 14.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.8ms\n",
      "Speed: 3.2ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_summer_189_3078_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_summer_189_3078_part2_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_spring_240_515.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_spring_1886_1982.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_148_549.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_spring_1536_1674.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_spring_1135_1323.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 12.8ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.8ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.7ms preprocess, 14.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 3.1ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.0ms\n",
      "Speed: 3.1ms preprocess, 16.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 3.4ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 14.7ms\n",
      "Speed: 3.3ms preprocess, 14.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 3.8ms preprocess, 15.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.2ms\n",
      "Speed: 4.5ms preprocess, 19.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.4ms preprocess, 14.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.4ms preprocess, 15.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 4.6ms preprocess, 15.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 15.6ms\n",
      "Speed: 3.5ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.7ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.5ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.6ms preprocess, 13.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_winter_4115_5522_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_winter_4115_5522_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 13.2ms\n",
      "Speed: 39.4ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.7ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.7ms\n",
      "Speed: 3.1ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.3ms\n",
      "Speed: 2.5ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.4ms preprocess, 14.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.6ms\n",
      "Speed: 3.5ms preprocess, 16.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.9ms\n",
      "Speed: 3.8ms preprocess, 14.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 3.5ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.3ms\n",
      "Speed: 2.7ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.4ms\n",
      "Speed: 3.4ms preprocess, 15.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.3ms\n",
      "Speed: 3.5ms preprocess, 21.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.2ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 kite, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam03_drunken03_place03_night_summer_156_1626_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam03_drunken03_place03_night_summer_156_1626_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_summer_4034_4540.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_winter_656_1284.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_spring_1118_1341.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 18.2ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.2ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.2ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 3.4ms preprocess, 15.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 4.5ms preprocess, 16.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.6ms\n",
      "Speed: 2.6ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.8ms\n",
      "Speed: 2.5ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.6ms\n",
      "Speed: 2.4ms preprocess, 15.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.3ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.1ms\n",
      "Speed: 4.5ms preprocess, 16.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.6ms preprocess, 15.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 3.5ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.7ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.8ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.8ms\n",
      "Speed: 2.4ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam01_drunken03_place03_night_summer_4111_5095_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam01_drunken03_place03_night_summer_4111_5095_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 18.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 3.6ms preprocess, 18.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.6ms preprocess, 17.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.2ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 clock, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.5ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.5ms preprocess, 17.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.2ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.7ms\n",
      "Speed: 2.7ms preprocess, 14.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.6ms preprocess, 17.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam01_drunken03_place03_night_winter_479_1427_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam01_drunken03_place03_night_winter_479_1427_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_summer_1397_1491.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_summer_1305_1703.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_summer_3565_4460.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_spring_2469_2704.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 19.9ms\n",
      "Speed: 30.4ms preprocess, 19.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.8ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.8ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 17.1ms\n",
      "Speed: 2.5ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 17.9ms\n",
      "Speed: 2.4ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.8ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.8ms preprocess, 14.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.5ms\n",
      "Speed: 2.4ms preprocess, 12.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 19.3ms\n",
      "Speed: 3.1ms preprocess, 19.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.4ms\n",
      "Speed: 4.3ms preprocess, 17.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 22.4ms\n",
      "Speed: 3.3ms preprocess, 22.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.2ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.6ms preprocess, 14.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.8ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.8ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.9ms\n",
      "Speed: 4.0ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.8ms\n",
      "Speed: 2.8ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.6ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.6ms\n",
      "Speed: 2.9ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.2ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 26.7ms\n",
      "Speed: 3.4ms preprocess, 26.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.6ms\n",
      "Speed: 3.2ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.0ms\n",
      "Speed: 4.7ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 25.9ms\n",
      "Speed: 3.2ms preprocess, 25.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 4.5ms preprocess, 16.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 14.6ms\n",
      "Speed: 2.3ms preprocess, 14.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 29.8ms\n",
      "Speed: 2.4ms preprocess, 29.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.7ms\n",
      "Speed: 2.3ms preprocess, 13.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.8ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.8ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 10.6ms\n",
      "Speed: 2.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.1ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.9ms\n",
      "Speed: 2.9ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.9ms\n",
      "Speed: 3.9ms preprocess, 16.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.5ms\n",
      "Speed: 2.9ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.0ms preprocess, 17.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.6ms\n",
      "Speed: 2.9ms preprocess, 15.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.8ms\n",
      "Speed: 3.9ms preprocess, 16.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.1ms\n",
      "Speed: 2.8ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.3ms\n",
      "Speed: 4.0ms preprocess, 18.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.8ms\n",
      "Speed: 3.1ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.0ms preprocess, 17.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 10.8ms\n",
      "Speed: 2.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.7ms\n",
      "Speed: 4.4ms preprocess, 18.7ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.2ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.8ms\n",
      "Speed: 4.5ms preprocess, 18.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 3.0ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.9ms preprocess, 16.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-1_cam02_drunken03_place03_night_winter_3063_4354_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-1_cam02_drunken03_place03_night_winter_3063_4354_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_summer_312_1066.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_summer_4053_4568.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_summer_1702_1852.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_summer_669_1113.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_summer_1950_2415.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 12.8ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 3.0ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.8ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.9ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.9ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 4.2ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.0ms\n",
      "Speed: 2.8ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.9ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 3.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.9ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.8ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.8ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.4ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 2.8ms preprocess, 15.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.8ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 2.9ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.9ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 17.0ms\n",
      "Speed: 3.5ms preprocess, 17.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.5ms\n",
      "Speed: 3.9ms preprocess, 14.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.8ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.0ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.9ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.9ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.2ms\n",
      "Speed: 3.9ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.8ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.2ms\n",
      "Speed: 2.9ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 4.1ms preprocess, 14.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 2.8ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.8ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.0ms\n",
      "Speed: 2.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.0ms\n",
      "Speed: 4.7ms preprocess, 18.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.1ms\n",
      "Speed: 2.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam02_drunken03_place03_night_winter_1597_3116_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam02_drunken03_place03_night_winter_1597_3116_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_spring_926_1230.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_winter_1875_2214.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_spring_2738_3480.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 14.3ms\n",
      "Speed: 18.3ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.9ms\n",
      "Speed: 3.4ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.3ms\n",
      "Speed: 3.7ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 5.0ms preprocess, 16.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.5ms preprocess, 14.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 5.1ms preprocess, 19.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 3.3ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.9ms\n",
      "Speed: 3.5ms preprocess, 14.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_winter_141_3004_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_winter_141_3004_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 3.2ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.7ms\n",
      "Speed: 2.6ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.6ms\n",
      "Speed: 2.8ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 4.0ms preprocess, 17.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.4ms\n",
      "Speed: 3.1ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.3ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 5.4ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 3.5ms preprocess, 18.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 3.0ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.8ms\n",
      "Speed: 3.1ms preprocess, 13.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.6ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.5ms\n",
      "Speed: 3.9ms preprocess, 16.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 2.8ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 3.9ms preprocess, 17.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.1ms\n",
      "Speed: 2.8ms preprocess, 15.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 3.9ms preprocess, 17.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 2.9ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 4.0ms preprocess, 16.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 2.9ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 4.0ms preprocess, 17.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 3.0ms preprocess, 15.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 3.9ms preprocess, 17.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.5ms\n",
      "Speed: 2.9ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.5ms\n",
      "Speed: 3.9ms preprocess, 17.5ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.6ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.9ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.8ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.8ms\n",
      "Speed: 2.3ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.6ms\n",
      "Speed: 2.3ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_winter_141_3004_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_winter_141_3004_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.8ms\n",
      "Speed: 2.7ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 3.4ms preprocess, 15.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.5ms\n",
      "Speed: 3.3ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 3.3ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.7ms\n",
      "Speed: 3.5ms preprocess, 15.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.7ms\n",
      "Speed: 2.6ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.7ms preprocess, 14.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 18.6ms\n",
      "Speed: 3.2ms preprocess, 18.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 15.9ms\n",
      "Speed: 3.1ms preprocess, 15.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.8ms\n",
      "Speed: 2.5ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam02_drunken03_place03_night_winter_141_3004_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam02_drunken03_place03_night_winter_141_3004_part2_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_209_356.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 15.8ms\n",
      "Speed: 19.1ms preprocess, 15.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.9ms\n",
      "Speed: 2.6ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.6ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.8ms\n",
      "Speed: 2.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.4ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.3ms\n",
      "Speed: 2.4ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.2ms\n",
      "Speed: 4.6ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 17.4ms\n",
      "Speed: 3.2ms preprocess, 17.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.9ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.7ms\n",
      "Speed: 3.8ms preprocess, 15.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 13.5ms\n",
      "Speed: 2.5ms preprocess, 13.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.5ms\n",
      "Speed: 3.2ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_spring_4649_6001_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_spring_4649_6001_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_summer_344_749.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 15.1ms\n",
      "Speed: 18.4ms preprocess, 15.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.7ms\n",
      "Speed: 3.6ms preprocess, 16.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.5ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 benchs, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 benchs, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 benchs, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 benchs, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 benchs, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 benchs, 11.2ms\n",
      "Speed: 2.6ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.6ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.5ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.7ms\n",
      "Speed: 2.6ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 3.6ms preprocess, 14.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 3.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 15.5ms\n",
      "Speed: 2.5ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 16.6ms\n",
      "Speed: 3.4ms preprocess, 16.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 21.9ms\n",
      "Speed: 3.5ms preprocess, 21.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.1ms\n",
      "Speed: 4.9ms preprocess, 16.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.5ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 14.4ms\n",
      "Speed: 3.2ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.9ms\n",
      "Speed: 4.7ms preprocess, 15.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 25.8ms\n",
      "Speed: 3.6ms preprocess, 25.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.6ms\n",
      "Speed: 2.6ms preprocess, 13.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 17.2ms\n",
      "Speed: 2.4ms preprocess, 17.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 2 persons, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.3ms\n",
      "Speed: 3.5ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.2ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.3ms\n",
      "Speed: 2.4ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.8ms\n",
      "Speed: 2.5ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 4.1ms preprocess, 14.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 25.2ms\n",
      "Speed: 3.3ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.5ms\n",
      "Speed: 3.8ms preprocess, 15.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 4.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-1_cam01_drunken03_place03_night_winter_2938_4297_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-1_cam01_drunken03_place03_night_winter_2938_4297_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_summer_3408_3737.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_3864_4319.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 16.2ms\n",
      "Speed: 18.5ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13.3ms\n",
      "Speed: 2.5ms preprocess, 13.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 15.1ms\n",
      "Speed: 2.3ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.6ms\n",
      "Speed: 2.4ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.7ms\n",
      "Speed: 2.3ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.5ms\n",
      "Speed: 2.7ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.0ms\n",
      "Speed: 4.6ms preprocess, 19.0ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.4ms\n",
      "Speed: 3.0ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.7ms\n",
      "Speed: 3.4ms preprocess, 16.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.3ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.9ms\n",
      "Speed: 2.5ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.8ms\n",
      "Speed: 2.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.5ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.4ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 20.4ms\n",
      "Speed: 4.7ms preprocess, 20.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.1ms\n",
      "Speed: 4.9ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.6ms\n",
      "Speed: 4.9ms preprocess, 14.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 14.8ms\n",
      "Speed: 3.5ms preprocess, 14.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_spring_158_3267_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_spring_158_3267_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.7ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.8ms\n",
      "Speed: 4.2ms preprocess, 18.8ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 2 backpacks, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.7ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.7ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.9ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.8ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.9ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.4ms\n",
      "Speed: 4.6ms preprocess, 18.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.4ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_spring_158_3267_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_spring_158_3267_part1_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.2ms\n",
      "Speed: 4.7ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-3_cam03_drunken03_place03_night_spring_158_3267_part2.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-3_cam03_drunken03_place03_night_spring_158_3267_part2_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 18.0ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.8ms\n",
      "Speed: 2.5ms preprocess, 12.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.8ms preprocess, 13.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 16.9ms\n",
      "Speed: 3.2ms preprocess, 16.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.8ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.1ms\n",
      "Speed: 2.7ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.2ms\n",
      "Speed: 2.6ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.3ms\n",
      "Speed: 4.7ms preprocess, 17.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.9ms\n",
      "Speed: 3.8ms preprocess, 13.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.7ms\n",
      "Speed: 3.4ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 12.9ms\n",
      "Speed: 2.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.7ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.4ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-1_cam03_drunken03_place03_night_winter_3034_4264_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-1_cam03_drunken03_place03_night_winter_3034_4264_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_spring_1719_1860.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_summer_1830_2530.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_winter_1217_1685.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_summer_1520_1678.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_spring_1472_1921.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_987_1251.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_summer_2648_3158.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 18.0ms\n",
      "Speed: 18.2ms preprocess, 18.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 2.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.8ms\n",
      "Speed: 2.4ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 10.7ms\n",
      "Speed: 2.3ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.8ms\n",
      "Speed: 4.8ms preprocess, 19.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.2ms\n",
      "Speed: 4.5ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.6ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.7ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 2.5ms preprocess, 12.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.6ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.1ms\n",
      "Speed: 3.5ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam03_drunken03_place03_night_winter_803_3075_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam03_drunken03_place03_night_winter_803_3075_part0_3fps_keypoints.mp4\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 16.6ms\n",
      "Speed: 3.4ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.8ms\n",
      "Speed: 3.2ms preprocess, 13.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.7ms\n",
      "Speed: 2.7ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.3ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.2ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.3ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.0ms\n",
      "Speed: 3.4ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.8ms\n",
      "Speed: 3.7ms preprocess, 15.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.7ms\n",
      "Speed: 3.5ms preprocess, 14.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 15.1ms\n",
      "Speed: 3.3ms preprocess, 15.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam03_drunken03_place03_night_winter_803_3075_part1.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam03_drunken03_place03_night_winter_803_3075_part1_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_winter_4161_4723.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_spring_911_1233.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 2 persons, 1 backpack, 1 microwave, 11.8ms\n",
      "Speed: 18.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 3.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 2.7ms preprocess, 13.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.5ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 14.4ms\n",
      "Speed: 3.4ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.6ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 microwave, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 bench, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.0ms\n",
      "Speed: 3.7ms preprocess, 17.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.7ms\n",
      "Speed: 4.8ms preprocess, 17.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 17.6ms\n",
      "Speed: 4.6ms preprocess, 17.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 16.3ms\n",
      "Speed: 2.5ms preprocess, 16.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 15.9ms\n",
      "Speed: 2.6ms preprocess, 15.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.5ms preprocess, 18.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 14.9ms\n",
      "Speed: 3.6ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 16.2ms\n",
      "Speed: 2.7ms preprocess, 16.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 16.7ms\n",
      "Speed: 3.2ms preprocess, 16.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 4.0ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 15.5ms\n",
      "Speed: 2.9ms preprocess, 15.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.3ms\n",
      "Speed: 3.6ms preprocess, 13.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 18.1ms\n",
      "Speed: 4.7ms preprocess, 18.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.7ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.6ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.2ms\n",
      "Speed: 2.7ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.6ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 12.3ms\n",
      "Speed: 2.5ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 toilet, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 11.4ms\n",
      "Speed: 2.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.5ms\n",
      "Speed: 2.6ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 suitcase, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 1 suitcase, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-4_cam02_drunken03_place03_night_summer_126_1653_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-4_cam02_drunken03_place03_night_summer_126_1653_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_spring_2943_3132.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_summer_955_1187.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_spring_2376_2612.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_3868_4449.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_spring_522_884.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_spring_2604_3095.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam02_drunken03_place03_night_winter_154_555.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam03_drunken03_place03_night_spring_4060_4259.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam01_drunken03_place03_night_spring_2876_3003.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_3511_3685.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam03_drunken03_place03_night_spring_247_424.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam02_drunken03_place03_night_summer_1608_1845.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam02_drunken03_place03_night_summer_3221_3478.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_winter_210_436.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-2_cam02_drunken03_place03_night_winter_1380_1691.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam03_drunken03_place03_night_spring_1398_2004.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-6_cam01_drunken03_place03_night_summer_1948_2098.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam03_drunken03_place03_night_summer_3939_4471.mp4: Video duration is less than 30 seconds.\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 18.8ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.5ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.4ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.3ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.1ms\n",
      "Speed: 2.3ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 10.8ms\n",
      "Speed: 2.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.4ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.3ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.4ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.4ms\n",
      "Speed: 3.5ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.5ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.3ms\n",
      "Speed: 2.7ms preprocess, 12.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.9ms\n",
      "Speed: 3.6ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 13.4ms\n",
      "Speed: 2.6ms preprocess, 13.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 19.6ms\n",
      "Speed: 4.4ms preprocess, 19.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.9ms\n",
      "Speed: 2.6ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.4ms preprocess, 12.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "\n",
      "0: 384x640 1 person, 1 backpack, 11.7ms\n",
      "Speed: 2.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x800 1 person, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 800)\n",
      "CSV 저장 완료: /home/alpaco/hsb/11_25project/OSHcsv_combined6/278-2_cam02_drunken03_place03_night_spring_210_1287_part0.mp4_combined.csv\n",
      "키포인트 시각화 완료: /home/alpaco/hsb/11_25project/OSH6/278-2_cam02_drunken03_place03_night_spring_210_1287_part0_3fps_keypoints.mp4\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_summer_2483_3332.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-1_cam01_drunken03_place03_night_summer_1163_1382.mp4: Video duration is less than 30 seconds.\n",
      "Skipping /home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06/278-4_cam01_drunken03_place03_night_winter_3100_3743.mp4: Video duration is less than 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "# 모델 설정\n",
    "model = YOLO('yolov8l.pt')\n",
    "pose = YOLO('yolov8l-pose.pt')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "pose.to(device)\n",
    "# 처리된 동영상 기록 파일\n",
    "PROCESSED_VIDEOS_LOG = 'OSHprocessed_videos.json'\n",
    "def load_processed_videos(log_path):\n",
    "    \"\"\"처리된 동영상 기록 로드\"\"\"\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, 'r') as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "def save_processed_videos(log_path, processed_videos):\n",
    "    \"\"\"처리된 동영상 기록 저장\"\"\"\n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(list(processed_videos), f)\n",
    "def split_video(input_video_path, segment_dir, segment_duration=30):\n",
    "    \"\"\"동영상을 정확히 30초 단위로 잘라 저장\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_duration = total_frames / input_fps if input_fps > 0 else 0\n",
    "    if video_duration < segment_duration:\n",
    "        print(f\"Skipping {input_video_path}: Video duration is less than {segment_duration} seconds.\")\n",
    "        cap.release()\n",
    "        return []\n",
    "    segments = []\n",
    "    base_name = os.path.splitext(os.path.basename(input_video_path))[0]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    segment_idx = 0\n",
    "    for start_frame in range(0, total_frames, int(segment_duration * input_fps)):\n",
    "        end_frame = start_frame + int(segment_duration * input_fps)\n",
    "        if end_frame > total_frames:\n",
    "            break  # 30초 미만 남은 부분 무시\n",
    "        segment_path = os.path.join(segment_dir, f\"{base_name}_part{segment_idx}.mp4\")\n",
    "        out = cv2.VideoWriter(segment_path, fourcc, input_fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # 시작 프레임 설정\n",
    "        for _ in range(int(segment_duration * input_fps)):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "        segments.append(segment_path)\n",
    "        segment_idx += 1\n",
    "    cap.release()\n",
    "    return segments\n",
    "def convert_to_3fps(input_video_path, output_dir):\n",
    "    \"\"\"동영상을 3FPS로 변환하여 저장\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    output_fps = 3\n",
    "    frame_skip = int(input_fps / output_fps)\n",
    "    base_name = os.path.splitext(os.path.basename(input_video_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_name}_3fps.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, output_fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % frame_skip == 0:\n",
    "            out.write(frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    return output_path\n",
    "def process_video(input_video_path, csv_path):\n",
    "    \"\"\"동영상을 처리하고 키포인트 데이터를 단일 CSV에 저장, 시각화\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    output_path = input_video_path.replace('.mp4', '_keypoints.mp4')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, int(cap.get(cv2.CAP_PROP_FPS)),\n",
    "                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    num_keypoints = 17\n",
    "    header = ['ID', 'frame'] + [f'x{i}' for i in range(1, num_keypoints + 1)] + [f'y{i}' for i in range(1, num_keypoints + 1)]\n",
    "    keypoint_data = {}\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = model.track(frame, persist=True, conf=0.1)\n",
    "        for result in results:\n",
    "            if result.boxes.id is not None:\n",
    "                for box, track_id, cls_id in zip(result.boxes.xyxy, result.boxes.id, result.boxes.cls):\n",
    "                    if int(cls_id) == 0:  # 사람 클래스만 필터링\n",
    "                         # 기존 프레임 복사 후 바운딩 박스 외부를 검정색으로 마스킹\n",
    "                        x1, y1, x2, y2 = map(int, box)\n",
    "                        masked_frame = np.zeros_like(frame)  # 전체 검정색\n",
    "                        masked_frame[y1:y2, x1:x2] = frame[y1:y2, x1:x2]  # 바운딩 박스 영역만 원본 이미지 유지\n",
    "                        keypoints_results = pose(frame, imgsz=800)\n",
    "                        if hasattr(keypoints_results[0], 'keypoints'):\n",
    "                            keypoints = keypoints_results[0].keypoints.xy.cpu().numpy()[0]\n",
    "                            if keypoints.shape[0] >= num_keypoints:\n",
    "                                track_id_str = str(track_id)\n",
    "                                if track_id_str not in keypoint_data:\n",
    "                                    keypoint_data[track_id_str] = {}\n",
    "                                keypoint_data[track_id_str][frame_idx] = [int(coord) for point in keypoints for coord in point]\n",
    "                                # 키포인트 시각화\n",
    "                                for i, (x, y) in enumerate(keypoints):\n",
    "                                    x, y = int(x), int(y)\n",
    "                                    cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)  # 초록색 원\n",
    "                                    cv2.putText(frame, f\"{i+1}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "                                # 사람 ID 및 키포인트 정보 텍스트 추가\n",
    "                                cv2.putText(frame, f\"ID: {track_id_str}\",\n",
    "                                            (int(box[0]), int(box[1])-10),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "        out.write(frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    if not keypoint_data:\n",
    "        print(f\"No keypoint data found for {input_video_path}. Skipping CSV generation.\")\n",
    "        return False\n",
    "    # CSV 저장\n",
    "    with open(csv_path, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        for track_id, frame_data in sorted(keypoint_data.items()):\n",
    "            for frame_idx in range(total_frames):\n",
    "                row = [f'ID: {track_id}', frame_idx] + frame_data.get(frame_idx, [0] * (num_keypoints * 2))\n",
    "                writer.writerow(row)\n",
    "    print(f\"CSV 저장 완료: {csv_path}\")\n",
    "    print(f\"키포인트 시각화 완료: {output_path}\")\n",
    "    return True\n",
    "# 메인 실행\n",
    "video_dir = '/home/alpaco/project/drunk_prj/data/videofile/confirm_video_file/totter/totter06'\n",
    "output_dir = '/home/alpaco/hsb/11_25project/OSH6'\n",
    "csv_dir = '/home/alpaco/hsb/11_25project/OSHcsv_combined6'\n",
    "segment_dir = '/home/alpaco/hsb/11_25project/cutted_vid6'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "os.makedirs(segment_dir, exist_ok=True)\n",
    "processed_videos = load_processed_videos(PROCESSED_VIDEOS_LOG)\n",
    "for video in os.listdir(video_dir):\n",
    "    if video in processed_videos:\n",
    "        print(f\"Skipping {video}: already processed.\")\n",
    "        continue\n",
    "    input_video_path = os.path.join(video_dir, video)\n",
    "    segments = split_video(input_video_path, segment_dir)\n",
    "    for segment in segments:\n",
    "        converted_video_path = convert_to_3fps(segment, output_dir)\n",
    "        csv_path = os.path.join(csv_dir, f\"{os.path.basename(segment)}_combined.csv\")\n",
    "        processed = process_video(converted_video_path, csv_path)\n",
    "        if processed:\n",
    "            processed_videos.add(video)\n",
    "save_processed_videos(PROCESSED_VIDEOS_LOG, processed_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 209-5_cam03_drunken01_place03_night_winter_381_2313_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-1_cam01_drunken01_place03_night_winter_6218_7538_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-3_cam03_drunken01_place03_night_winter_5752_6663_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-3_cam03_drunken01_place03_night_winter_443_4252_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-5_cam01_drunken01_place03_night_winter_2378_3736_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-3_cam03_drunken01_place03_night_winter_443_4252_part3.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-3_cam01_drunken01_place03_night_winter_382_4237_part3.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-5_cam02_drunken01_place03_night_summer_5991_7044_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-3_cam03_drunken01_place03_night_winter_443_4252_part2.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-5_cam01_drunken01_place03_night_winter_394_2328_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-1_cam03_drunken01_place03_night_winter_5808_7585_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 209-5_cam03_drunken01_place03_night_winter_381_2313_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_winter_3804_6397_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam01_drunken04_place02_night_summer_4043_6527_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam01_drunken01_place03_night_spring_7528_8836_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam02_drunken01_place03_night_summer_3382_5565_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam01_drunken01_place03_night_summer_3170_5510_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam02_drunken01_place03_night_spring_7555_8790_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 212-5_cam03_drunken03_place03_night_summer_1644_2608_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam01_drunken04_place02_night_winter_3801_6423_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_winter_3804_6397_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_summer_3995_6473_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam02_drunken01_place03_night_spring_3083_4004_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam02_drunken04_place02_night_spring_3440_5836_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam02_drunken04_place02_night_summer_4001_6485_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam02_drunken04_place02_night_summer_4001_6485_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-7_cam01_drunken01_place03_night_spring_4584_5996_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam02_drunken04_place02_night_spring_3440_5836_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_spring_3372_5824_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam01_drunken04_place02_night_winter_3801_6423_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-7_cam02_drunken01_place03_night_summer_5929_7621_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-7_cam01_drunken01_place03_night_summer_5890_7544_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam01_drunken01_place03_night_summer_3170_5510_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-9_cam02_drunken01_place03_night_spring_4006_5174_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_spring_3372_5824_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 623-2_cam01_drunken04_place02_night_spring_3310_4503_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam01_drunken01_place03_night_spring_3115_4019_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_summer_7269_8601_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam02_drunken04_place02_night_winter_3778_6383_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-8_cam02_drunken01_place03_night_summer_3382_5565_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam02_drunken04_place02_night_winter_3778_6383_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam01_drunken04_place02_night_summer_4043_6527_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-7_cam02_drunken01_place03_night_spring_4421_5879_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam01_drunken04_place02_night_spring_3377_5818_part1.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 618-9_cam01_drunken01_place03_night_spring_3807_4969_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 214-2_cam03_drunken04_place02_night_summer_3995_6473_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 623-2_cam02_drunken04_place02_night_summer_3827_5457_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 216-4_cam02_drunken02_place01_night_spring_194_1700_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 217-6_cam03_drunken04_place03_night_spring_4523_5987_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 216-4_cam01_drunken02_place01_night_summer_243_1404_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 217-6_cam02_drunken04_place03_night_spring_4527_5936_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 229-2_cam02_drunken04_place03_night_summer_137_1066_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 229-5_cam03_drunken04_place03_night_winter_171_1180_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 229-5_cam01_drunken04_place03_night_winter_156_1119_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 218-5_cam02_drunken01_place03_night_spring_541_1523_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 229-5_cam02_drunken04_place03_night_summer_122_1208_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-3_cam01_drunken03_place02_night_summer_1205_2292_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam02_drunken03_place02_night_summer_4693_5799_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam02_drunken03_place02_night_spring_7390_8709_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam02_drunken03_place02_night_winter_1594_3038_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam03_drunken03_place02_night_spring_7371_8417_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam01_drunken03_place02_night_summer_1227_2149_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam03_drunken03_place02_night_winter_1589_3053_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-2_cam01_drunken03_place02_night_winter_8030_8939_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam03_drunken03_place02_night_summer_4668_5697_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-2_cam02_drunken03_place02_night_spring_7694_8961_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam02_drunken03_place02_night_winter_8296_9225_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam01_drunken03_place02_night_winter_4408_5592_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-2_cam02_drunken03_place02_night_winter_7950_8870_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam01_drunken03_place02_night_summer_4699_5755_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam02_drunken03_place02_night_winter_4418_5531_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam03_drunken03_place02_night_spring_2435_3361_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam03_drunken03_place02_night_winter_4353_5577_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam03_drunken03_place02_night_summer_7625_8630_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam02_drunken03_place02_night_summer_7436_8740_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-3_cam01_drunken03_place02_night_winter_7568_8475_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam01_drunken03_place02_night_spring_7460_8736_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-2_cam01_drunken03_place02_night_spring_7893_9008_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-2_cam03_drunken03_place02_night_spring_7702_8943_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam02_drunken03_place02_night_spring_3793_4965_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-4_cam01_drunken03_place02_night_winter_1681_3116_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 233-6_cam03_drunken03_place02_night_spring_3786_4969_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 278-4_cam01_drunken03_place03_night_summer_133_1529_part0.mp4_combined.csv: Duplicate frames found within the file.\n",
      "Skipping 278-3_cam02_drunken03_place03_night_spring_173_3282_part2.mp4_combined.csv: Duplicate frames found within the file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 디렉토리 설정\n",
    "input_dirs = ['/home/alpaco/hsb/11_25project/csv_combined','/home/alpaco/hsb/11_25project/OSHcsv_combined2','/home/alpaco/hsb/11_25project/OSHcsv_combined3','/home/alpaco/hsb/11_25project/OSHcsv_combined4','/home/alpaco/hsb/11_25project/OSHcsv_combined5','/home/alpaco/hsb/11_25project/OSHcsv_combined6']\n",
    "output_dir = '/home/alpaco/project/drunk_prj/data/chroma3frame'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "duplicates = []\n",
    "# CSV 파일 처리\n",
    "for input_dir in input_dirs:\n",
    "    for csv_file in os.listdir(input_dir):\n",
    "        input_path = os.path.join(input_dir, csv_file)\n",
    "        df = pd.read_csv(input_path)\n",
    "\n",
    "        if 'frame' not in df.columns:\n",
    "            print(f\"Skipping {csv_file}: 'frame' column not found.\")\n",
    "            continue\n",
    "\n",
    "        # 현재 CSV 내부에서 중복된 frame 확인\n",
    "        if df['frame'].duplicated().any():  # 중복된 값이 하나라도 있으면\n",
    "            print(f\"Skipping {csv_file}: Duplicate frames found within the file.\")\n",
    "            duplicates.append(input_path)\n",
    "            continue\n",
    "\n",
    "        # # 중복이 없는 경우 저장\n",
    "        # output_path = os.path.join(output_dir, csv_file)\n",
    "        # df.to_csv(output_path, index=False)\n",
    "        # print(f\"Saved {csv_file} to {output_dir}.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 및 출력 디렉토리 설정\n",
    "input_folder = '/home/alpaco/project/drunk_prj/data/chroma3frame'\n",
    "output_folder = '/home/alpaco/project/drunk_prj/data/chroma3frame0넣기'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 입력 폴더의 모든 파일 반복 처리\n",
    "for csv_file in os.listdir(input_folder):\n",
    "    # CSV 파일만 처리\n",
    "    if not csv_file.endswith('.csv'):\n",
    "        continue\n",
    "\n",
    "    min_frame, max_frame = 0,99\n",
    "\n",
    "    # CSV 파일 읽기\n",
    "    csv_path = os.path.join(input_folder, csv_file)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"CSV 파일 읽기 성공: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 파일을 읽을 수 없습니다: {csv_path}. 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열 확인\n",
    "    if 'ID' not in df.columns:\n",
    "        print(f\"'ID' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'frame' 열 확인\n",
    "    if 'frame' not in df.columns:\n",
    "        print(f\"'frame' 열이 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 'label' 열을 기준으로 그룹화 및 프레임 보정\n",
    "    processed_dfs = []\n",
    "    for label, group in df.groupby('ID'):\n",
    "        # 전체 프레임 범위 생성\n",
    "        full_frames = pd.DataFrame({'frame': range(min_frame, max_frame + 1)})\n",
    "\n",
    "        # 기존 데이터와 병합하여 누락된 프레임 추가\n",
    "        merged = pd.merge(full_frames, group, on='frame', how='left')\n",
    "\n",
    "        # 누락된 값 채우기\n",
    "        for col in merged.columns:\n",
    "            if col != 'frame' and col != 'label':  # frame과 label 열은 유지\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "        merged['ID'] = merged['ID'].fillna(label)\n",
    "\n",
    "        # 처리된 데이터 저장\n",
    "        processed_dfs.append(merged)\n",
    "\n",
    "    # 그룹 데이터 병합\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"처리된 데이터가 없습니다: {csv_path}. 파일을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 결과 저장 경로\n",
    "    output_path = os.path.join(output_folder, csv_file)\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"처리 완료: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"결과를 저장하는 동안 오류 발생: {output_path}. 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1737.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>1675.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>893.0</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>1654.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>821.0</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>1580.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>802.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>735.0</td>\n",
       "      <td>1341.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54219</th>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54220</th>\n",
       "      <td>251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54221</th>\n",
       "      <td>252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54222</th>\n",
       "      <td>253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54223</th>\n",
       "      <td>254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54224 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame      x1     y1      x2     y2   x3   y3      x4     y4   x5  ...  \\\n",
       "0          6  1710.0  711.0  1715.0  701.0  0.0  0.0  1737.0  706.0  0.0  ...   \n",
       "1          7     0.0    0.0     0.0    0.0  0.0  0.0  1621.0  659.0  0.0  ...   \n",
       "2          8     0.0    0.0  1499.0  601.0  0.0  0.0  1516.0  602.0  0.0  ...   \n",
       "3          9     0.0    0.0     0.0    0.0  0.0  0.0  1430.0  553.0  0.0  ...   \n",
       "4         10     0.0    0.0     0.0    0.0  0.0  0.0  1352.0  500.0  0.0  ...   \n",
       "...      ...     ...    ...     ...    ...  ...  ...     ...    ...  ...  ...   \n",
       "54219    250     0.0    0.0     0.0    0.0  0.0  0.0     0.0    0.0  0.0  ...   \n",
       "54220    251     0.0    0.0     0.0    0.0  0.0  0.0     0.0    0.0  0.0  ...   \n",
       "54221    252     0.0    0.0     0.0    0.0  0.0  0.0     0.0    0.0  0.0  ...   \n",
       "54222    253     0.0    0.0     0.0    0.0  0.0  0.0     0.0    0.0  0.0  ...   \n",
       "54223    254     0.0    0.0     0.0    0.0  0.0  0.0     0.0    0.0  0.0  ...   \n",
       "\n",
       "          y14     x15     y15     x16     y16     x17     y17  label  y  \\\n",
       "0      1031.0  1675.0  1000.0  1720.0  1076.0  1660.0  1052.0    1.0  0   \n",
       "1       893.0  1648.0   919.0  1480.0   939.0  1654.0  1026.0    1.0  0   \n",
       "2       821.0  1538.0   836.0  1461.0   891.0  1580.0   926.0    1.0  0   \n",
       "3       802.0  1390.0   728.0  1461.0   884.0  1360.0   785.0    1.0  0   \n",
       "4       735.0  1341.0   713.0  1376.0   797.0  1329.0   777.0    1.0  0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...    ... ..   \n",
       "54219     0.0     0.0     0.0     0.0     0.0     0.0     0.0    6.0  0   \n",
       "54220     0.0     0.0     0.0     0.0     0.0     0.0     0.0    6.0  0   \n",
       "54221     0.0     0.0     0.0     0.0     0.0     0.0     0.0    6.0  0   \n",
       "54222     0.0     0.0     0.0     0.0     0.0     0.0     0.0    6.0  0   \n",
       "54223     0.0     0.0     0.0     0.0     0.0     0.0     0.0    6.0  0   \n",
       "\n",
       "                                             FILENAME  \n",
       "0       C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "1       C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "2       C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "3       C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "4       C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "...                                               ...  \n",
       "54219  C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6  \n",
       "54220  C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6  \n",
       "54221  C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6  \n",
       "54222  C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6  \n",
       "54223  C_32_11_smp_su_09-11_15-01-00_a_aft_DF2_label6  \n",
       "\n",
       "[54224 rows x 38 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal = pd.DataFrame()\n",
    "for csv_name in os.listdir('/home/alpaco/project/drunk_prj/data/normal_editted_Final'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/normal_editted_Final',csv_name)\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data['y']=0\n",
    "    data['FILENAME'] =csv_name.split(\".\")[0]\n",
    "    num_cols = data.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    data[num_cols] = data[num_cols].clip(lower=0)\n",
    "    normal = pd.concat([normal,data],ignore_index=True)\n",
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>y13</th>\n",
       "      <th>y14</th>\n",
       "      <th>y15</th>\n",
       "      <th>y16</th>\n",
       "      <th>y17</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(274.)</td>\n",
       "      <td>217-2_cam02_drunken04_place03_night_summer_153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>...</td>\n",
       "      <td>1912</td>\n",
       "      <td>469</td>\n",
       "      <td>1948</td>\n",
       "      <td>426</td>\n",
       "      <td>2061</td>\n",
       "      <td>488</td>\n",
       "      <td>2132</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(274.)</td>\n",
       "      <td>217-2_cam02_drunken04_place03_night_summer_153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>306</td>\n",
       "      <td>...</td>\n",
       "      <td>1838</td>\n",
       "      <td>483</td>\n",
       "      <td>1898</td>\n",
       "      <td>615</td>\n",
       "      <td>1988</td>\n",
       "      <td>492</td>\n",
       "      <td>2093</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(274.)</td>\n",
       "      <td>217-2_cam02_drunken04_place03_night_summer_153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>...</td>\n",
       "      <td>1794</td>\n",
       "      <td>614</td>\n",
       "      <td>1818</td>\n",
       "      <td>591</td>\n",
       "      <td>1965</td>\n",
       "      <td>578</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(274.)</td>\n",
       "      <td>217-2_cam02_drunken04_place03_night_summer_153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "      <td>...</td>\n",
       "      <td>1785</td>\n",
       "      <td>644</td>\n",
       "      <td>1752</td>\n",
       "      <td>603</td>\n",
       "      <td>1928</td>\n",
       "      <td>792</td>\n",
       "      <td>1902</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(274.)</td>\n",
       "      <td>217-2_cam02_drunken04_place03_night_summer_153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24195</th>\n",
       "      <td>95</td>\n",
       "      <td>2533</td>\n",
       "      <td>955</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2528</td>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2509</td>\n",
       "      <td>...</td>\n",
       "      <td>1190</td>\n",
       "      <td>2517</td>\n",
       "      <td>1193</td>\n",
       "      <td>2516</td>\n",
       "      <td>1287</td>\n",
       "      <td>2509</td>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24196</th>\n",
       "      <td>96</td>\n",
       "      <td>2532</td>\n",
       "      <td>953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2527</td>\n",
       "      <td>944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2508</td>\n",
       "      <td>...</td>\n",
       "      <td>1191</td>\n",
       "      <td>2516</td>\n",
       "      <td>1194</td>\n",
       "      <td>2516</td>\n",
       "      <td>1287</td>\n",
       "      <td>2508</td>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197</th>\n",
       "      <td>97</td>\n",
       "      <td>2528</td>\n",
       "      <td>950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2523</td>\n",
       "      <td>941</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2505</td>\n",
       "      <td>...</td>\n",
       "      <td>1192</td>\n",
       "      <td>2517</td>\n",
       "      <td>1194</td>\n",
       "      <td>2516</td>\n",
       "      <td>1286</td>\n",
       "      <td>2507</td>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24198</th>\n",
       "      <td>98</td>\n",
       "      <td>2529</td>\n",
       "      <td>948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2524</td>\n",
       "      <td>940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2503</td>\n",
       "      <td>...</td>\n",
       "      <td>1192</td>\n",
       "      <td>2518</td>\n",
       "      <td>1194</td>\n",
       "      <td>2515</td>\n",
       "      <td>1286</td>\n",
       "      <td>2509</td>\n",
       "      <td>1293</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24199</th>\n",
       "      <td>99</td>\n",
       "      <td>2527</td>\n",
       "      <td>951</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2522</td>\n",
       "      <td>942</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2503</td>\n",
       "      <td>...</td>\n",
       "      <td>1191</td>\n",
       "      <td>2517</td>\n",
       "      <td>1193</td>\n",
       "      <td>2517</td>\n",
       "      <td>1284</td>\n",
       "      <td>2509</td>\n",
       "      <td>1290</td>\n",
       "      <td>1</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24200 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame    x1   x2  x3  x4    x5   x6  x7  x8    x9  ...   y11   y12  \\\n",
       "0          0     0    0   0   0     0    0   0   0     0  ...     0     0   \n",
       "1          1     0    0   0   0     0    0   0   0   209  ...  1912   469   \n",
       "2          2     0    0   0   0     0    0   0   0   306  ...  1838   483   \n",
       "3          3     0    0   0   0     0    0   0   0   343  ...  1794   614   \n",
       "4          4     0    0   0   0     0    0   0   0   387  ...  1785   644   \n",
       "...      ...   ...  ...  ..  ..   ...  ...  ..  ..   ...  ...   ...   ...   \n",
       "24195     95  2533  955   0   0  2528  946   0   0  2509  ...  1190  2517   \n",
       "24196     96  2532  953   0   0  2527  944   0   0  2508  ...  1191  2516   \n",
       "24197     97  2528  950   0   0  2523  941   0   0  2505  ...  1192  2517   \n",
       "24198     98  2529  948   0   0  2524  940   0   0  2503  ...  1192  2518   \n",
       "24199     99  2527  951   0   0  2522  942   0   0  2503  ...  1191  2517   \n",
       "\n",
       "        y13   y14   y15   y16   y17  y             label  \\\n",
       "0         0     0     0     0     0  1  ID: tensor(274.)   \n",
       "1      1948   426  2061   488  2132  1  ID: tensor(274.)   \n",
       "2      1898   615  1988   492  2093  1  ID: tensor(274.)   \n",
       "3      1818   591  1965   578  2018  1  ID: tensor(274.)   \n",
       "4      1752   603  1928   792  1902  1  ID: tensor(274.)   \n",
       "...     ...   ...   ...   ...   ... ..               ...   \n",
       "24195  1193  2516  1287  2509  1294  1  ID: tensor(602.)   \n",
       "24196  1194  2516  1287  2508  1294  1  ID: tensor(602.)   \n",
       "24197  1194  2516  1286  2507  1292  1  ID: tensor(602.)   \n",
       "24198  1194  2515  1286  2509  1293  1  ID: tensor(602.)   \n",
       "24199  1193  2517  1284  2509  1290  1  ID: tensor(602.)   \n",
       "\n",
       "                                                FILENAME  \n",
       "0      217-2_cam02_drunken04_place03_night_summer_153...  \n",
       "1      217-2_cam02_drunken04_place03_night_summer_153...  \n",
       "2      217-2_cam02_drunken04_place03_night_summer_153...  \n",
       "3      217-2_cam02_drunken04_place03_night_summer_153...  \n",
       "4      217-2_cam02_drunken04_place03_night_summer_153...  \n",
       "...                                                  ...  \n",
       "24195  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24196  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24197  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24198  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24199  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "\n",
       "[24200 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "Combined = pd.DataFrame()\n",
    "chroma =pd.DataFrame()\n",
    "for csv_name in os.listdir('/home/alpaco/project/drunk_prj/data/chroma3frame0넣기'):\n",
    "    csv_path = os.path.join('/home/alpaco/project/drunk_prj/data/chroma3frame0넣기',csv_name)\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data['y']=1\n",
    "    data['label']=data['ID']\n",
    "    data = data.drop(columns=['ID'])\n",
    "    data['FILENAME'] = csv_name.split(\".\")[0]\n",
    "    num_cols = data.select_dtypes(include=['number']).columns  # 숫자형 열만 선택\n",
    "    data[num_cols] = data[num_cols].clip(lower=0)\n",
    "    chroma = pd.concat([chroma,data],ignore_index=True)\n",
    "\n",
    "chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined = pd.concat([normal,chroma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path =  '/home/alpaco/project/jsw_model/1126'\n",
    "output_path = '/home/alpaco/project/jsw_model/30s_1126'\n",
    "os.makedirs(output_path,exist_ok=True)\n",
    "for i in os.listdir(input_path):\n",
    "    csv_file = os.path.join(input_path,i)\n",
    "    data = pd.read_csv(csv_file)\n",
    "    first_frame = data['frame'].min()\n",
    "    last_frame = data['frame'].max()\n",
    "    \n",
    "    if int(last_frame)-int(first_frame) >90:\n",
    "        full_frames = pd.DataFrame({'frame': range(first_frame, last_frame + 1)})\n",
    "\n",
    "    else:\n",
    "        full_frames = pd.DataFrame({'frame': range(first_frame, first_frame + 91)})\n",
    "    # 기존 데이터와 병합하여 누락된 프레임 추가\n",
    "    merged = pd.merge(full_frames, data, on='frame', how='left')\n",
    "\n",
    "    # 누락된 값 채우기\n",
    "    for col in merged.columns:\n",
    "        if col != 'frame' and col != 'label':  # frame과 label 열은 유지\n",
    "            merged[col] = merged[col].fillna(0)\n",
    "    merged['label'] = merged['label'].fillna(label)\n",
    "\n",
    "    # 처리된 데이터 저장\n",
    "    togo = os.path.join(output_path,i)\n",
    "    merged.to_csv(togo,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반인 조정한 것으로 다시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1737.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>1675.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>893.0</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>1654.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>821.0</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>1580.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>802.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>735.0</td>\n",
       "      <td>1341.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24195</th>\n",
       "      <td>95</td>\n",
       "      <td>2533.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2509.0</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24196</th>\n",
       "      <td>96</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>1067.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2508.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24197</th>\n",
       "      <td>97</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2523.0</td>\n",
       "      <td>2523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2507.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24198</th>\n",
       "      <td>98</td>\n",
       "      <td>2529.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2526.0</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2515.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2509.0</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>1293.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24199</th>\n",
       "      <td>99</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>1064.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2509.0</td>\n",
       "      <td>2496.0</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78424 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame      x1      y1      x2     y2   x3   y3      x4      y4      x5  \\\n",
       "0          6  1710.0   711.0  1715.0  701.0  0.0  0.0  1737.0   706.0     0.0   \n",
       "1          7     0.0     0.0     0.0    0.0  0.0  0.0  1621.0   659.0     0.0   \n",
       "2          8     0.0     0.0  1499.0  601.0  0.0  0.0  1516.0   602.0     0.0   \n",
       "3          9     0.0     0.0     0.0    0.0  0.0  0.0  1430.0   553.0     0.0   \n",
       "4         10     0.0     0.0     0.0    0.0  0.0  0.0  1352.0   500.0     0.0   \n",
       "...      ...     ...     ...     ...    ...  ...  ...     ...     ...     ...   \n",
       "24195     95  2533.0  1069.0   955.0    0.0  0.0  0.0     0.0  2522.0  2528.0   \n",
       "24196     96  2532.0  1067.0   953.0    0.0  0.0  0.0     0.0  2527.0  2527.0   \n",
       "24197     97  2528.0  1063.0   950.0    0.0  0.0  0.0     0.0  2523.0  2523.0   \n",
       "24198     98  2529.0  1065.0   948.0    0.0  0.0  0.0     0.0  2526.0  2524.0   \n",
       "24199     99  2527.0  1064.0   951.0    0.0  0.0  0.0     0.0  2522.0  2522.0   \n",
       "\n",
       "       ...     y14     x15     y15     x16     y16     x17     y17  \\\n",
       "0      ...  1031.0  1675.0  1000.0  1720.0  1076.0  1660.0  1052.0   \n",
       "1      ...   893.0  1648.0   919.0  1480.0   939.0  1654.0  1026.0   \n",
       "2      ...   821.0  1538.0   836.0  1461.0   891.0  1580.0   926.0   \n",
       "3      ...   802.0  1390.0   728.0  1461.0   884.0  1360.0   785.0   \n",
       "4      ...   735.0  1341.0   713.0  1376.0   797.0  1329.0   777.0   \n",
       "...    ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "24195  ...  2516.0     0.0  1287.0     0.0  2509.0  2497.0  1294.0   \n",
       "24196  ...  2516.0     0.0  1287.0     0.0  2508.0  2498.0  1294.0   \n",
       "24197  ...  2516.0     0.0  1286.0     0.0  2507.0  2498.0  1292.0   \n",
       "24198  ...  2515.0     0.0  1286.0     0.0  2509.0  2501.0  1293.0   \n",
       "24199  ...  2517.0     0.0  1284.0     0.0  2509.0  2496.0  1290.0   \n",
       "\n",
       "                  label  y                                           FILENAME  \n",
       "0                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "1                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "2                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "3                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "4                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "...                 ... ..                                                ...  \n",
       "24195  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24196  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24197  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24198  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "24199  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "\n",
       "[78424 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "norm_data = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_normal.csv')\n",
    "drunk_data = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_drunken.csv')\n",
    "\n",
    "\n",
    "Combined = pd.concat([norm_data,drunk_data],ignore_index=False)\n",
    "Combined.drop(['Unnamed: 0.1','Unnamed: 0'],axis=1,inplace=True)\n",
    "Combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined.to_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_head_coordinates_for_csv(csv_path, output_path):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 얼굴에 해당하는 키포인트 인덱스 (1, 2, 3, 4, 5)\n",
    "    face_indices = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # x_head와 y_head를 계산하여 새로운 열로 추가\n",
    "    x_columns = [f\"x{i}\" for i in range(1, 18)]  # x1 ~ x17\n",
    "    y_columns = [f\"y{i}\" for i in range(1, 18)]  # y1 ~ y17\n",
    "\n",
    "    def calculate_head(row):\n",
    "        x_values = [row[f\"x{i}\"] for i in face_indices if row[f\"x{i}\"] != 0]\n",
    "        y_values = [row[f\"y{i}\"] for i in face_indices if row[f\"y{i}\"] != 0]\n",
    "        x_head = np.mean(x_values) if x_values else 0\n",
    "        y_head = np.mean(y_values) if y_values else 0\n",
    "        return pd.Series({\"x_head\": x_head, \"y_head\": y_head})\n",
    "\n",
    "    # 데이터프레임에 새로운 열 추가\n",
    "    df[[\"x18\", \"y18\"]] = df.apply(calculate_head, axis=1)\n",
    "    df.drop(['x1','x2','x3','x4','x5','y1','y2','y3','y4','y5'],axis=1,inplace= True)\n",
    "\n",
    "    # 결과 CSV 파일로 저장\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"결과가 {output_path}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 /home/alpaco/project/drunk_prj/data/3_frame_data/aaafinal_combined.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 입력 CSV 경로와 출력 경로 설정\n",
    "input_csv = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv\"\n",
    "output_csv = \"/home/alpaco/project/drunk_prj/data/3_frame_data/aaafinal_combined.csv\"\n",
    "\n",
    "# 얼굴 중심 좌표 계산\n",
    "calculate_head_coordinates_for_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Combined = pd.read_csv(\"/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv\")\n",
    "Combined.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "columns_to_convert = Combined.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)\n",
    "#스케일링 진행 후\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 26개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(Combined, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 150/150 [00:00<00:00, 181.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0383, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 150/150 [00:00<00:00, 331.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.0006, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 150/150 [00:00<00:00, 329.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 0.0002, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 150/150 [00:00<00:00, 320.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 0.0001, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 150/150 [00:00<00:00, 321.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.0001, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 150/150 [00:00<00:00, 292.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 150/150 [00:00<00:00, 298.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 150/150 [00:00<00:00, 330.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 150/150 [00:00<00:00, 305.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 150/150 [00:00<00:00, 257.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 150/150 [00:00<00:00, 317.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 150/150 [00:00<00:00, 327.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 150/150 [00:00<00:00, 319.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 150/150 [00:00<00:00, 338.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 150/150 [00:00<00:00, 314.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 150/150 [00:00<00:00, 305.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 150/150 [00:00<00:00, 322.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 150/150 [00:00<00:00, 316.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 150/150 [00:00<00:00, 310.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 150/150 [00:00<00:00, 297.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 150/150 [00:00<00:00, 331.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 150/150 [00:00<00:00, 269.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 150/150 [00:00<00:00, 333.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 150/150 [00:00<00:00, 252.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 150/150 [00:00<00:00, 235.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 150/150 [00:00<00:00, 257.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 150/150 [00:00<00:00, 307.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 150/150 [00:00<00:00, 302.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 150/150 [00:00<00:00, 313.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 150/150 [00:00<00:00, 291.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 150/150 [00:00<00:00, 269.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 150/150 [00:00<00:00, 258.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 150/150 [00:00<00:00, 308.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 150/150 [00:00<00:00, 323.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 150/150 [00:00<00:00, 299.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 150/150 [00:00<00:00, 285.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 150/150 [00:00<00:00, 285.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 150/150 [00:00<00:00, 270.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 150/150 [00:00<00:00, 249.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 150/150 [00:00<00:00, 283.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 150/150 [00:00<00:00, 276.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 150/150 [00:00<00:00, 251.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 150/150 [00:00<00:00, 274.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 150/150 [00:00<00:00, 295.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 150/150 [00:00<00:00, 231.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 150/150 [00:00<00:00, 266.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 150/150 [00:00<00:00, 300.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 150/150 [00:00<00:00, 268.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 150/150 [00:00<00:00, 259.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 150/150 [00:00<00:00, 283.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Loss: 0.0000, F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 분류용\n",
    "optimizer = optim.NAdam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 및 검증 함수\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = evaluate(model, valid_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1128LSTM05onehead.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_591898/1690274835.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1128_LSTM04_align.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLSTMModel(\n",
       "  (lstm): LSTM(34, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "loaded_model = BinaryLSTMModel(X_seq.shape[2],50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1128_LSTM04_align.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 /home/alpaco/project/drunk_prj/data/3_frame_data/nohead_final_test.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_test.csv')\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "#14014    \n",
    "\n",
    "input_csv = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_test.csv\"\n",
    "output_csv = \"/home/alpaco/project/drunk_prj/data/3_frame_data/nohead_final_test.csv\"\n",
    "\n",
    "# 얼굴 중심 좌표 계산\n",
    "calculate_head_coordinates_for_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>y</th>\n",
       "      <th>label</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 110.0</td>\n",
       "      <td>617-2_cam02_drunken04_place03_night_spring_717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14009</th>\n",
       "      <td>1131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14010</th>\n",
       "      <td>1141</td>\n",
       "      <td>1760</td>\n",
       "      <td>557</td>\n",
       "      <td>1764</td>\n",
       "      <td>554</td>\n",
       "      <td>1757</td>\n",
       "      <td>554</td>\n",
       "      <td>1769</td>\n",
       "      <td>557</td>\n",
       "      <td>1751</td>\n",
       "      <td>...</td>\n",
       "      <td>676</td>\n",
       "      <td>1747</td>\n",
       "      <td>676</td>\n",
       "      <td>1765</td>\n",
       "      <td>705</td>\n",
       "      <td>1749</td>\n",
       "      <td>705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14011</th>\n",
       "      <td>1151</td>\n",
       "      <td>1761</td>\n",
       "      <td>564</td>\n",
       "      <td>1765</td>\n",
       "      <td>561</td>\n",
       "      <td>1758</td>\n",
       "      <td>561</td>\n",
       "      <td>1771</td>\n",
       "      <td>564</td>\n",
       "      <td>1754</td>\n",
       "      <td>...</td>\n",
       "      <td>679</td>\n",
       "      <td>1754</td>\n",
       "      <td>678</td>\n",
       "      <td>1760</td>\n",
       "      <td>711</td>\n",
       "      <td>1757</td>\n",
       "      <td>710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14012</th>\n",
       "      <td>1161</td>\n",
       "      <td>1765</td>\n",
       "      <td>564</td>\n",
       "      <td>1768</td>\n",
       "      <td>561</td>\n",
       "      <td>1761</td>\n",
       "      <td>561</td>\n",
       "      <td>1774</td>\n",
       "      <td>563</td>\n",
       "      <td>1756</td>\n",
       "      <td>...</td>\n",
       "      <td>683</td>\n",
       "      <td>1757</td>\n",
       "      <td>682</td>\n",
       "      <td>1777</td>\n",
       "      <td>716</td>\n",
       "      <td>1761</td>\n",
       "      <td>716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14013</th>\n",
       "      <td>1171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID: 530.0</td>\n",
       "      <td>618-2_cam02_drunken04_place03_night_spring_180...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14014 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame    x1   y1    x2   y2    x3   y3    x4   y4    x5  ...  y14  \\\n",
       "0        640     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "1        650     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "2        660     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "3        670     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "4        680     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "...      ...   ...  ...   ...  ...   ...  ...   ...  ...   ...  ...  ...   \n",
       "14009   1131     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "14010   1141  1760  557  1764  554  1757  554  1769  557  1751  ...  676   \n",
       "14011   1151  1761  564  1765  561  1758  561  1771  564  1754  ...  679   \n",
       "14012   1161  1765  564  1768  561  1761  561  1774  563  1756  ...  683   \n",
       "14013   1171     0    0     0    0     0    0     0    0     0  ...    0   \n",
       "\n",
       "        x15  y15   x16  y16   x17  y17    y      label  \\\n",
       "0         0    0     0    0     0    0  0.0  ID: 110.0   \n",
       "1         0    0     0    0     0    0  0.0  ID: 110.0   \n",
       "2         0    0     0    0     0    0  0.0  ID: 110.0   \n",
       "3         0    0     0    0     0    0  0.0  ID: 110.0   \n",
       "4         0    0     0    0     0    0  0.0  ID: 110.0   \n",
       "...     ...  ...   ...  ...   ...  ...  ...        ...   \n",
       "14009     0    0     0    0     0    0  0.0  ID: 530.0   \n",
       "14010  1747  676  1765  705  1749  705  0.0  ID: 530.0   \n",
       "14011  1754  678  1760  711  1757  710  0.0  ID: 530.0   \n",
       "14012  1757  682  1777  716  1761  716  0.0  ID: 530.0   \n",
       "14013     0    0     0    0     0    0  0.0  ID: 530.0   \n",
       "\n",
       "                                                FILENAME  \n",
       "0      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "1      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "2      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "3      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "4      617-2_cam02_drunken04_place03_night_spring_717...  \n",
       "...                                                  ...  \n",
       "14009  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "14010  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "14011  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "14012  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "14013  618-2_cam02_drunken04_place03_night_spring_180...  \n",
       "\n",
       "[14014 rows x 38 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_test.csv')\n",
    "test_data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 크로마키 영상\n",
    "import os\n",
    "\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = test_data[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_data[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "columns_to_convert = test_data.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "test_data[columns_to_convert] = test_data[columns_to_convert].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "test_x_seq,test_y_seq = create_sequences(test_data,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 90, 34)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_591898/1849309413.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 62.34%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.44\n",
      "Recall: 0.91\n",
      "F1 Score: 0.60\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1128LSTM05onehead.pt')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_x_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "\n",
    "loaded_model = BinaryLSTMModel(X_seq.shape[2],50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/jsw_model/90frame000_LSTM.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.279592</td>\n",
       "      <td>0.729758</td>\n",
       "      <td>2.823165</td>\n",
       "      <td>0.196155</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>3.007860</td>\n",
       "      <td>0.165098</td>\n",
       "      <td>-0.568466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332505</td>\n",
       "      <td>0.968596</td>\n",
       "      <td>0.720503</td>\n",
       "      <td>2.111399</td>\n",
       "      <td>0.385364</td>\n",
       "      <td>0.954696</td>\n",
       "      <td>0.795424</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.639187</td>\n",
       "      <td>-0.709313</td>\n",
       "      <td>-0.673755</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>2.764943</td>\n",
       "      <td>0.114162</td>\n",
       "      <td>-0.568466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184493</td>\n",
       "      <td>0.939757</td>\n",
       "      <td>0.589061</td>\n",
       "      <td>1.686628</td>\n",
       "      <td>0.236471</td>\n",
       "      <td>0.948244</td>\n",
       "      <td>0.753156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.639187</td>\n",
       "      <td>-0.709313</td>\n",
       "      <td>2.382737</td>\n",
       "      <td>0.087040</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>2.545061</td>\n",
       "      <td>0.052388</td>\n",
       "      <td>-0.568466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107269</td>\n",
       "      <td>0.822261</td>\n",
       "      <td>0.454374</td>\n",
       "      <td>1.653000</td>\n",
       "      <td>0.184304</td>\n",
       "      <td>0.868669</td>\n",
       "      <td>0.590584</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.639187</td>\n",
       "      <td>-0.709313</td>\n",
       "      <td>-0.673755</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>2.364967</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-0.568466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086891</td>\n",
       "      <td>0.664175</td>\n",
       "      <td>0.279118</td>\n",
       "      <td>1.653000</td>\n",
       "      <td>0.176696</td>\n",
       "      <td>0.632096</td>\n",
       "      <td>0.361357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.639187</td>\n",
       "      <td>-0.709313</td>\n",
       "      <td>-0.673755</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>2.201626</td>\n",
       "      <td>-0.058155</td>\n",
       "      <td>-0.568466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015030</td>\n",
       "      <td>0.611836</td>\n",
       "      <td>0.254777</td>\n",
       "      <td>1.502560</td>\n",
       "      <td>0.082144</td>\n",
       "      <td>0.598760</td>\n",
       "      <td>0.348352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78419</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2.203074</td>\n",
       "      <td>1.454354</td>\n",
       "      <td>1.273510</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>-0.629613</td>\n",
       "      <td>2.133193</td>\n",
       "      <td>2.538005</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925241</td>\n",
       "      <td>-0.820546</td>\n",
       "      <td>1.186230</td>\n",
       "      <td>-0.932796</td>\n",
       "      <td>1.942762</td>\n",
       "      <td>1.854750</td>\n",
       "      <td>1.188848</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78420</th>\n",
       "      <td>96.0</td>\n",
       "      <td>2.201952</td>\n",
       "      <td>1.450306</td>\n",
       "      <td>1.269432</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>-0.629613</td>\n",
       "      <td>2.138612</td>\n",
       "      <td>2.536776</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925241</td>\n",
       "      <td>-0.820546</td>\n",
       "      <td>1.186230</td>\n",
       "      <td>-0.932796</td>\n",
       "      <td>1.941675</td>\n",
       "      <td>1.855825</td>\n",
       "      <td>1.188848</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78421</th>\n",
       "      <td>97.0</td>\n",
       "      <td>2.197464</td>\n",
       "      <td>1.442210</td>\n",
       "      <td>1.263315</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>-0.629613</td>\n",
       "      <td>2.134277</td>\n",
       "      <td>2.531860</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925241</td>\n",
       "      <td>-0.820546</td>\n",
       "      <td>1.184607</td>\n",
       "      <td>-0.932796</td>\n",
       "      <td>1.940589</td>\n",
       "      <td>1.855825</td>\n",
       "      <td>1.185597</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78422</th>\n",
       "      <td>98.0</td>\n",
       "      <td>2.198586</td>\n",
       "      <td>1.446258</td>\n",
       "      <td>1.259237</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>-0.629613</td>\n",
       "      <td>2.137528</td>\n",
       "      <td>2.533089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.924168</td>\n",
       "      <td>-0.820546</td>\n",
       "      <td>1.184607</td>\n",
       "      <td>-0.932796</td>\n",
       "      <td>1.942762</td>\n",
       "      <td>1.859051</td>\n",
       "      <td>1.187223</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78423</th>\n",
       "      <td>99.0</td>\n",
       "      <td>2.196342</td>\n",
       "      <td>1.444234</td>\n",
       "      <td>1.265354</td>\n",
       "      <td>-0.568739</td>\n",
       "      <td>-0.533452</td>\n",
       "      <td>-0.639122</td>\n",
       "      <td>-0.629613</td>\n",
       "      <td>2.133193</td>\n",
       "      <td>2.530632</td>\n",
       "      <td>...</td>\n",
       "      <td>1.926313</td>\n",
       "      <td>-0.820546</td>\n",
       "      <td>1.181362</td>\n",
       "      <td>-0.932796</td>\n",
       "      <td>1.942762</td>\n",
       "      <td>1.853675</td>\n",
       "      <td>1.182346</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78424 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       frame        x1        y1        x2        y2        x3        y3  \\\n",
       "0        6.0  1.279592  0.729758  2.823165  0.196155 -0.533452 -0.639122   \n",
       "1        7.0 -0.639187 -0.709313 -0.673755 -0.568739 -0.533452 -0.639122   \n",
       "2        8.0 -0.639187 -0.709313  2.382737  0.087040 -0.533452 -0.639122   \n",
       "3        9.0 -0.639187 -0.709313 -0.673755 -0.568739 -0.533452 -0.639122   \n",
       "4       10.0 -0.639187 -0.709313 -0.673755 -0.568739 -0.533452 -0.639122   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "78419   95.0  2.203074  1.454354  1.273510 -0.568739 -0.533452 -0.639122   \n",
       "78420   96.0  2.201952  1.450306  1.269432 -0.568739 -0.533452 -0.639122   \n",
       "78421   97.0  2.197464  1.442210  1.263315 -0.568739 -0.533452 -0.639122   \n",
       "78422   98.0  2.198586  1.446258  1.259237 -0.568739 -0.533452 -0.639122   \n",
       "78423   99.0  2.196342  1.444234  1.265354 -0.568739 -0.533452 -0.639122   \n",
       "\n",
       "             x4        y4        x5  ...       y14       x15       y15  \\\n",
       "0      3.007860  0.165098 -0.568466  ...  0.332505  0.968596  0.720503   \n",
       "1      2.764943  0.114162 -0.568466  ...  0.184493  0.939757  0.589061   \n",
       "2      2.545061  0.052388 -0.568466  ...  0.107269  0.822261  0.454374   \n",
       "3      2.364967 -0.000716 -0.568466  ...  0.086891  0.664175  0.279118   \n",
       "4      2.201626 -0.058155 -0.568466  ...  0.015030  0.611836  0.254777   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "78419 -0.629613  2.133193  2.538005  ...  1.925241 -0.820546  1.186230   \n",
       "78420 -0.629613  2.138612  2.536776  ...  1.925241 -0.820546  1.186230   \n",
       "78421 -0.629613  2.134277  2.531860  ...  1.925241 -0.820546  1.184607   \n",
       "78422 -0.629613  2.137528  2.533089  ...  1.924168 -0.820546  1.184607   \n",
       "78423 -0.629613  2.133193  2.530632  ...  1.926313 -0.820546  1.181362   \n",
       "\n",
       "            x16       y16       x17       y17             label    y  \\\n",
       "0      2.111399  0.385364  0.954696  0.795424               1.0  0.0   \n",
       "1      1.686628  0.236471  0.948244  0.753156               1.0  0.0   \n",
       "2      1.653000  0.184304  0.868669  0.590584               1.0  0.0   \n",
       "3      1.653000  0.176696  0.632096  0.361357               1.0  0.0   \n",
       "4      1.502560  0.082144  0.598760  0.348352               1.0  0.0   \n",
       "...         ...       ...       ...       ...               ...  ...   \n",
       "78419 -0.932796  1.942762  1.854750  1.188848  ID: tensor(602.)  1.0   \n",
       "78420 -0.932796  1.941675  1.855825  1.188848  ID: tensor(602.)  1.0   \n",
       "78421 -0.932796  1.940589  1.855825  1.185597  ID: tensor(602.)  1.0   \n",
       "78422 -0.932796  1.942762  1.859051  1.187223  ID: tensor(602.)  1.0   \n",
       "78423 -0.932796  1.942762  1.853675  1.182346  ID: tensor(602.)  1.0   \n",
       "\n",
       "                                                FILENAME  \n",
       "0          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "1          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "2          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "3          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "4          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "...                                                  ...  \n",
       "78419  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78420  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78421  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78422  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78423  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "\n",
       "[78424 rows x 38 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Combined = pd.read_csv(\"/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv\")\n",
    "Combined.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "columns_to_convert = Combined.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)\n",
    "#스케일링 진행 후\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 26개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized\n",
    "\n",
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(Combined, sequence_length)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "\n",
    "# 텐서 타입 확인\n",
    "train_X_tensor = train_X_tensor.float()\n",
    "valid_X_tensor = valid_X_tensor.float()\n",
    "\n",
    "train_y_tensor = train_y_tensor.float()\n",
    "valid_y_tensor = valid_y_tensor.float()\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308, 90, 34)\n"
     ]
    }
   ],
   "source": [
    "print(X_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BertForKeypointClassification(nn.Module):\n",
    "    def __init__(self, input_size, sequence_length, hidden_size=256, num_classes=1):\n",
    "        super(BertForKeypointClassification, self).__init__()\n",
    "        # BERT 설정\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=8,\n",
    "            num_hidden_layers=4,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            max_position_embeddings=sequence_length,\n",
    "            vocab_size=1  # 가상의 토큰 ID\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # 입력 차원 조정\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # 분류기\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()  # 이진 분류용\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, hidden_size)\n",
    "        bert_output = self.bert(inputs_embeds=x)  # BERT의 출력\n",
    "        pooled_output = bert_output.pooler_output  # [CLS] 토큰의 출력\n",
    "        return self.classifier(pooled_output)  # 분류 결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 모델 예측 및 손실 계산\n",
    "            outputs = model(sequences).squeeze()  # (batch_size,)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 손실 및 정확도 업데이트\n",
    "            epoch_loss += loss.item()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 165/165 [00:02<00:00, 58.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.5155, Accuracy: 0.9939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 165/165 [00:02<00:00, 62.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 0.0081, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 165/165 [00:02<00:00, 78.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Loss: 0.0031, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 165/165 [00:02<00:00, 70.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Loss: 0.0017, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 165/165 [00:02<00:00, 70.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 0.0012, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 165/165 [00:02<00:00, 71.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Loss: 0.0008, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 165/165 [00:02<00:00, 73.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Loss: 0.0006, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 165/165 [00:02<00:00, 74.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Loss: 0.0004, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 165/165 [00:02<00:00, 71.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Loss: 0.0003, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 165/165 [00:02<00:00, 66.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.0003, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 165/165 [00:02<00:00, 64.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Loss: 0.0002, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 165/165 [00:02<00:00, 74.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Loss: 0.0002, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 165/165 [00:02<00:00, 72.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 165/165 [00:02<00:00, 74.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 165/165 [00:02<00:00, 74.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 165/165 [00:02<00:00, 74.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 165/165 [00:02<00:00, 69.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 165/165 [00:02<00:00, 70.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 165/165 [00:02<00:00, 68.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 165/165 [00:02<00:00, 58.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 165/165 [00:02<00:00, 60.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 165/165 [00:02<00:00, 70.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 165/165 [00:02<00:00, 69.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 165/165 [00:02<00:00, 63.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 165/165 [00:02<00:00, 76.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 165/165 [00:02<00:00, 73.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 165/165 [00:02<00:00, 78.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 165/165 [00:02<00:00, 74.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 165/165 [00:02<00:00, 73.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 165/165 [00:02<00:00, 75.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 165/165 [00:02<00:00, 73.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 165/165 [00:02<00:00, 78.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 165/165 [00:02<00:00, 60.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 165/165 [00:02<00:00, 62.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 165/165 [00:02<00:00, 72.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 165/165 [00:02<00:00, 78.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 165/165 [00:02<00:00, 76.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 165/165 [00:02<00:00, 72.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 165/165 [00:02<00:00, 73.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 165/165 [00:02<00:00, 77.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 165/165 [00:02<00:00, 79.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 165/165 [00:02<00:00, 75.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 165/165 [00:02<00:00, 71.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 165/165 [00:02<00:00, 76.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 165/165 [00:02<00:00, 74.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 165/165 [00:02<00:00, 74.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 165/165 [00:02<00:00, 72.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 165/165 [00:02<00:00, 70.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 165/165 [00:02<00:00, 75.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 165/165 [00:02<00:00, 75.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: 0.0000, Accuracy: 1.0000\n",
      "Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "input_size = 34  # x1~x17, y1~y17\n",
    "sequence_length = 90\n",
    "hidden_size = 256\n",
    "num_classes = 1  # 이진 분류\n",
    "model = BertForKeypointClassification(input_size, sequence_length, hidden_size, num_classes)\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 50\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1203_BertModel.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 75.32%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.57\n",
      "Recall: 0.83\n",
      "F1 Score: 0.68\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_test.csv')\n",
    "test_data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "test_data\n",
    "\n",
    "# 1. 크로마키 영상\n",
    "import os\n",
    "\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = test_data[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_data[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "columns_to_convert = test_data.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "test_data[columns_to_convert] = test_data[columns_to_convert].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "test_x_seq,test_y_seq = create_sequences(test_data,sequence_length)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_x_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "# 텐서 타입 확인\n",
    "train_X_tensor = train_X_tensor.float()\n",
    "valid_X_tensor = valid_X_tensor.float()\n",
    "\n",
    "train_y_tensor = train_y_tensor.float()\n",
    "valid_y_tensor = valid_y_tensor.float()\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "# 모델 초기화\n",
    "input_size = 34  # x1~x17, y1~y17\n",
    "sequence_length = 90\n",
    "hidden_size = 256\n",
    "num_classes = 1  # 이진 분류\n",
    "loaded_model = BertForKeypointClassification(input_size, sequence_length, hidden_size, num_classes)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1203_BertModel.pt'))\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0028693191707134247\n",
      "Epoch 2, Loss: 0.0006203787634149194\n",
      "Epoch 3, Loss: 0.00019176640489604324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# 역전파\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 71\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/optim/nadam.py:183\u001b[0m, in \u001b[0;36mNAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    173\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         state_steps,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m     \u001b[43mnadam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmu_products\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmomentum_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/optim/nadam.py:621\u001b[0m, in \u001b[0;36mnadam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps, decoupled_weight_decay, foreach, capturable, differentiable, has_complex, maximize, beta1, beta2, lr, weight_decay, momentum_decay, eps)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_nadam\n\u001b[0;32m--> 621\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmu_products\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/optim/nadam.py:345\u001b[0m, in \u001b[0;36m_single_tensor_nadam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, mu_products, state_steps, beta1, beta2, lr, weight_decay, momentum_decay, eps, decoupled_weight_decay, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    343\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    344\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 345\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mor\u001b[39;00m capturable:\n\u001b[1;32m    348\u001b[0m     denom \u001b[38;5;241m=\u001b[39m denom\u001b[38;5;241m.\u001b[39madd(eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaConfig, RobertaModel\n",
    "\n",
    "\n",
    "# Transformer 모델을 위한 설정\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention 레이어\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 시퀀스 길이, 배치 크기, 특성 차원에 맞게 변환\n",
    "        x = x.transpose(0, 1)  # Transformer는 (seq_len, batch_size, features)의 형태를 기대함\n",
    "        \n",
    "        # Attention 통과\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(attn_output)\n",
    "        \n",
    "        # 마지막 시퀀스 출력을 사용 (기본적으로 클래스 레이블 예측)\n",
    "        output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Fully connected layers 통과\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = 34 # 입력 특징의 크기\n",
    "hidden_size = 50\n",
    "num_classes = 1  # 이진 분류\n",
    "model = TransformerModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 루프\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(output.squeeze(), batch_y.float())\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1128LSTM05onehead.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 80.52%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.65\n",
      "Recall: 0.78\n",
      "F1 Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 열 값의 개수:\n",
      "0    15907\n",
      "1     8593\n",
      "Name: y, dtype: int64\n",
      "0의 개수: 15907\n",
      "1의 개수: 8593\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test= pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_3frame_test.csv')\n",
    "\n",
    "test.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "# y 열의 값이 0과 1로만 이루어졌다고 가정\n",
    "if 'y' in test.columns:\n",
    "    value_counts = test['y'].value_counts()\n",
    "    print(\"y 열 값의 개수:\")\n",
    "    print(value_counts)\n",
    "\n",
    "    # 개별 값 출력\n",
    "    count_0 = value_counts.get(0, 0)  # 0의 개수\n",
    "    count_1 = value_counts.get(1, 0)  # 1의 개수\n",
    "    print(f\"0의 개수: {count_0}\")\n",
    "    print(f\"1의 개수: {count_1}\")\n",
    "else:\n",
    "    print(\"DataFrame에 'y' 열이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_3frame_test.csv')\n",
    "\n",
    "test.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "test\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = test[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "columns_to_convert = test.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "test[columns_to_convert] = test[columns_to_convert].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(test, sequence_length)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(X_seq)\n",
    "test_y_tensor = torch.LongTensor(Y_seq)\n",
    "\n",
    "\n",
    "# 텐서 타입 확인\n",
    "test_X_tensor = test_X_tensor.float()\n",
    "test_y_tensor = test_y_tensor.float()\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_639417/874574802.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1203_BertModel.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 54.62%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.42\n",
      "Recall: 0.78\n",
      "F1 Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class BertForKeypointClassification(nn.Module):\n",
    "    def __init__(self, input_size, sequence_length, hidden_size=256, num_classes=1):\n",
    "        super(BertForKeypointClassification, self).__init__()\n",
    "        # BERT 설정\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=8,\n",
    "            num_hidden_layers=4,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            max_position_embeddings=sequence_length,\n",
    "            vocab_size=1  # 가상의 토큰 ID\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # 입력 차원 조정\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # 분류기\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()  # 이진 분류용\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, hidden_size)\n",
    "        bert_output = self.bert(inputs_embeds=x)  # BERT의 출력\n",
    "        pooled_output = bert_output.pooler_output  # [CLS] 토큰의 출력\n",
    "        return self.classifier(pooled_output)  # 분류 결과\n",
    "\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "# 모델 초기화\n",
    "input_size = 34  # x1~x17, y1~y17\n",
    "sequence_length = 90\n",
    "hidden_size = 256\n",
    "num_classes = 1  # 이진 분류\n",
    "loaded_model = BertForKeypointClassification(input_size, sequence_length, hidden_size, num_classes)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1203_BertModel.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_639417/39978865.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1203_Transformer.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=34, out_features=34, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=34, out_features=34, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=34, out_features=50, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=50, out_features=34, bias=True)\n",
       "        (norm1): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=34, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaConfig, RobertaModel\n",
    "\n",
    "\n",
    "# Transformer 모델을 위한 설정\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention 레이어\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 시퀀스 길이, 배치 크기, 특성 차원에 맞게 변환\n",
    "        x = x.transpose(0, 1)  # Transformer는 (seq_len, batch_size, features)의 형태를 기대함\n",
    "        \n",
    "        # Attention 통과\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(attn_output)\n",
    "        \n",
    "        # 마지막 시퀀스 출력을 사용 (기본적으로 클래스 레이블 예측)\n",
    "        output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Fully connected layers 통과\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = 34  # 입력 특징의 크기\n",
    "hidden_size = 50\n",
    "num_classes = 1  # 이진 분류\n",
    "loaded_model = TransformerModel(input_size, hidden_size, num_classes)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1203_Transformer.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 78.59%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.66\n",
      "Recall: 0.81\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_639417/2219515276.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1128_LSTM02.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLSTMModel(\n",
       "  (lstm): LSTM(34, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BinaryLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 이진 분류이므로 출력 노드를 1개로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = X_seq.shape[2]\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "loaded_model = BinaryLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1128_LSTM02.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 72.36%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGwCAYAAACKOz5MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEVElEQVR4nO3deXgUVdr38V8lkIWsgCRNIEKQsIMgKEZwYcywyvKC4+BkEJXlURMVEBAe2QVRcJswCC6jiIOio8KDoIwMKKhEZAtGCJE9bAlKSEIC2ev9g6G1JbSEdOii8/141XXRdU5Vn+rpSe7c9zlVhmmapgAAACzMy90DAAAA+D0ELAAAwPIIWAAAgOURsAAAAMsjYAEAAJZHwAIAACyPgAUAAFheDXcPwNOVlZXp2LFjCgoKkmEY7h4OAKCCTNPU6dOnFRERIS+vqvk7v6CgQEVFRS45l4+Pj/z8/FxyLishYKlix44dU2RkpLuHAQCopMOHD6thw4YuP29BQYH8g+pKJWdccj6bzaYDBw54XNBCwFLFgoKCJEk+rYbK8PZx82iAqnHPmAfdPQSgyhSdzdf7j8Taf567/PxFRVLJGfm2GipV9vdEaZEydr2toqIiAhZUzPkykOHtQ8ACj+VTK9DdQwCqXJWX9Wv4Vfr3hGl47tRUAhYAAKzAkFTZoMiDp0oSsAAAYAWG17mtsufwUJ57ZQAAwGOQYQEAwAoMwwUlIc+tCRGwAABgBZSEnPLcKwMAAB6DDAsAAFZAScgpAhYAACzBBSUhDy6ceO6VAQAAj0GGBQAAK6Ak5BQBCwAAVsAqIac898oAAIDHIMMCAIAVUBJyioAFAAAroCTkFAELAABWQIbFKc8NxQAAgMcgwwIAgBVQEnKKgAUAACswDBcELJSEAAAA3IYMCwAAVuBlnNsqew4PRcACAIAVMIfFKc+9MgAA4DHIsAAAYAXch8UpAhYAAKyAkpBTnntlAADAY5BhAQDACigJOUXAAgCAFVAScoqABQAAKyDD4pTnhmIAAMBjkGEBAMAKKAk5RcACAIAVUBJyynNDMQAA4DHIsAAAYAkuKAl5cB6CgAUAACugJOSU54ZiAADAY5BhAQDACgzDBauEPDfDQsACAIAVsKzZKc+9MgAA4NSGDRvUt29fRUREyDAMLV++3KHdNE1NmTJF9evXl7+/v2JjY7Vnzx6HPllZWYqLi1NwcLBCQ0M1bNgw5eXlOfT5/vvvdeutt8rPz0+RkZGaM2dOhcdKwAIAgBWcn3Rb2a0C8vPzdf3112v+/Pnlts+ZM0eJiYlauHChNm3apICAAPXo0UMFBQX2PnFxcdq5c6fWrFmjlStXasOGDRo5cqS9PTc3V927d1ejRo20detWzZ07V9OmTdNrr71WobFSEgIAwArcUBLq1auXevXqVW6baZp6+eWXNWnSJPXv31+StHjxYoWHh2v58uUaPHiwUlNTtXr1am3evFmdOnWSJM2bN0+9e/fW888/r4iICC1ZskRFRUV688035ePjo9atWys5OVkvvviiQ2Dze8iwAABgBS7MsOTm5jpshYWFFR7OgQMHlJGRodjYWPu+kJAQde7cWUlJSZKkpKQkhYaG2oMVSYqNjZWXl5c2bdpk73PbbbfJx8fH3qdHjx5KS0vTqVOnLnk8BCwAAHiYyMhIhYSE2LfZs2dX+BwZGRmSpPDwcIf94eHh9raMjAyFhYU5tNeoUUN16tRx6FPeOX79HpeCkhAAAFbgwpLQ4cOHFRwcbN/t6+tbufNaABkWAACswIUloeDgYIftcgIWm80mScrMzHTYn5mZaW+z2Ww6ceKEQ3tJSYmysrIc+pR3jl+/x6UgYAEAABeIioqSzWbT2rVr7ftyc3O1adMmxcTESJJiYmKUnZ2trVu32vusW7dOZWVl6ty5s73Phg0bVFxcbO+zZs0aNW/eXLVr177k8RCwAABgAYZhuGSriLy8PCUnJys5OVnSuYm2ycnJSk9Pl2EYGjVqlGbOnKkVK1YoJSVF9913nyIiIjRgwABJUsuWLdWzZ0+NGDFC3333nb755hslJCRo8ODBioiIkCT95S9/kY+Pj4YNG6adO3fq/fff19/+9jeNGTOmQmNlDgsAABZwOQFHOSepUPctW7aoW7du9tfng4ihQ4dq0aJFGj9+vPLz8zVy5EhlZ2era9euWr16tfz8/OzHLFmyRAkJCbrzzjvl5eWlQYMGKTEx0d4eEhKizz//XPHx8erYsaOuueYaTZkypUJLmiXJME3TrNARqJDc3FyFhITIt+0IGd4+v38AcBWKm/iQu4cAVJmiM3l654EY5eTkOExkdZXzvyf8+82XUdO/Uucyi8/q7Ir4KhurO5FhAQDACoz/bpU9h4ciYAEAwALcURK6mjDpFgAAWB4ZFgAALIAMi3MELAAAWAABi3MELAAAWAABi3PMYQEAAJZHhgUAACtgWbNTBCwAAFgAJSHnKAkBAADLI8MCAIAFGIZckGFxzVisiIAFAAALMOSCkpAHRyyUhAAAgOWRYQEAwAKYdOscAQsAAFbAsmanKAkBAADLI8MCAIAVuKAkZFISAgAAVckVc1gqv8rIughYAACwAAIW55jDAgAALI8MCwAAVsAqIacIWAAAsABKQs5REgIAAJZHhgUAAAsgw+IcAQsAABZAwOIcJSEAAGB5ZFgAALAAMizOEbAAAGAFLGt2ipIQAACwPDIsAABYACUh5whYAACwAAIW5whYAACwAAIW55jDAgAALI8MCwAAVsAqIacIWAAAsABKQs5REgIAAJZHhgWWc0uH6/TokFhd3+Ja1a8Xorixr+nT9d/b2+/qdr0eGNhV7VtcqzqhAbo1brZ++PGovT00uJYmjuyjbje3UMPw2jqZnadVX36vZxauVG5+gb1fw/DaemHCn9W1UzPlnynU0lWbNH3+CpWWll3R6wUkKcSvhvq3sal1eKBq1vDSz3lF+ufWI0rPPved7d0yTDc0DFFt/5oqLTOVnn1Wn+zM1KFTZx3O09oWqF4twhQR4qeSUlN7fs7X69+mu+OSUEFkWJy7KgIWwzC0bNkyDRgwwN1DwRVQy99XP/x4VP9ckaR/zh15QXuAn4++3bFPy/+zTYmT4i5or18vRLZ6IZryt2XavT9DkfXr6MUJg2WrF6L7J/xDkuTlZej9lx9W5slc9Rj2gmzXhGjBtCEqLinV0698UuXXCPyaf00vjbm9ifb8nK9XNh5SXmGJ6gX66kzxL8HzidOF+lfyMf2cX6Sa3l76Q3RdJXRtrOn//lF5RaWSpPYRwbr3hgh9sjNTP/6ULy/DUP1gX3ddFirIkAsCFg+exOL2klBGRoYeffRRNWnSRL6+voqMjFTfvn21du1adw9NkmSapqZMmaL69evL399fsbGx2rNnj7uH5dH+s3GXZi1cqVVffl9u+/ufbdbcN1bry+/Sym1P3XdcQ598Q6u/+kEHj/6sr7b8qJkLPlHPW9vI2/vcV/4PN7dU8yib/mfK2/rhx6P6z8ZdembhKg3/022qWcO7yq4NKM8fm9XTqbPF+ufWozp06qxOninW7hN5+jm/yN5ny5Ecpf2Ur5NnipVxulAff58h/5reigjxkyR5GdKg6+treUqmvj5wSifyipRxulDbj+a667IAl3JrwHLw4EF17NhR69at09y5c5WSkqLVq1erW7duio+Pd+fQ7ObMmaPExEQtXLhQmzZtUkBAgHr06KGCgoLfPxiWERzop9P5BfZyz41to7Rr3zH9lHXa3mftt6kKDvRXiyb13TVMVFNt6wcp/dRZPXhTpGb3bqEn/3Cdbmlc+6L9vQ1DXaJq60xRqY7mnPtZFBnqr9r+NWXK1JN/uE6zejfXw7c0IsNyFTlfEqrs5qncGrA88sgjMgxD3333nQYNGqRmzZqpdevWGjNmjL799tuLHvfkk0+qWbNmqlWrlpo0aaLJkyeruLjY3r5jxw5169ZNQUFBCg4OVseOHbVlyxZJ0qFDh9S3b1/Vrl1bAQEBat26tT799NNy38c0Tb388suaNGmS+vfvr3bt2mnx4sU6duyYli9f7tLPAlWnTkiAxg3rpbeXbbTvC6sbrBMnTzv0++nkub9Ew68JvqLjA64J8NGtTerop/wizf/moL7en6W7r6+vzteGOvRrYwvSC/1a6qUBrdSt6TX6+zcHlf/fctA1AT6Szs11+ffun7Rw4yGdLS7V47dGqVZNsoZXBcNFm4dy2xyWrKwsrV69WrNmzVJAQMAF7aGhoRc9NigoSIsWLVJERIRSUlI0YsQIBQUFafz48ZKkuLg4dejQQQsWLJC3t7eSk5NVs2ZNSVJ8fLyKioq0YcMGBQQEaNeuXQoMDCz3fQ4cOKCMjAzFxsba94WEhKhz585KSkrS4MGDLzimsLBQhYWF9te5uaRj3SkowE/vv/yw0g4c17OvrXL3cIByGYaUfqpAn+zMlCQdySlQ/WA/dY2qo03p2fZ+P/6Up9lr9ynQx1u3RNXRgzdF6vkv9ymvsNT+e+rfu39S8rFzP3f+ufWonu7VXB0aBuubA6eu8FUBruW2gGXv3r0yTVMtWrSo8LGTJk2y/7tx48YaO3asli5dag9Y0tPTNW7cOPu5o6Oj7f3T09M1aNAgtW3bVpLUpEmTi75PRkaGJCk8PNxhf3h4uL3tt2bPnq3p06dX+JrgeoG1fPVh4iPKO1Ogv457XSW/Wv1z4mSuOrZu5NC/Xt1zmZXMnwkycWXlFpQo47RjmTnjdKHaN3DM9hWVmvo5v0g/50sHTx3VlO7RuqVRbX3+48/KKSiRJB0//csfTCVlpk7mF6mOf82qvwhUGquEnHNbScg0zcs+9v3331eXLl1ks9kUGBioSZMmKT39l2V7Y8aM0fDhwxUbG6tnn31W+/bts7c99thjmjlzprp06aKpU6fq++/Ln9h5uSZOnKicnBz7dvjwYZeeH5cmKMBPH81LUFFxqf4y5lUVFpU4tG9OOaBW10Xomtq/ZNe6dW6h3LyzSjtQfjAKVJX9J88oLNBxrklYoI+yzhRf5IhzDBmq8d+J5Iezz6q4tEzhvzqPlyHVqfX754E1MIfFObcFLNHR0TIMQ7t3767QcUlJSYqLi1Pv3r21cuVKbd++XU899ZSKin6ZTT9t2jTt3LlTffr00bp169SqVSstW7ZMkjR8+HDt379fQ4YMUUpKijp16qR58+aV+142m02SlJmZ6bA/MzPT3vZbvr6+Cg4OdthQMQH+PmrTrIHaNGsgSWoUUVdtmjVQw/BzkxBDg2upTbMGahF17n+D6EbhatOsgcLqBkk6H6zEK8DfR48+vURBgX4KqxuksLpB8vI693/mdd+mKu1AhhZOH6o20Q30h5tb6qmH7tIb/9qgouKSckYFVJ11e08qqk4tdW9eT9cE+KhTwxB1iaqjDftPSpJ8vA31bR2uxrXPTayNDPVT3A0NFOpfQ9uO5EiSCkrK9PWBLPVuFaYWYYEKC/TR4A4RkqRtR3Pcdm24dIbhms1TGWZlUh2V1KtXL6WkpCgtLe2CeSzZ2dn2eSy/vg/LCy+8oFdeecUhazJ8+HB9+OGHys7OLvd97r33XuXn52vFihUXtE2cOFGrVq0qN9NimqYiIiI0duxYPfHEE5LOzUkJCwvTokWLyp3D8lu5ubkKCQmRb9sRMrx9frc/pC43RGvlq49fsP/dld8qfvo/de9dnfXK1CEXtD/72qd67vVPL3q8JLXrN0WHj2dJkiJttfXChMHq0jFaZ84W6r1V32n63/+PG8ddhriJD7l7CFe9NrYg9WsdrnqBPjqZX6R1e09q48Fz805qeBm6/8aGalynlgJ8vHWmqFSHTp3V6rSflP6rG8d5GVL/1jbdeG2oanobOpR1Vh9+f1wZvyoToeKKzuTpnQdilJOTUyV/hJ7/PRGV8KG8fGtV6lxlhWd04O93V9lY3cmtN46bP3++unTpoptuukkzZsxQu3btVFJSojVr1mjBggVKTU294Jjo6Gilp6dr6dKluvHGG7Vq1Sp79kSSzp49q3Hjxunuu+9WVFSUjhw5os2bN2vQoEGSpFGjRqlXr15q1qyZTp06pS+++EItW7Ysd3yGYWjUqFGaOXOmoqOjFRUVpcmTJysiIoKb2FWhb7btUe0bEy7a/t7KTXpv5abLPv68wxmndM+oBZc1RsDVfsg4rR8yTpfbVlJm6o1Nv19eLjOlZT9kaNkPlDWvRucyJJWdw+KiwViQWwOWJk2aaNu2bZo1a5aeeOIJHT9+XPXq1VPHjh21YEH5v0j69eun0aNHKyEhQYWFherTp48mT56sadOmSZK8vb118uRJ3XfffcrMzNQ111yjgQMH2ifClpaWKj4+XkeOHFFwcLB69uypl1566aJjHD9+vPLz8zVy5EhlZ2era9euWr16tfz8/Fz+eQAAqjFXlHQ8OGBxa0moOqAkhOqAkhA82ZUqCTV57EN5+154m4+KKC3M1/5ESkIAAKCKsKzZOQIWAAAswBWrfDw4XnH/ww8BAAB+DxkWAAAswMvLsN8r6nKZlTzeyghYAACwAEpCzlESAgAAlkeGBQAAC2CVkHMELAAAWAAlIecIWAAAsAAyLM4xhwUAAFgeGRYAACyADItzBCwAAFgAc1icoyQEAAAsj4AFAAALMGTYy0KXvaliKZbS0lJNnjxZUVFR8vf313XXXaenn35apmna+5imqSlTpqh+/fry9/dXbGys9uzZ43CerKwsxcXFKTg4WKGhoRo2bJjy8vJc8rmcR8ACAIAFnC8JVXariOeee04LFizQ3//+d6Wmpuq5557TnDlzNG/ePHufOXPmKDExUQsXLtSmTZsUEBCgHj16qKCgwN4nLi5OO3fu1Jo1a7Ry5Upt2LBBI0eOdNVHI4k5LAAAVFsbN25U//791adPH0lS48aN9d577+m7776TdC678vLLL2vSpEnq37+/JGnx4sUKDw/X8uXLNXjwYKWmpmr16tXavHmzOnXqJEmaN2+eevfureeff14REREuGSsZFgAALKDS5aBfrTLKzc112AoLC8t9z1tuuUVr167Vjz/+KEnasWOHvv76a/Xq1UuSdODAAWVkZCg2NtZ+TEhIiDp37qykpCRJUlJSkkJDQ+3BiiTFxsbKy8tLmzZtctnnQ4YFAAALcOUqocjISIf9U6dO1bRp0y7oP2HCBOXm5qpFixby9vZWaWmpZs2apbi4OElSRkaGJCk8PNzhuPDwcHtbRkaGwsLCHNpr1KihOnXq2Pu4AgELAAAe5vDhwwoODra/9vX1LbffBx98oCVLlujdd99V69atlZycrFGjRikiIkJDhw69UsO9JAQsAABYgCtvHBccHOwQsFzMuHHjNGHCBA0ePFiS1LZtWx06dEizZ8/W0KFDZbPZJEmZmZmqX7++/bjMzEy1b99ekmSz2XTixAmH85aUlCgrK8t+vCswhwUAAAtwxyqhM2fOyMvLMRTw9vZWWVmZJCkqKko2m01r1661t+fm5mrTpk2KiYmRJMXExCg7O1tbt26191m3bp3KysrUuXPny/w0LkSGBQAAC3DHrfn79u2rWbNm6dprr1Xr1q21fft2vfjii3rwwQft5xs1apRmzpyp6OhoRUVFafLkyYqIiNCAAQMkSS1btlTPnj01YsQILVy4UMXFxUpISNDgwYNdtkJIImABAKDamjdvniZPnqxHHnlEJ06cUEREhP7nf/5HU6ZMsfcZP3688vPzNXLkSGVnZ6tr165avXq1/Pz87H2WLFmihIQE3XnnnfLy8tKgQYOUmJjo0rEa5q9vZweXy83NVUhIiHzbjpDh7ePu4QBVIm7iQ+4eAlBlis7k6Z0HYpSTk3NJ80Iq6vzviY5TV6mGX0ClzlVSkK+t0/tU2VjdiQwLAAAWwNOanWPSLQAAsDwyLAAAWIArbxzniQhYAACwAEpCzlESAgAAlkeGBQAAC6Ak5BwBCwAAFkBJyDlKQgAAwPLIsAAAYAFkWJwjYAEAwAKYw+IcAQsAABZAhsU55rAAAADLI8MCAIAFUBJyjoAFAAALoCTkHCUhAABgeWRYAACwAEMuKAm5ZCTWRMACAIAFeBmGvCoZsVT2eCujJAQAACyPDAsAABbAKiHnCFgAALAAVgk5R8ACAIAFeBnntsqew1MxhwUAAFgeGRYAAKzAcEFJx4MzLAQsAABYAJNunaMkBAAALI8MCwAAFmD897/KnsNTEbAAAGABrBJyjpIQAACwPDIsAABYADeOc46ABQAAC2CVkHOXFLCsWLHikk/Yr1+/yx4MAABAeS4pYBkwYMAlncwwDJWWllZmPAAAVEtehiGvSqZIKnu8lV1SwFJWVlbV4wAAoFqjJORcpeawFBQUyM/Pz1VjAQCg2mLSrXMVXtZcWlqqp59+Wg0aNFBgYKD2798vSZo8ebL+8Y9/uHyAAAAAFQ5YZs2apUWLFmnOnDny8fGx72/Tpo3eeOMNlw4OAIDq4nxJqLKbp6pwwLJ48WK99tpriouLk7e3t33/9ddfr927d7t0cAAAVBfnJ91WdvNUFQ5Yjh49qqZNm16wv6ysTMXFxS4ZFAAAwK9VOGBp1aqVvvrqqwv2f/jhh+rQoYNLBgUAQHVjuGjzVBVeJTRlyhQNHTpUR48eVVlZmT7++GOlpaVp8eLFWrlyZVWMEQAAj8cqIecqnGHp37+/PvnkE/3nP/9RQECApkyZotTUVH3yySf64x//WBVjBAAA1dxl3Yfl1ltv1Zo1a1w9FgAAqi0v49xW2XN4qsu+cdyWLVuUmpoq6dy8lo4dO7psUAAAVDeUhJyrcMBy5MgR3Xvvvfrmm28UGhoqScrOztYtt9yipUuXqmHDhq4eIwAAqOYqPIdl+PDhKi4uVmpqqrKyspSVlaXU1FSVlZVp+PDhVTFGAACqBW4ad3EVzrCsX79eGzduVPPmze37mjdvrnnz5unWW2916eAAAKguKAk5V+GAJTIystwbxJWWlioiIsIlgwIAoLph0q1zFS4JzZ07V48++qi2bNli37dlyxY9/vjjev755106OAAAAOkSMyy1a9d2SDPl5+erc+fOqlHj3OElJSWqUaOGHnzwQQ0YMKBKBgoAgCejJOTcJQUsL7/8chUPAwCA6s0Vt9b33HDlEgOWoUOHVvU4AAAALuqybxwnSQUFBSoqKnLYFxwcXKkBAQBQHXkZhrwqWdKp7PFWVuFJt/n5+UpISFBYWJgCAgJUu3Zthw0AAFRcZe/B4un3YqlwwDJ+/HitW7dOCxYskK+vr9544w1Nnz5dERERWrx4cVWMEQAAVHMVLgl98sknWrx4se644w498MADuvXWW9W0aVM1atRIS5YsUVxcXFWMEwAAj8YqIecqnGHJyspSkyZNJJ2br5KVlSVJ6tq1qzZs2ODa0QEAUE1QEnKuwgFLkyZNdODAAUlSixYt9MEHH0g6l3k5/zBEAAAAV6pwwPLAAw9ox44dkqQJEyZo/vz58vPz0+jRozVu3DiXDxAAgOrg/Cqhym6eqsJzWEaPHm3/d2xsrHbv3q2tW7eqadOmateunUsHBwBAdeGKko4HxyuVuw+LJDVq1EiNGjVyxVgAAKi2mHTr3CUFLImJiZd8wscee+yyBwMAAFCeSwpYXnrppUs6mWEYBCwXkf7l89wFGB6r49TP3T0EoMqUFuZfkffx0mVMLC3nHJ7qkgKW86uCAABA1aAk5JwnB2MAAOB3HD16VH/9619Vt25d+fv7q23bttqyZYu93TRNTZkyRfXr15e/v79iY2O1Z88eh3NkZWUpLi5OwcHBCg0N1bBhw5SXl+fScRKwAABgAYYheVVyq2iC5dSpU+rSpYtq1qypzz77TLt27dILL7zg8GzAOXPmKDExUQsXLtSmTZsUEBCgHj16qKCgwN4nLi5OO3fu1Jo1a7Ry5Upt2LBBI0eOdNVHI8kFq4QAAEDlnQ86KnsOScrNzXXY7+vrK19f3wv6P/fcc4qMjNRbb71l3xcVFWX/t2maevnllzVp0iT1799fkrR48WKFh4dr+fLlGjx4sFJTU7V69Wpt3rxZnTp1kiTNmzdPvXv31vPPP6+IiIjKXdT5a3PJWQAAgGVERkYqJCTEvs2ePbvcfitWrFCnTp30pz/9SWFhYerQoYNef/11e/uBAweUkZGh2NhY+76QkBB17txZSUlJkqSkpCSFhobagxXp3H3avLy8tGnTJpddExkWAAAswJWTbg8fPuywMrW87Iok7d+/XwsWLNCYMWP0v//7v9q8ebMee+wx+fj4aOjQocrIyJAkhYeHOxwXHh5ub8vIyFBYWJhDe40aNVSnTh17H1e4rIDlq6++0quvvqp9+/bpww8/VIMGDfTOO+8oKipKXbt2ddngAACoLlxZEgoODr6kW2mUlZWpU6dOeuaZZyRJHTp00A8//KCFCxdq6NChlRuMi1W4JPTRRx+pR48e8vf31/bt21VYWChJysnJsV8wAACwvvr166tVq1YO+1q2bKn09HRJks1mkyRlZmY69MnMzLS32Ww2nThxwqG9pKREWVlZ9j6uUOGAZebMmVq4cKFef/111axZ076/S5cu2rZtm8sGBgBAdXL+WUKV3SqiS5cuSktLc9j3448/2h+5ExUVJZvNprVr19rbc3NztWnTJsXExEiSYmJilJ2dra1bt9r7rFu3TmVlZercufNlfhoXqnBJKC0tTbfddtsF+0NCQpSdne2KMQEAUO244mnLFT1+9OjRuuWWW/TMM8/onnvu0XfffafXXntNr732mqRzc2JGjRqlmTNnKjo6WlFRUZo8ebIiIiI0YMAASecyMj179tSIESO0cOFCFRcXKyEhQYMHD3bZCiHpMgIWm82mvXv3qnHjxg77v/76azVp0sRV4wIAoFpxx635b7zxRi1btkwTJ07UjBkzFBUVpZdffllxcXH2PuPHj1d+fr5Gjhyp7Oxsde3aVatXr5afn5+9z5IlS5SQkKA777xTXl5eGjRoUIWeQ3gpKhywjBgxQo8//rjefPNNGYahY8eOKSkpSWPHjtXkyZNdOjgAAFC17rrrLt11110XbTcMQzNmzNCMGTMu2qdOnTp69913q2J4dhUOWCZMmKCysjLdeeedOnPmjG677Tb5+vpq7NixevTRR6tijAAAeLzLmYNS3jk8VYUDFsMw9NRTT2ncuHHau3ev8vLy1KpVKwUGBlbF+AAAqBa85II5LPLciOWybxzn4+NzwVIoAACAqlDhgKVbt25O78S3bt26Sg0IAIDqiJKQcxUOWNq3b+/wuri4WMnJyfrhhx8sd1c8AACuFq68060nqnDA8tJLL5W7f9q0acrLy6v0gAAAAH7LZU9r/utf/6o333zTVacDAKBaMYxfbh53uRsloUuQlJTkcBMZAABw6ZjD4lyFA5aBAwc6vDZNU8ePH9eWLVu4cRwAAKgSFQ5YQkJCHF57eXmpefPmmjFjhrp37+6ygQEAUJ0w6da5CgUspaWleuCBB9S2bVvVrl27qsYEAEC1Y/z3v8qew1NVaNKtt7e3unfvzlOZAQBwsfMZlspunqrCq4TatGmj/fv3V8VYAAAAylXhgGXmzJkaO3asVq5cqePHjys3N9dhAwAAFUeGxblLnsMyY8YMPfHEE+rdu7ckqV+/fg636DdNU4ZhqLS01PWjBADAwxmG4fTRN5d6Dk91yQHL9OnT9dBDD+mLL76oyvEAAABc4JIDFtM0JUm33357lQ0GAIDqimXNzlVoWbMnp5oAAHAn7nTrXIUClmbNmv1u0JKVlVWpAQEAAPxWhQKW6dOnX3CnWwAAUHnnH2BY2XN4qgoFLIMHD1ZYWFhVjQUAgGqLOSzOXfJ9WJi/AgAA3KXCq4QAAEAVcMGkWw9+lNClByxlZWVVOQ4AAKo1LxnyqmTEUdnjraxCc1gAAEDVYFmzcxV+lhAAAMCVRoYFAAALYJWQcwQsAABYAPdhcY6SEAAAsDwyLAAAWACTbp0jYAEAwAK85IKSkAcva6YkBAAALI8MCwAAFkBJyDkCFgAALMBLlS97eHLZxJOvDQAAeAgyLAAAWIBhGDIqWdOp7PFWRsACAIAFGKr8w5Y9N1whYAEAwBK4061zzGEBAACWR4YFAACL8Nz8SOURsAAAYAHch8U5SkIAAMDyyLAAAGABLGt2joAFAAAL4E63znnytQEAAA9BhgUAAAugJOQcAQsAABbAnW6doyQEAAAsjwwLAAAWQEnIOQIWAAAsgFVCzhGwAABgAWRYnPPkYAwAAHgIMiwAAFgAq4ScI2ABAMACePihc5SEAACA5ZFhAQDAArxkyKuSRZ3KHm9lBCwAAFgAJSHnKAkBAADLI8MCAIAFGP/9r7Ln8FQELAAAWAAlIecoCQEAAMsjwwIAgAUYLlgl5MklITIsAABYwPmSUGW3y/Xss8/KMAyNGjXKvq+goEDx8fGqW7euAgMDNWjQIGVmZjocl56erj59+qhWrVoKCwvTuHHjVFJScvkDuQgCFgAALMCdAcvmzZv16quvql27dg77R48erU8++UT/+te/tH79eh07dkwDBw60t5eWlqpPnz4qKirSxo0b9fbbb2vRokWaMmVKZT6KchGwAADgYXJzcx22wsLCi/bNy8tTXFycXn/9ddWuXdu+PycnR//4xz/04osv6g9/+IM6duyot956Sxs3btS3334rSfr888+1a9cu/fOf/1T79u3Vq1cvPf3005o/f76Kiopcek0ELAAAWIDhov8kKTIyUiEhIfZt9uzZF33f+Ph49enTR7GxsQ77t27dquLiYof9LVq00LXXXqukpCRJUlJSktq2bavw8HB7nx49eig3N1c7d+505cfDpFsAAKzAyzi3VfYcknT48GEFBwfb9/v6+pbbf+nSpdq2bZs2b958QVtGRoZ8fHwUGhrqsD88PFwZGRn2Pr8OVs63n29zJQIWAAA8THBwsEPAUp7Dhw/r8ccf15o1a+Tn53eFRnb5KAkBAGABriwJXYqtW7fqxIkTuuGGG1SjRg3VqFFD69evV2JiomrUqKHw8HAVFRUpOzvb4bjMzEzZbDZJks1mu2DV0PnX5/u4CgELAAAWcKVXCd15551KSUlRcnKyfevUqZPi4uLs/65Zs6bWrl1rPyYtLU3p6emKiYmRJMXExCglJUUnTpyw91mzZo2Cg4PVqlUrl302EiUhAACqpaCgILVp08ZhX0BAgOrWrWvfP2zYMI0ZM0Z16tRRcHCwHn30UcXExOjmm2+WJHXv3l2tWrXSkCFDNGfOHGVkZGjSpEmKj4+/6LyZy0XAAgCABRiq/J1qXX2f25deekleXl4aNGiQCgsL1aNHD73yyiv2dm9vb61cuVIPP/ywYmJiFBAQoKFDh2rGjBkuHgkBCwAAluDKVUKX68svv3R47efnp/nz52v+/PkXPaZRo0b69NNPK/fGl4A5LAAAwPLIsOCq8M22vZr3zn+0Y3e6Mn7O1T/njlCfO663t5umqdmvrtLi5RuVk3dWnds10QsT/qzrrg2z99mx+7CmzVuubbvS5e1tqF+39po5epACa7m2zgpUlJch/U+369SrXYTqBvro59OF+iT5mN5Yv9/eZ+Qd16lHG5vCQ/xUXFqm1GO5emXtXv1wNMfeJ9i/hsb3bqlbm9WTaZpam5qp5z9L09miUndcFiqooqt8LnYOT3VVZFgMw9Dy5cvdPQy40ZmzhWrTrIHmjv9zue1/W/wfvfr+er04cbDWvDVWtfx9NOjR+SooLJYkHf8pWwPi5ykqsp7+89ZYffi3eKXuz1D89Heu5GUA5RraNUp3d4rUnFWpuvvv3yhxzR7d16WxBne+1t4n/WS+nvs0VX9+ZaOG/eM7Hc8+q/n33aDQWjXtfWYOaqcm9QIUv3irRr27XTc0qq1JfV27UgNVx90PP7Q6twcsGRkZevTRR9WkSRP5+voqMjJSffv2dVhG5U4ff/yxunfvrrp168owDCUnJ7t7SNXSH7u01qSH++qubtdf0Gaapha+94XGPthDvW9vpzbRDbRg+n3K+DlHq9bvkCT9+6sfVLOGt54ff4+iG4frhtaN9OLEP2vFumTtP/zTlb4cwMH1kaH6Mu2Evt7zs45nF2jtrkx9u++kWjf45cZfq1My9N3+LB09dVb7f8rXi/9OU6BfTUWHB0mSGl8ToC7R1+jpFbv0w9EcJadna86nu9W9jU3XBJFFvBoYLto8lVsDloMHD6pjx45at26d5s6dq5SUFK1evVrdunVTfHy8O4dml5+fr65du+q5555z91BwEYeOnlTmyVzdcVML+76QQH91bN1Ym78/KEkqKi5RzRre8vL65Svv7+sjSfo2ed8VHS/wWzsOZ+umqLq6tm4tSVJ0eKDaXxuqjXt+Lrd/DW9DAzs21OmzxdqTeVqS1C4yRLlni5V6LNfe77v9WSozTbVtEFL1FwFUMbfOYXnkkUdkGIa+++47BQQE2Pe3bt1aDz744EWPe/LJJ7Vs2TIdOXJENptNcXFxmjJlimrWPJca3bFjh0aNGqUtW7bIMAxFR0fr1VdfVadOnXTo0CElJCTo66+/VlFRkRo3bqy5c+eqd+/e5b7XkCFDJJ0Lri5FYWGhw1Mxc3NznfSGK2SePPcZ16sb5LA/rG6QTvy37dZOzfXUSx8r8Z3/6KHBd+jM2SJN//v/SZIyfs4R4E6Lvj6gQN8a+iihi8pMU16GoVfW7dVnKY7PYrm12TV65u528qvprZ/zCvXI4q3KPnOu7Fk30FdZ+Y5Pxy0tM5V7tkR1g3yu2LXg8nnJkFclazpeHpxjcVvAkpWVpdWrV2vWrFkOwcp5v33Y0q8FBQVp0aJFioiIUEpKikaMGKGgoCCNHz9ekhQXF6cOHTpowYIF8vb2VnJysj2YiY+PV1FRkTZs2KCAgADt2rVLgYGBLruu2bNna/r06S47H1yj5XX19cq0IZr00seaMX+FvL28NPLPtyusTpBD1gVwhz+2tqlnu/p66qMU7T+Rp2a2ID3Rq7l+yi3Uyh3H7P02HzilexcmKbSWj/5fxwZ69p7rNfT1TTr1m0AFVydXlHQ8N1xxY8Cyd+9emaapFi1a/H7n35g0aZL9340bN9bYsWO1dOlSe8CSnp6ucePG2c8dHR1t75+enq5Bgwapbdu2kqQmTZpU5jIuMHHiRI0ZM8b+Ojc3V5GRkS59DzgKr3uuzv/TydOyXfNL6vvEydNq26yh/fWfet6oP/W8USdO5qqWv68MQ3rl3XVq3KDuFR8z8GuPd2+mRV8f0Oc/nMuo7D2Rp/qhfnrg1iiHgKWguFRHss7qSNZZ/XAkR8se66IBNzTQW18d0Mm8QtUJcMykeHsZCvavoZOnCWhw9XPbn5amaV72se+//766dOkim82mwMBATZo0Senp6fb2MWPGaPjw4YqNjdWzzz6rfft+maPw2GOPaebMmerSpYumTp2q77//vlLX8Vu+vr72p2ReytMyUXmNGtRVeN1grd+cZt+Xm3dWW3ce1I3tGl/QP6xusAJr+WrZmm3y86mpbp0rHjQDruRX0+uCn4ll5u+v+PAyDNX0Pvdj/PvDOQr2r6kW9X8pjd4YVUdehqGUo5Q9rwrMunXKbQFLdHS0DMPQ7t27K3RcUlKS4uLi1Lt3b61cuVLbt2/XU089paKiX/6CmDZtmnbu3Kk+ffpo3bp1atWqlZYtWyZJGj58uPbv368hQ4YoJSVFnTp10rx581x6bXC9vDOFSkk7opS0I5KkQ8dOKiXtiA5nZMkwDD10bzc9/+Zqfbr+e+3ce1QPT3tHtmtC1Of2X1YVvfbBeu3YfVh7D2Xq9Q/Wa/ycDzQlvp9Cgmq567IASdJXaT/pwVubqGv0Naof6qduLcIUF9NIX+w+90A5v5reir+zqdo0DJEtxE8t6gdpSv/Wqhfkq//sPJeVOfhzvr7Z87Mm92ut1g2CdX1kqMb3bqHPf8jQz6cLnb09LOJKP635amOYlUl1VFKvXr2UkpKitLS0C+axZGdn2+exGIahZcuWacCAAXrhhRf0yiuvOGRNhg8frg8//PCCR2Cfd++99yo/P18rVqy4oG3ixIlatWrV72ZaDh48qKioKG3fvl3t27e/5GvMzc1VSEiIMk/mkG2phK+3/qi+DyVesP/ePp31yrQh9hvHvb3sG+XkndXN11+n55+8R00bhdv7PjR1sT7/5gflnylSdONwJfz1Tg3ufdOVvAyP1XHq5+4ewlWtlo+3Hv5DU3VrGabaAeduHLc6JUOvr9+nklJTPjW8NGtQW7VpGKLQWj7KOVOkncdy9Y/1+7XrV6uCgv1r6MneLXVr8//eOG7XCc39bDc3jquk0sJ8pb0wUDk5VfNz/PzvibXb0xUQVLnz55/O1Z0drq2ysbqTW1cJzZ8/X126dNFNN92kGTNmqF27diopKdGaNWu0YMECpaamXnBMdHS00tPTtXTpUt14441atWqVPXsiSWfPntW4ceN09913KyoqSkeOHNHmzZs1aNAgSdKoUaPUq1cvNWvWTKdOndIXX3yhli1bXnSMWVlZSk9P17Fj5+rIaWnnyg42m002m82VHwec6NqxmU5t/vtF2w3D0P8+dJf+96G7Ltpn4fT7qmJoQKWdKSrVC6vT9MLqtHLbi0rKNO79Hb97ntyzJXrqoxRXDw9Xiitu/Oa5CRb33oelSZMm2rZtm7p166YnnnhCbdq00R//+EetXbtWCxYsKPeYfv36afTo0UpISFD79u21ceNGTZ482d7u7e2tkydP6r777lOzZs10zz33qFevXvaVO6WlpYqPj1fLli3Vs2dPNWvWzOHJk7+1YsUKdejQQX369JEkDR48WB06dNDChQtd+EkAAKo7prA459aSUHVASQjVASUheLIrVRJal5yuwEqWhPJO5+oP7SkJAQCAqsKNWJwiYAEAwAJ4WrNzBCwAAFiAK562zNOaAQAA3IgMCwAAFsAUFucIWAAAsAIiFqcoCQEAAMsjwwIAgAWwSsg5AhYAACyAVULOURICAACWR4YFAAALYM6tcwQsAABYARGLU5SEAACA5ZFhAQDAAlgl5BwBCwAAFsAqIecIWAAAsACmsDjHHBYAAGB5ZFgAALACUixOEbAAAGABTLp1jpIQAACwPDIsAABYAKuEnCNgAQDAApjC4hwlIQAAYHlkWAAAsAJSLE4RsAAAYAGsEnKOkhAAALA8MiwAAFgAq4ScI2ABAMACmMLiHAELAABWQMTiFHNYAACA5ZFhAQDAAlgl5BwBCwAAVuCCSbceHK9QEgIAANZHhgUAAAtgzq1zBCwAAFgBEYtTlIQAAIDlkWEBAMACWCXkHAELAAAWwK35naMkBAAALI8MCwAAFsCcW+cIWAAAsAIiFqcIWAAAsAAm3TrHHBYAAGB5ZFgAALAAQy5YJeSSkVgTAQsAABbAFBbnKAkBAADLI8MCAIAFcOM45whYAACwBIpCzlASAgCgmpo9e7ZuvPFGBQUFKSwsTAMGDFBaWppDn4KCAsXHx6tu3boKDAzUoEGDlJmZ6dAnPT1dffr0Ua1atRQWFqZx48appKTEpWMlYAEAwALOl4Qqu1XE+vXrFR8fr2+//VZr1qxRcXGxunfvrvz8fHuf0aNH65NPPtG//vUvrV+/XseOHdPAgQPt7aWlperTp4+Kioq0ceNGvf3221q0aJGmTJniqo9GkmSYpmm69IxwkJubq5CQEGWezFFwcLC7hwNUiY5TP3f3EIAqU1qYr7QXBionp2p+jp//PbH70E8KquT5T+fmqkWjejp8+LDDWH19feXr6/u7x//0008KCwvT+vXrddtttyknJ0f16tXTu+++q7vvvluStHv3brVs2VJJSUm6+eab9dlnn+muu+7SsWPHFB4eLklauHChnnzySf3000/y8fGp1DWdR4YFAAAPExkZqZCQEPs2e/bsSzouJydHklSnTh1J0tatW1VcXKzY2Fh7nxYtWujaa69VUlKSJCkpKUlt27a1ByuS1KNHD+Xm5mrnzp2uuiQm3QIAYAWuXCVUXobl95SVlWnUqFHq0qWL2rRpI0nKyMiQj4+PQkNDHfqGh4crIyPD3ufXwcr59vNtrkLAAgCABbjyWULBwcEVLl/Fx8frhx9+0Ndff12pMVQVSkIAAFiB4aLtMiQkJGjlypX64osv1LBhQ/t+m82moqIiZWdnO/TPzMyUzWaz9/ntqqHzr8/3cQUCFgAAqinTNJWQkKBly5Zp3bp1ioqKcmjv2LGjatasqbVr19r3paWlKT09XTExMZKkmJgYpaSk6MSJE/Y+a9asUXBwsFq1auWysVISAgDAAtxx27j4+Hi9++67+r//+z8FBQXZ55yEhITI399fISEhGjZsmMaMGaM6deooODhYjz76qGJiYnTzzTdLkrp3765WrVppyJAhmjNnjjIyMjRp0iTFx8df0tyZS0XAAgCABbjj1vwLFiyQJN1xxx0O+9966y3df//9kqSXXnpJXl5eGjRokAoLC9WjRw+98sor9r7e3t5auXKlHn74YcXExCggIEBDhw7VjBkzKnMpFyBgAQCgmrqUW7H5+flp/vz5mj9//kX7NGrUSJ9++qkrh3YBAhYAACzAlauEPBEBCwAAVsCzD51ilRAAALA8MiwAAFgACRbnCFgAALAAd6wSuppQEgIAAJZHhgUAAEuo/CohTy4KEbAAAGABlIScoyQEAAAsj4AFAABYHiUhAAAsgJKQcwQsAABYALfmd46SEAAAsDwyLAAAWAAlIecIWAAAsABuze8cJSEAAGB5ZFgAALACUixOEbAAAGABrBJyjpIQAACwPDIsAABYAKuEnCNgAQDAApjC4hwBCwAAVkDE4hRzWAAAgOWRYQEAwAJYJeQcAQsAABbApFvnCFiqmGmakqTTubluHglQdUoL8909BKDKlBaekfTLz/OqkuuC3xOuOIdVEbBUsdOnT0uSmkZFunkkAIDKOH36tEJCQlx+Xh8fH9lsNkW76PeEzWaTj4+PS85lJYZZ1SFjNVdWVqZjx44pKChIhifn6iwiNzdXkZGROnz4sIKDg909HMDl+I5feaZp6vTp04qIiJCXV9WsVSkoKFBRUZFLzuXj4yM/Pz+XnMtKyLBUMS8vLzVs2NDdw6h2goOD+WEOj8Z3/MqqiszKr/n5+XlkkOFKLGsGAACWR8ACAAAsj4AFHsXX11dTp06Vr6+vu4cCVAm+46iumHQLAAAsjwwLAACwPAIWAABgeQQsAADA8ghYYGmGYWj58uXuHgZQJfh+A5eOgAVuk5GRoUcffVRNmjSRr6+vIiMj1bdvX61du9bdQ5N07u6WU6ZMUf369eXv76/Y2Fjt2bPH3cPCVcLq3++PP/5Y3bt3V926dWUYhpKTk909JMApAha4xcGDB9WxY0etW7dOc+fOVUpKilavXq1u3bopPj7e3cOTJM2ZM0eJiYlauHChNm3apICAAPXo0UMFBQXuHhos7mr4fufn56tr16567rnn3D0U4NKYgBv06tXLbNCggZmXl3dB26lTp+z/lmQuW7bM/nr8+PFmdHS06e/vb0ZFRZmTJk0yi4qK7O3JycnmHXfcYQYGBppBQUHmDTfcYG7evNk0TdM8ePCgedddd5mhoaFmrVq1zFatWpmrVq0qd3xlZWWmzWYz586da9+XnZ1t+vr6mu+9914lrx6ezurf7187cOCAKcncvn37ZV8vcCXwLCFccVlZWVq9erVmzZqlgICAC9pDQ0MvemxQUJAWLVqkiIgIpaSkaMSIEQoKCtL48eMlSXFxcerQoYMWLFggb29vJScnq2bNmpKk+Ph4FRUVacOGDQoICNCuXbsUGBhY7vscOHBAGRkZio2Nte8LCQlR586dlZSUpMGDB1fiE4Anuxq+38DViIAFV9zevXtlmqZatGhR4WMnTZpk/3fjxo01duxYLV261P4DPT09XePGjbOfOzo62t4/PT1dgwYNUtu2bSVJTZo0uej7ZGRkSJLCw8Md9oeHh9vbgPJcDd9v4GrEHBZccWYlbq78/vvvq0uXLrLZbAoMDNSkSZOUnp5ubx8zZoyGDx+u2NhYPfvss9q3b5+97bHHHtPMmTPVpUsXTZ06Vd9//32lrgMoD99voGoQsOCKi46OlmEY2r17d4WOS0pKUlxcnHr37q2VK1dq+/bteuqpp1RUVGTvM23aNO3cuVN9+vTRunXr1KpVKy1btkySNHz4cO3fv19DhgxRSkqKOnXqpHnz5pX7XjabTZKUmZnpsD8zM9PeBpTnavh+A1cl906hQXXVs2fPCk9KfP75580mTZo49B02bJgZEhJy0fcZPHiw2bdv33LbJkyYYLZt27bctvOTbp9//nn7vpycHCbd4pJY/fv9a0y6xdWCDAvcYv78+SotLdVNN92kjz76SHv27FFqaqoSExMVExNT7jHR0dFKT0/X0qVLtW/fPiUmJtr/upSks2fPKiEhQV9++aUOHTqkb775Rps3b1bLli0lSaNGjdK///1vHThwQNu2bdMXX3xhb/stwzA0atQozZw5UytWrFBKSoruu+8+RUREaMCAAS7/POBZrP79ls5NDk5OTtauXbskSWlpaUpOTmaOFqzL3RETqq9jx46Z8fHxZqNGjUwfHx+zQYMGZr9+/cwvvvjC3ke/WfY5btw4s27dumZgYKD55z//2XzppZfsf4EWFhaagwcPNiMjI00fHx8zIiLCTEhIMM+ePWuapmkmJCSY1113nenr62vWq1fPHDJkiPnzzz9fdHxlZWXm5MmTzfDwcNPX19e88847zbS0tKr4KOCBrP79fuutt0xJF2xTp06tgk8DqDzDNCsxQwwAAOAKoCQEAAAsj4AFAABYHgELAACwPAIWAABgeQQsAADA8ghYAACA5RGwAAAAyyNgAQAAlkfAAlQD999/v8MjBe644w6NGjXqio/jyy+/lGEYys7OvmgfwzC0fPnySz7ntGnT1L59+0qN6+DBgzIMQ8nJyZU6D4CqQ8ACuMn9998vwzBkGIZ8fHzUtGlTzZgxQyUlJVX+3h9//LGefvrpS+p7KUEGAFS1Gu4eAFCd9ezZU2+99ZYKCwv16aefKj4+XjVr1tTEiRMv6FtUVCQfHx+XvG+dOnVcch4AuFLIsABu5OvrK5vNpkaNGunhhx9WbGysVqxYIemXMs6sWbMUERGh5s2bS5IOHz6se+65R6GhoapTp4769++vgwcP2s9ZWlqqMWPGKDQ0VHXr1tX48eP120eG/bYkVFhYqCeffFKRkZHy9fVV06ZN9Y9//EMHDx5Ut27dJEm1a9eWYRi6//77JUllZWWaPXu2oqKi5O/vr+uvv14ffvihw/t8+umnatasmfz9/dWtWzeHcV6qJ598Us2aNVOtWrXUpEkTTZ48WcXFxRf0e/XVVxUZGalatWrpnnvuUU5OjkP7G2+8oZYtW8rPz08tWrTQK6+8UuGxAHAfAhbAQvz9/VVUVGR/vXbtWqWlpWnNmjVauXKliouL1aNHDwUFBemrr77SN998o8DAQPXs2dN+3AsvvKBFixbpzTff1Ndff62srCwtW7bM6fved999eu+995SYmKjU1FS9+uqrCgwMVGRkpD766CNJUlpamo4fP66//e1vkqTZs2dr8eLFWrhwoXbu3KnRo0frr3/9q9avXy/pXGA1cOBA9e3bV8nJyRo+fLgmTJhQ4c8kKChIixYt0q5du/S3v/1Nr7/+ul566SWHPnv37tUHH3ygTz75RKtXr9b27dv1yCOP2NuXLFmiKVOmaNasWUpNTdUzzzyjyZMn6+23367weAC4iZufFg1UW0OHDjX79+9vmqZplpWVmWvWrDF9fX3NsWPH2tvDw8PNwsJC+zHvvPOO2bx5c7OsrMy+r7Cw0PT39zf//e9/m6ZpmvXr1zfnzJljby8uLjYbNmxofy/TNM3bb7/dfPzxx03TNM20tDRTkrlmzZpyx/nFF1+YksxTp07Z9xUUFJi1atUyN27c6NB32LBh5r333muapmlOnDjRbNWqlUP7k08+ecG5fkuSuWzZsou2z5071+zYsaP99dSpU01vb2/zyJEj9n2fffaZ6eXlZR4/ftw0TdO87rrrzHfffdfhPE8//bQZExNjmqZpHjhwwJRkbt++/aLvC8C9mMMCuNHKlSsVGBio4uJilZWV6S9/+YumTZtmb2/btq3DvJUdO3Zo7969CgoKcjhPQUGB9u3bp5ycHB0/flydO3e2t9WoUUOdOnW6oCx0XnJysry9vXX77bdf8rj37t2rM2fO6I9//KPD/qKiInXo0EGSlJqa6jAOSYqJibnk9zjv/fffV2Jiovbt26e8vDyVlJQoODjYoc+1116rBg0aOLxPWVmZ0tLSFBQUpH379mnYsGEaMWKEvU9JSYlCQkIqPB4A7kHAArhRt27dtGDBAvn4+CgiIkI1ajj+XzIgIMDhdV5enjp27KglS5ZccK569epd1hj8/f0rfExeXp4kadWqVQ6BgnRuXo6rJCUlKS4uTtOnT1ePHj0UEhKipUuX6oUXXqjwWF9//fULAihvb2+XjRVA1SJgAdwoICBATZs2veT+N9xwg95//32FhYVdkGU4r379+tq0aZNuu+02SecyCVu3btUNN9xQbv+2bduqrKxM69evV2xs7AXt5zM8paWl9n2tWrWSr6+v0tPTL5qZadmypX0C8Xnffvvt71/kr2zcuFGNGjXSU089Zd936NChC/qlp6fr2LFjioiIsL+Pl5eXmjdvrvDwcEVERGj//v2Ki4ur0PsDsA4m3QJXkbi4OF1zzTXq37+/vvrqKx04cEBffvmlHnvsMR05ckSS9Pjjj+vZZ5/V8uXLtXv3bj3yyCNO76HSuHFjDR06VA8++KCWL19uP+cHH3wgSWrUqJEMw9DKlSv1008/KS8vT0FBQRo7dqxGjx6tt99+W/v27dO2bds0b948+0TWhx56SHv27NG4ceOUlpamd999V4sWLarQ9UZHRys9PV1Lly7Vvn37lJiYWO4EYj8/Pw0dOlQ7duzQV199pccee0z33HOPbDabJGn69OmaPXu2EhMT9eOPPyolJUVvvfWWXnzxxQqNB4D7ELAAV5FatWppw4YNuvbaazVw4EC1bNlSw4YNU0FBgT3j8sQTT2jIkCEaOnSoYmJiFBQUpP/3//6f0/MuWLBAd999tx555BG1aNFCI0aMUH5+viSpQYMGmj59uiZMmKDw8HAlJCRIkp5++mlNnjxZs2fPVsuWLdWzZ0+tWrVKUVFRks7NK/noo4+0fPlyXX/99Vq4cKGeeeaZCl1vv379NHr0aCUkJKh9+/bauHGjJk+efEG/pk2bauDAgerdu7e6d++udu3aOSxbHj58uN544w299dZbatu2rW6//XYtWrTIPlYA1meYF5uJBwAAYBFkWAAAgOURsAAAAMsjYAEAAJZHwAIAACyPgAUAAFgeAQsAALA8AhYAAGB5BCwAAMDyCFgAAIDlEbAAAADLI2ABAACW9/8Bv21QAwRuFQEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.57\n",
      "Recall: 0.88\n",
      "F1 Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오른쪽팔 6 8 10\n",
    "# 왼쪽팔 5 7 9 \n",
    "# 오른족 다리 12 14 16\n",
    "# 왼쪽 다리 11 13 15\n",
    "# 이걸로 각도 & \n",
    "# 독립변수 : 12345, 각도 4개 \n",
    "# 독립변수 : 얼굴 하나로 & 전체\n",
    "# 독립변수: 얼굴 하나로 & 각도 4개\n",
    "# 서로 대칭되는 곳끼리 거리 (56)(78)(910)(1112)(1314)(1516) & 머리\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 히트맵그리기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Combined = pd.read_csv(\"/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv\")\n",
    "Combined.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "columns_to_convert = Combined.columns.difference(['FILENAME','label'])\n",
    "\n",
    "# float으로 변환\n",
    "Combined[columns_to_convert] = Combined[columns_to_convert].astype(float)\n",
    "#스케일링 진행 후\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = Combined[coordinate_cols].values  # 26개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "Combined[coordinate_cols] = X_normalized\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        \n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(Combined, sequence_length)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Time: [13  8 11 17  9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Transformer 설정\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=input_size, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=hidden_size, \n",
    "                dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 키포인트별 중요도를 추정할 가중치 레이어\n",
    "        self.keypoint_importance_layer = nn.Linear(input_size, 1)\n",
    "        \n",
    "        # 분류를 위한 fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 키포인트 중요도 저장 변수\n",
    "        self.keypoint_importances = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(x)\n",
    "        \n",
    "        # 각 키포인트의 중요도 계산\n",
    "        keypoint_importances = self.keypoint_importance_layer(transformer_output)  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        # 중요도는 키포인트 차원 기준 Softmax 정규화\n",
    "        keypoint_importances = torch.softmax(keypoint_importances.squeeze(-1), dim=-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # 인스턴스 변수에 저장\n",
    "        self.keypoint_importances = keypoint_importances\n",
    "        \n",
    "        # 마지막 시퀀스의 출력 사용\n",
    "        output = transformer_output[:, -1, :]\n",
    "        \n",
    "        # 분류 수행\n",
    "        classification_output = self.fc(output)\n",
    "        \n",
    "        return classification_output\n",
    "\n",
    "def visualize_keypoint_importance(keypoint_importances):\n",
    "    \"\"\"\n",
    "    키포인트 중요도 시각화\n",
    "    \n",
    "    Parameters:\n",
    "    - keypoint_importances: numpy array of keypoint importances\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(range(len(keypoint_importances)), keypoint_importances)\n",
    "    plt.title(' Importance in Intoxication Detection')\n",
    "    plt.xlabel('Sequence Index')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 상위 5개 중요 키포인트 반환\n",
    "    top_keypoints = keypoint_importances.argsort()[-5:][::-1]\n",
    "    return top_keypoints\n",
    "\n",
    "model = TransformerModel(34,50,1,2,4,0.1)\n",
    "# 훈련 루프\n",
    "epoch_keypoint_importances = []\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 모델 예측\n",
    "    output = model(batch_X)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(output.squeeze(), batch_y.float())\n",
    "    \n",
    "    # 역전파\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 배치별 중요도 저장\n",
    "    if model.keypoint_importances is not None:\n",
    "        # 배치별 평균 중요도 계산 후 저장\n",
    "        batch_importance = model.keypoint_importances.mean(dim=0).detach().cpu().numpy()\n",
    "        epoch_keypoint_importances.append(batch_importance)\n",
    "\n",
    "# 에포크 평균 중요도 계산\n",
    "if epoch_keypoint_importances:\n",
    "    avg_keypoint_importances = np.mean(np.array(epoch_keypoint_importances), axis=0)\n",
    "    top_keypoints = visualize_keypoint_importance(avg_keypoint_importances)\n",
    "    print(f\"Top 5 Important Time: {top_keypoints}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_keypoint_importances:\n\u001b[1;32m    106\u001b[0m     avg_keypoint_importances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(epoch_keypoint_importances), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m     top_keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_keypoint_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg_keypoint_importances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 5 Important Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_keypoints\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 68\u001b[0m, in \u001b[0;36mvisualize_keypoint_importance\u001b[0;34m(keypoint_importances)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 시각화\u001b[39;00m\n\u001b[1;32m     67\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 68\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mavg_keypoint_importances\u001b[49m\u001b[43m)\u001b[49m), avg_keypoint_importances)\n\u001b[1;32m     69\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeypoint Index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance Score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Transformer Encoder 설정\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=input_size, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=hidden_size, \n",
    "                dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 키포인트 중요도를 계산할 레이어\n",
    "        self.keypoint_importance_layer = nn.Linear(input_size, 1)\n",
    "        \n",
    "        # 분류를 위한 Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 중요도 저장\n",
    "        self.keypoint_importances = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(x)  # (batch_size, seq_len, input_size)\n",
    "        \n",
    "        # 각 키포인트의 중요도 계산\n",
    "        keypoint_importances = self.keypoint_importance_layer(transformer_output).squeeze(-1)  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        # 중요도는 키포인트 차원 기준 Softmax 정규화\n",
    "        keypoint_importances = torch.softmax(keypoint_importances.mean(dim=1), dim=-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # 인스턴스 변수에 저장\n",
    "        self.keypoint_importances = keypoint_importances  # (batch_size, seq_len)\n",
    "        \n",
    "        # Transformer의 마지막 시퀀스의 출력 사용\n",
    "        output = transformer_output[:, -1, :]  # (batch_size, input_size)\n",
    "        \n",
    "        # 분류 수행\n",
    "        classification_output = self.fc(output)  # (batch_size, num_classes)\n",
    "        \n",
    "        return classification_output\n",
    "\n",
    "# 키포인트 중요도 시각화\n",
    "def visualize_keypoint_importance(keypoint_importances):\n",
    "    \"\"\"\n",
    "    키포인트 중요도 시각화\n",
    "    \n",
    "    Parameters:\n",
    "    - keypoint_importances: Tensor (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    # 배치 평균으로 키포인트 중요도 계산\n",
    "    avg_keypoint_importances = model.keypoint_importances.mean(dim=0).detach().cpu().numpy()  # (input_size,)\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(avg_keypoint_importances)), avg_keypoint_importances)\n",
    "    plt.xlabel(\"Keypoint Index\")\n",
    "    plt.ylabel(\"Importance Score\")\n",
    "    plt.title(\"Keypoint Importance\")\n",
    "    plt.show()\n",
    "\n",
    "model = TransformerModel(34,50,1)\n",
    "\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 훈련 루프\n",
    "epoch_keypoint_importances = []\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 모델 예측\n",
    "    output = model(batch_X)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(output.squeeze(), batch_y.float())\n",
    "    \n",
    "    # 역전파\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 배치별 중요도 저장\n",
    "    if model.keypoint_importances is not None:\n",
    "        # 배치별 평균 중요도 계산 후 저장\n",
    "        batch_importance = model.keypoint_importances.mean(dim=0).detach().cpu().numpy()\n",
    "        epoch_keypoint_importances.append(batch_importance)\n",
    "\n",
    "# 에포크 평균 중요도 계산\n",
    "if epoch_keypoint_importances:\n",
    "    avg_keypoint_importances = np.mean(np.array(epoch_keypoint_importances), axis=0)\n",
    "    top_keypoints = visualize_keypoint_importance(avg_keypoint_importances)\n",
    "    print(f\"Top 5 Important Time: {top_keypoints}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6303960680961609\n",
      "Top 5 Important Keypoints: [3 1 0 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6237255930900574\n",
      "Top 5 Important Keypoints: [3 2 1 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), batch_y\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# 역전파\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 키포인트 중요도 저장\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Transformer Encoder 설정\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=input_size, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=hidden_size, \n",
    "                dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # 키포인트 중요도를 계산할 레이어\n",
    "        self.keypoint_importance_layer = nn.Linear(input_size, 1)\n",
    "        \n",
    "        # 분류를 위한 Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 중요도 저장\n",
    "        self.keypoint_importances = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(x)  # (batch_size, seq_len, input_size)\n",
    "        \n",
    "        # 각 키포인트의 중요도 계산\n",
    "        keypoint_importances = self.keypoint_importance_layer(transformer_output)  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        # 중요도는 키포인트 차원 기준 Softmax 정규화\n",
    "        keypoint_importances = torch.softmax(keypoint_importances.squeeze(-1), dim=-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # 인스턴스 변수에 저장\n",
    "        self.keypoint_importances = keypoint_importances.mean(dim=1)  # (batch_size, seq_len 평균)\n",
    "        \n",
    "        # Transformer의 마지막 시퀀스의 출력 사용\n",
    "        output = transformer_output[:, -1, :]  # (batch_size, input_size)\n",
    "        \n",
    "        # 분류 수행\n",
    "        classification_output = self.fc(output)  # (batch_size, num_classes)\n",
    "        \n",
    "        return classification_output\n",
    "\n",
    "model = TransformerModel(34,50,1,2,4,0.1)\n",
    "#(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_keypoint_importances = []\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(output.squeeze(), batch_y.float())\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 키포인트 중요도 저장\n",
    "        if model.keypoint_importances is not None:\n",
    "            epoch_keypoint_importances.append(\n",
    "                model.keypoint_importances.detach().cpu().numpy()\n",
    "            )\n",
    "    \n",
    "    # 에포크 평균 중요도 계산\n",
    "    if epoch_keypoint_importances:\n",
    "        avg_keypoint_importances = np.mean(np.vstack(epoch_keypoint_importances), axis=0)\n",
    "        \n",
    "        # 시각화\n",
    "        top_keypoints = visualize_keypoint_importance(avg_keypoint_importances)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "        print(f\"Top 5 Important Keypoints: {top_keypoints}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([8, 90, 34])\n",
      "torch.Size([4, 90, 34])\n"
     ]
    }
   ],
   "source": [
    "for batch_X, batch_y in train_loader:\n",
    "    print(batch_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     18\u001b[0m last_attention_scores \u001b[38;5;241m=\u001b[39m epoch_attention_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# 마지막 배치\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_attention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[0;34m(attn_weights, seq_idx, head_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_attention\u001b[39m(attn_weights, seq_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, head_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# seq_idx: 특정 시퀀스 인덱스\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# head_idx: 특정 Attention Head\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mattn_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# (query_len, key_len)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(scores, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 시퀀스 길이와 각 키포인트에 대한 Attention Score 시각화\n",
    "def visualize_attention(attn_weights, seq_idx=0, head_idx=0):\n",
    "    # seq_idx: 특정 시퀀스 인덱스\n",
    "    # head_idx: 특정 Attention Head\n",
    "\n",
    "    scores = attn_weights[seq_idx, head_idx, :, :]  # (query_len, key_len)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(scores, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label=\"Attention Score\")\n",
    "    plt.xlabel(\"Key Points\")\n",
    "    plt.ylabel(\"Sequence Positions\")\n",
    "    plt.title(\"Attention Scores\")\n",
    "    plt.show()\n",
    "    \n",
    "last_attention_scores = epoch_attention_scores[-2]  # 마지막 배치\n",
    "visualize_attention(last_attention_scores, seq_idx=0, head_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 90, 34])\n",
      "torch.Size([16])\n",
      "torch.Size([4, 90, 34])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "tmp = pd.read_csv('/home/alpaco/project/drunk_prj/data/아카이브/616-6_cam02_drunken03_place01_day_summer_396_2687_part1.mp4_combined.csv')\n",
    "tmp['FILENAME'] = '/home/alpaco/project/drunk_prj/data/아카이브/616-6_cam02_drunken03_place01_day_summer_396_2687_part1.mp4_combined.csv'\n",
    "tmp.rename(columns=lambda col: 'label' if col == 'ID' else col, inplace=True)\n",
    "columns_to_convert = tmp.columns.difference(['FILENAME','label'])\n",
    "# float으로 변환\n",
    "tmp[columns_to_convert] = tmp[columns_to_convert].astype(float)\n",
    "#스케일링 진행 후\n",
    "\n",
    "\n",
    "\n",
    "coordinate_cols = [f'x{i}' for i in range(1, 18)] + [f'y{i}' for i in range(1, 18)]\n",
    "X = tmp[coordinate_cols].values  # 26개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "tmp[coordinate_cols] = X_normalized\n",
    "\n",
    "tmpx, tmpy = create_sequences(tmp, sequence_length)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "tmp_X_tensor = torch.FloatTensor(tmpx)\n",
    "tmp_y_tensor = torch.LongTensor(tmpy)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "\n",
    "tmp_dataset = TensorDataset(tmp_X_tensor, tmp_y_tensor)\n",
    "tmp_loader = DataLoader(tmp_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights shape: torch.Size([1, 90, 90])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAK9CAYAAABIGaGzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAADsiElEQVR4nOzdeXQUVcIF8FudpbOHfUdA0AFEQQEREaKIIK7ogIgLiA6DKG4ZETKoiIIBFQccBVwGXEbFDRx0FGEY1BFBZFFHVEBEUPY1IVtn6fr+cMxHS94l/dJJQbg/T58j/VKvXlW9qu7XVXXLcV3XhYiIiIiISIT4vG6AiIiIiIhULxpkiIiIiIhIRGmQISIiIiIiEaVBhoiIiIiIRJQGGSIiIiIiElEaZIiIiIiISERpkCEiIiIiIhGlQYaIiIiIiESUBhkiIiIiIhJRGmSIiFSA4zh44IEHvG6GiIjIUUWDDBHxzPTp0+E4Drp06VJm+TfffIMHHngAP/74Y5nTPv/885XbwP957733jrqBxAMPPADHcbBnz54yy5s3b45LLrmkUtvwyiuvYOrUqZU6DxEROTZpkCEinnn55ZfRvHlzrFixAt9///1h5d988w3Gjx9/VAwyxo8fX2ZZfn4+7r333ippx9FGgwwRETHRIENEPLFp0yZ8+umnePzxx1G3bl28/PLLXjfJSlxcHKKjo71uhoiIyFFFgwwR8cTLL7+MmjVr4uKLL0b//v0PG2Q8//zzGDBgAADgvPPOg+M4cBwHH374IZo3b461a9fio48+Kn3/3HPPLZ32wIEDuPPOO9G0aVP4/X60atUKkydPRjAYLP2bH3/8EY7j4LHHHsMzzzyDli1bwu/3o3Pnzvj8889L/+6GG27AU089BQCl83Icp7S8rHsy1qxZg759+yIlJQVJSUk4//zzsXz58sOWz3EcLF26FOnp6ahbty4SExNxxRVXYPfu3RVatybBYBBTp07FKaecgri4ONSvXx/Dhw/H/v37Q/7uH//4By6++GI0atQIfr8fLVu2xEMPPYSSkpLSvzn33HPxz3/+E5s3by5dJ82bNwcAfPjhh3AcB6+//jrGjx+Pxo0bIzk5Gf3790dWVhYCgQDuvPNO1KtXD0lJSRg6dCgCgUBIG2bPno2ePXuiXr168Pv9aNu2LWbMmHHYMv16WdjChQvRoUMHxMXFoW3btpg7d27kV6CIiJSbfn4TEU+8/PLLuPLKKxEbG4tBgwZhxowZ+Pzzz9G5c2cAQI8ePXD77bfjiSeewJ///Ge0adMGANCmTRtMnToVt912G5KSkjB27FgAQP369QEAeXl5SEtLw9atWzF8+HCccMIJ+PTTT5GRkYHt27cfdnnPK6+8goMHD2L48OFwHAePPPIIrrzySvzwww+IiYnB8OHDsW3bNixatAgvvfTSEZdr7dq16N69O1JSUnDPPfcgJiYGTz/9NM4991x89NFHh91/ctttt6FmzZoYN24cfvzxR0ydOhUjR47Ea6+9Vq71uG/fvjLfP3RA9avhw4fj+eefx9ChQ3H77bdj06ZNePLJJ7FmzRosXboUMTExAH4ZACUlJSE9PR1JSUn497//jfvvvx/Z2dl49NFHAQBjx45FVlYWfv75Z/zlL38BACQlJYXMLzMzE/Hx8RgzZgy+//57/PWvf0VMTAx8Ph/279+PBx54AMuXL8fzzz+PFi1a4P777y+ddsaMGTjllFNw2WWXITo6Gu+88w5uueUWBINB3HrrrSHz2bBhAwYOHIibb74ZQ4YMwezZszFgwAAsWLAAF1xwQbnWo4iIRJgrIlLFVq5c6QJwFy1a5Lqu6waDQbdJkybuHXfcEfJ3b7zxhgvAXbJkyWF1nHLKKW5aWtph7z/00ENuYmKiu379+pD3x4wZ40ZFRblbtmxxXdd1N23a5AJwa9eu7e7bt6/07/7xj3+4ANx33nmn9L1bb73VNR0uAbjjxo0r/Xe/fv3c2NhYd+PGjaXvbdu2zU1OTnZ79OhR+t7s2bNdAG6vXr3cYDBY+v5dd93lRkVFuQcOHChzfr8aN26cC4C+Lr744tK//89//uMCcF9++eWQehYsWHDY+3l5eYfNb/jw4W5CQoJbUFBQ+t7FF1/sNmvW7LC/XbJkiQvAbdeunVtYWFj6/qBBg1zHcdy+ffuG/H3Xrl0Pq6esNvTp08c98cQTQ95r1qyZC8B96623St/LyspyGzZs6J5++umH1SEiIlVDl0uJSJV7+eWXUb9+fZx33nkAfrnkaODAgZgzZ07IJTk23njjDXTv3h01a9bEnj17Sl+9evVCSUkJPv7445C/HzhwIGrWrFn67+7duwMAfvjhh7DnXVJSgoULF6Jfv3448cQTS99v2LAhrrnmGnzyySfIzs4OmeaPf/xjyOVX3bt3R0lJCTZv3lyueb711ltYtGjRYa9fz+z86o033kBqaiouuOCCkPXSsWNHJCUlYcmSJaV/Gx8fX/r/Bw8exJ49e9C9e3fk5eXhu+++K/f6GDx4cOnZEQDo0qULXNfFjTfeGPJ3Xbp0wU8//YTi4uIy25CVlYU9e/YgLS0NP/zwA7KyskKmb9SoEa644orSf6ekpGDw4MFYs2YNduzYUe72iohI5OhyKRGpUiUlJZgzZw7OO+88bNq0qfT9Ll26YMqUKVi8eDF69+5tXf+GDRvw1VdfoW7dumWW79q1K+TfJ5xwQsi/fx1w/PY+hfLYvXs38vLy8Lvf/e6wsjZt2iAYDOKnn37CKaecErH59+jRA3Xq1Dns/bi4uJB/b9iwAVlZWahXr16Z9Ry6XtauXYt7770X//73vw8bFP32Cz7z22VLTU0FADRt2vSw94PBILKyslC7dm0AwNKlSzFu3DgsW7YMeXl5h7Xh17oAoFWrViEDNQA4+eSTAfxy702DBg3K3WYREYkMDTJEpEr9+9//xvbt2zFnzhzMmTPnsPKXX365QoOMYDCICy64APfcc0+Z5b9++fxVVFRUmX/nuq51G8JRVfMPBoOoV6+eMcXr10HZgQMHkJaWhpSUFDz44INo2bIl4uLisHr1aowePbrMez1MTMt2pGXeuHEjzj//fLRu3RqPP/44mjZtitjYWLz33nv4y1/+ElYbRETEGxpkiEiVevnll1GvXr3SxKZDzZ07F/PmzcPMmTMRHx9/2K/ThzKVtWzZEjk5OejVq1fE2szacai6desiISEB69atO6zsu+++g8/nO+xX/KrSsmVL/Otf/0K3bt1CLkX6rQ8//BB79+7F3Llz0aNHj9L3Dz3r9KvyrpdwvfPOOwgEApg/f37I2ZBDL+k61Pfffw/XdUPas379egAoTbwSEZGqpXsyRKTK5OfnY+7cubjkkkvQv3//w14jR47EwYMHMX/+fABAYmIigF9+Xf+txMTEMt+/6qqrsGzZMnzwwQeHlR04cCDkuv/yYu04VFRUFHr37o1//OMfIQ8Q3LlzJ1555RWcc845SElJCXv+kXDVVVehpKQEDz300GFlxcXFpcv261mGQ8+kFBYWYvr06YdNl5iYGNblU+VVVhuysrIwe/bsMv9+27ZtmDdvXum/s7Oz8eKLL6JDhw66VEpExCM6kyEiVWb+/Pk4ePAgLrvssjLLzzrrrNIH8w0cOBAdOnRAVFQUJk+ejKysLPj9/tJnJ3Ts2BEzZszAhAkT0KpVK9SrVw89e/bEqFGjMH/+fFxyySW44YYb0LFjR+Tm5uK///0v3nzzTfz4449l3sPAdOzYEQBw++23o0+fPoiKisLVV19d5t9OmDABixYtwjnnnINbbrkF0dHRePrppxEIBPDII4+Et8IiKC0tDcOHD0dmZia++OIL9O7dGzExMdiwYQPeeOMNTJs2Df3798fZZ5+NmjVrYsiQIbj99tvhOA5eeumlMi/f6tixI1577TWkp6ejc+fOSEpKwqWXXlrhtvbu3RuxsbG49NJLMXz4cOTk5ODZZ59FvXr1sH379sP+/uSTT8ZNN92Ezz//HPXr18esWbOwc+dO46BERESqgHfBViJyvLn00kvduLg4Nzc31/g3N9xwgxsTE+Pu2bPHdV3XffbZZ90TTzzRjYqKComz3bFjh3vxxRe7ycnJLoCQONuDBw+6GRkZbqtWrdzY2Fi3Tp067tlnn+0+9thjpZGqv0bYPvroo4e1Ab+JpS0uLnZvu+02t27duq7jOCFxtr/9W9d13dWrV7t9+vRxk5KS3ISEBPe8885zP/3005C/+TXC9vPPPw95/9f417Jiew/1a4Tt7t27yyxv1qxZSITtr5555hm3Y8eObnx8vJucnOyeeuqp7j333ONu27at9G+WLl3qnnXWWW58fLzbqFEj95577nE/+OCDw9qVk5PjXnPNNW6NGjVcAKUxtL8uwxtvvFGuZS5rWebPn++edtppblxcnNu8eXN38uTJ7qxZs1wA7qZNmw5bzg8++MA97bTTXL/f77Zu3fqweYuISNVyXLeK7m4UERGJsObNm6Ndu3Z49913vW6KiIgcQvdkiIiIiIhIRGmQISIiIiIiEaVBhoiIiIiIRJTuyRARERERkYjSmQwREREREYkoDTJERERERCSiNMgQEREREZGIqpZP/O7b/C67CYPk9pRosqry881l8fF2bYmyGP+x22tKSsxlDplXkEzH1pftrT6OU/b7PsP7AN82MeYy10eW23L47QSKzYVsnZiWu5itf7ZNLdcXm66oyFxmKyam7PcLAuZpWF9gWH+N85vL2DYoLDSXRUeF3xa2bKz9/lhzWTHpkwzrrz6ybKSfuClJZb7v5JFjKOuvtscZti7ZNiDHk2CiuQ/58sruJy7Z31h9UfsOGsvYsgVrlb3+AfB9n/DlFJgLS4JWdRoFSX2s/Zb9JFjTvL58uw+QCQ3zi/TnIsD7K9tP2XcMNr8Ir+f3108Oe5qqEtxxsmfz9jVY79m8I8nTMxl79uzBI488giuuuAJdu3ZF165dccUVV+DRRx/F7t27vWyaiIiIiMhR76mnnkLz5s0RFxeHLl26YMWKFca/Xbt2LX7/+9+jefPmcBwHU6dOrXCdJp4NMj7//HOcfPLJeOKJJ5CamooePXqgR48eSE1NxRNPPIHWrVtj5cqVR6wnEAggOzs75BV0LX+5ExEREZHjXtDD/8Lx2muvIT09HePGjcPq1avRvn179OnTB7t27Srz7/Py8nDiiSdi0qRJaNCgQUTqNPEswvass85C+/btMXPmTDi/Of3mui5uvvlmfPXVV1i2bBmt54EHHsD48eND3muZ2gUn1egafqN0udThdLlUWHS5VJh0uVQoXS51OF0udRhdLlUGXS5VRpkul6qI4h2tPJt3dIPvy/23Xbp0QefOnfHkk08CAILBIJo2bYrbbrsNY8aModM2b94cd955J+68886I1Xkoz85kfPnll7jrrrsOG2AAgOM4uOuuu/DFF18csZ6MjAxkZWWFvFqmdq6EFouIiIiIVK6yrtIJBA7/8a2wsBCrVq1Cr169St/z+Xzo1avXEX+kN4lknZ4NMho0aECv71qxYgXq169/xHr8fj9SUlJCXj6nWt7PLiIiIiJVoMQNevbKzMxEampqyCszM/OwNu7ZswclJSWHfV+uX78+duzYYbXckazTs2/jd999N/74xz9i1apVOP/880sXZufOnVi8eDGeffZZPPbYY141T0RERESkymVkZCA9PT3kPb+fXNZ7lPJskHHrrbeiTp06+Mtf/oLp06ej5H/3DERFRaFjx454/vnncdVVV9lVzq63Zdj1hC65FjSKXPfIpov07TDW8yLT2d53YbtsxunYfSPsXhTzstErkNmqjGHX2lteg8zuDzGh181aXm9L51eFJz7Zdca27WfNZ9uNtYX1c5t9h61jtn8ztvsiu56bodePm6Zh90exfZ+sE9v7AUj76X1cbLooQ5n1vmixjo8wHbs/hHGjzTN0bD+LTVh9pnUM2K9nm/voAPM2YF3Sm9tjyxbpfnk0LVsYgvCu3fF+f7kGFXXq1EFUVBR27twZ8v7OnTuNN3VXZZ2eRtgOHDgQy5cvR15eHrZu3YqtW7ciLy8Py5cvtx9giIiIiIhUc7GxsejYsSMWL15c+l4wGMTixYvRtatFAFKE6zwqbl6IiYlBw4YNvW6GiIiIiEjYUbJeSU9Px5AhQ9CpUyeceeaZmDp1KnJzczF06FAAwODBg9G4cePSezoKCwvxzTfflP7/1q1b8cUXXyApKQmtWrUqV53ldVQMMkREREREJDwDBw7E7t27cf/992PHjh3o0KEDFixYUHqv85YtW+A75FLPbdu24fTTTy/992OPPYbHHnsMaWlp+PDDD8tVZ3l59pyMytT3hDvtJmSrgmXeF5EcepKrbp1Db6zP8p4Mdv0lu665Mu7JMGHXQrMcfbbdbJ5FAn5PhpNHnu/AmO7rYX2LbW92jTtbJwx7XoQtU1vY8ycq4/pq1oeYfPKMAHqvlqEtbNuw9c+e82H7fBPbezLIcznc1MQy33dyyX7DjqGVcU8GOS64sYbnugBwE8zPKnEKDP2ZPScjzlxf1P4cYxnr5+y5D7b3ZPjyzdvOKYzwMYMdDy2P50wwNcFY5tubbZ7QtA1s73Gs6udk2NwjyJBlO5qfk5G7vZln805suNmzeUeSzmSIiIiIiByipPr9Bl/lqucgw3Zkz56gzH6VzMkzl7FfGAvJL4wlFr8+sqf+2j7htqoZU3cq4ddr9sswfWI2+ZXH9unKVk+AtkwiijL/Gkt//bVdNvq0dkOfLSLTsH2RPdmeYdOxsxysLeRXb+P8Ysk+DHJ2h2Htp7+ssr7AjpUkbShg2YdssP5KvzyYt5vDpguQY7bFL8N0Xpa/NAfJWVhfoeW2YcdK05km26dGs7NyUeSz1vYsAf0cIPuV6TgaS44X7Owtw/Zhh12JYLHdgMg/xV2qteo5yBARERERseRlhG114WmErYiIiIiIVD8aZIiIiIiISETpcikRERERkUOU6HKpCtOZDBERERERiSidyRAREREROYRu/K6442+QwWJqWTSbz9zZXBJ/SYNXbR8iZ2wI2yEsdxZWZ2U8WMjmIWXsoUJsOtIO9mAqp6QSHupmmp91DCfBHmjFVMLDrowxzrYPgbTtk/Fx5jIW18rqZLG4puVjD/djaFRlJaxL24eTmhJ62XGZRTWzfsIivW33HdtY2SLD8rFdisWIsjLSRl++5YMZCaeYPRTUInqcTcP2U7ZOGLK+HNN2A+yOzWz9088q9hlnGffNHjrJYtpt6HkTx63jb5AhIiIiIkLoYXwVp3syREREREQkojw/k/Htt99i+fLl6Nq1K1q3bo3vvvsO06ZNQyAQwHXXXYeePXvS6QOBAAKBQMh7QbcYPsfzRRMREREROS55eiZjwYIF6NChA+6++26cfvrpWLBgAXr06IHvv/8emzdvRu/evfHvf/+b1pGZmYnU1NSQ18asz6toCURERESkugl6+KouPB1kPPjggxg1ahT27t2L2bNn45prrsGwYcOwaNEiLF68GKNGjcKkSZNoHRkZGcjKygp5tUztXEVLICIiIiIiv+XpIGPt2rW44YYbAABXXXUVDh48iP79+5eWX3vttfjqq69oHX6/HykpKSEvXSolIiIiIrZK4Hr2qi48/zbu/C+uzufzIS4uDqmpqaVlycnJyMrKCr9SFgOZEG8uI0kC7v4DxrJgbp6xzPnN/SIhZSzu1BCH6BaZI/CCeeZ2+GJJnCOLuWNsomgBODbzY3GzLEqQRdGydeIjy5aUaG5LapJ5fgUkvvCAoZ/XSC37fQDurj3meTWoa55Xjrmf0IjIQKG5jMXbsphRw77qkn3YifOb64uJMZeRiMiSH38ylrnFkY/9NEVSWu0bAKLq1jaWuewYFE+Oh0SwRrKxbMslNYxla2+dUeb7F/e4wjwzcnxl/YRFjLok7tQhfcgh+35+qybGsvhP1hkqNB9nCju1MpbFbd1tLHNNsdAA9pzXyFiWfaKxCCcsMq/n2LW7zG0xxRnT6GTztmHL5sSYP0/Z9mb95MBVHY1lNd7YZK7TFBvNPvNZ9HBCgnk6clxz69Y0lhXXNNe5syM5LrCkZsNhIWj5FUOOfZ6eyWjevDk2bNhQ+u9ly5bhhBNOKP33li1b0LBhQy+aJiIiIiIiljw9kzFixAiUHPLAqnbt2oWUv//++0dMlxIRERERiST23F0pH08HGTfffDMtf/jhh6uoJSIiIiIiEime35MhIiIiInI0qU5Rsl7RE79FRERERCSidCZDREREROQQJSApk1IujuuyLLljU98W6eZCtrgsatNvjjt19x0wljm1zRFytC2xhhhF2/bbbmYWD8vmF2ksIpVETrok1hBRdgcQp5jFIZaYyxjT9rFd/yyCl8Uoslhi1hZbprYUmGNXaRsZtg8km6NJUUCie0mkNI3TDRr6CdumbB/wWWZEmqI2Ad6XyfGQRoj7DfHDpvUB8P4aJNuU7fusL7DlZjGpfnMZPWaY6osl9WWTGGpWZzyJf2aizX3PKSTHBdO6tIw/Z3GtMMXlHgnpQ26ieX05B8k2MO3HbNlYfzVEXgPgx3pDFD4AHn1r+dloRLr/grUTIzuvCNrwsznyubKd1GSbZ/OOJF0uJSIiIiIiEaXLpUREREREDsFOlEr56EyGiIiIiIhElM5kiIiIiIgcQjd+V5zOZIiIiIiISERpkCEiIiIiIhFVPS+XYlF2LDqPlQXMMZbBVk2NZVHb9pjrZPFyNkgbaXQeW24Wm8mwWD0brP0k7tRhUaisjWydxMeZy4Ikq88iKtEJkHbEmbeNS/qWQ9aly5Y7kSy3JSdgiIBlkbK2McFs21jGltJY30KyP5q2j03sLXg0qXMwl7TDvL3dWimkLeZ16USZozGDSWW305dPooBt47dtI7b9bBvYRoFb9FnbqGZWJWkHi+AFi+Bl69lmGdjnIluPpN/x+ZFtyroQi1Y21UmPF2QfsMWOeWS/injPO0aflKDLpSpOZzJERERERCSiPB1krF69Gps2bSr990svvYRu3bqhadOmOOecczBnzpwj1hEIBJCdnR3yCrqWv3SKiIiIyHEv6DqevaoLTwcZQ4cOxcaNGwEAzz33HIYPH45OnTph7Nix6Ny5M4YNG4ZZs2bROjIzM5Gamhry2nhwZVU0X0REREREyuDpPRkbNmzASSedBACYPn06pk2bhmHDhpWWd+7cGRMnTsSNN95orCMjIwPp6ekh7w1ok1E5DRYRERGRak/3ZFScp4OMhIQE7NmzB82aNcPWrVtx5plnhpR36dIl5HKqsvj9fvj9oTcT+hzLG8BERERERKTCPL1cqm/fvpgxYwYAIC0tDW+++WZI+euvv45WrVp50TQREREREbHk6ZmMyZMno1u3bkhLS0OnTp0wZcoUfPjhh2jTpg3WrVuH5cuXY968eeFXHGeOc6SReiw6r8R8M7lDymhbWOSeqU7Wfn+suYxhsYwsvpOxjawzLR+L4mOiyTom65JFuTokItmNN28Dh8RAOgWG+EIWD0kjCEmfZNOxdWL7kwSNhjYsN+t3tn2Bsa0zMd5cZlo2wBwdy/ZFEpnp5JOoZnZcYP28kPRzdqwk69KXZ4j1ZdGkbB9gMdS2EeEsAp19DrA+VBR+PKnDprE9vgZI+9lyk/hk2mdN24dGyZPtxrY3i4y2RLcBndCwDCz2lkbikuVm659F5lZGrOwxGlVrUqIA1grzdA02atQIa9asQdeuXbFgwQK4rosVK1Zg4cKFaNKkCZYuXYqLLrrIyyaKiIiIiEiYPH8YX40aNTBp0iRMmjTJ66aIiIiIiFSrKFmv6FyQiIiIiIhElAYZIiIiIiISUZ5fLiUiIiIicjTRczIqrnoOMliiAsPSHQrNKRMsNYhNB79FB2bpDWxexzrXLmXJti84LOGEJFbRvsBSckx1shQWJoq0n6UeRbHUF8u22ExH01RIGUsTY1iyDkuBY6lILJnGtH18bF6kjbEkQYql7rBtEx1nLmNIMlgwPqbM942pU4B9v7PFtjdJrHLJcjumY5Rt2iHrrwzbr2LL3jYAeD9nZWx+JmydsGM9+/yuDKwtpmVg07CUymLL1CaWeMbaUhkJfnJcqp6DDBERERERSyXWue3yK61BERERERGJKJ3JEBERERE5RFC/w1eY1qCIiIiIiESUBhkiIiIiIhJRulxKREREROQQirCtuOo5yGBRoUxRgbmMxRPuzbKajrYzzhBJaRuBx6ZjWAynbZ02WKxhDIleZJGmLOaVzM8lMa9OwC5a0jXEVTosqpJtb0R+nTj5JGbUlqktBQHzNCxemMYykjLT/gbwiM68fHMZiyA11sm2KUGaT7FjUAHZ3vF+c1m++TjqM20fto5Zf2XblO3fDFknLKbWjbX4OGVNJFG6DoslZvuA3xxL7LJjbALZ3kXmbefYRKGyYy8po/Ni/YTG4pK+l5MXfp0shppF8NrGOLP2szpZjDPD1rMcl6rnIENERERExJIibCvO00HG9u3bMWPGDHzyySfYvn07fD4fTjzxRPTr1w833HADomxH0yIiIiIi4hnPhmkrV65EmzZt8N5776GoqAgbNmxAx44dkZiYiLvvvhs9evTAwYMHj1hPIBBAdnZ2yCvoWj4NVUREREREKsyzQcadd96Ju+66CytXrsR//vMfPP/881i/fj3mzJmDH374AXl5ebj33nuPWE9mZiZSU1NDXhuzV1bBEoiIiIhIdRSE49mruvBskLF69Wpcf/31pf++5pprsHr1auzcuRM1a9bEI488gjfffPOI9WRkZCArKyvk1TKlU2U2XURERERECM/uyahXrx62b9+OE088EQCwc+dOFBcXIyUlBQBw0kknYd++fUesx+/3w+8PTb7wObqfXURERETslOhRchXm2bfxfv364eabb8ajjz4Kv9+Phx56CGlpaYiPjwcArFu3Do0bN7ar3DbujU3nIzehs+jbWJIt6YvwKTEWgWcb68tURp0mLI6SlZEIPxZFS/sCizy0nM4xRXjaRi/aRoIylRFZbIp5pfMi/Y4lwNL+SvZT2+3N1rOpLZXQt/hxzVyEIIkmZdHKrO+ZlptFe9ouNzu+sv2KRKE6QVYnmc7QTpfVx3ZTFuNM9g83OvJfmmh0rM1nBIupZduUzcvy2EXjymn/MrzPYq1tv7ewdrDvLXQ6y37iKMJWQnk2yJgwYQK2b9+OSy+9FCUlJejatSv+/ve/l5Y7joPMzEyvmiciIiIiIpY8G2QkJSXhtddeQ0FBAYqLi5GUlBRS3rt3b49aJiIiIiLHMz0no+I8v3khLs78BFIRERERETn2eD7IEBERERE5mgR143eFaQ2KiIiIiEhE6UyGiIiIiMghStzq81A8r2iQcajKiL5lSljUoyF6jkXx0TIS70fjEI8BtuuExUf6LKP4WJwjY+pDbLvZYuukKmOJgcqJxbVRCfGX1v0y0vOqDLbzM8VNF1fCMagy+jI71tt+Dhi4tlGuQcuYVFJG21KF64RGD9uiMcgR7pe264PFtDO2MbW28zOpjO0mx4Rj/NuliIiIiIgcbXQmQ0RERETkEHrid8VpDYqIiIiISETpTIaIiIiIyCGCehhfhWkNioiIiIhIRB0Vg4yff/4ZOTk5h71fVFSEjz/+2IMWiYiIiIiILU8vl9q+fTsuv/xyrFq1Co7j4JprrsH06dORlJQEANi3bx/OO+88lLCo17KwGFEWIcei7IqLzWVRhrhZAAiymFrSzpICc5kJjTVkMX0s5pWURTqeEDBvA9YOtm3IdLT1ZF268bHm6QKFVnUi1lBndCXsotGsv1Zx1KCpLUGy3NYxkGTZikgfirHcBjbrMs5vLqN9i/xeZLl/IDbGXFZUZC6LMU/nRpfdTodE2Lpk/TvF5PhaSNYXO9ab9kXwKFc3ivRLw3LT2NgYsp+y7U2Kgonm/uUUmz+P2LK55PPWiXS0ckHAXMb2Hfr5Z162oN/cl6NIPzfucywalh2D2HcaGsFL9g/WFnaMreqYcw/pxu+K83QNjhkzBj6fD5999hkWLFiAb775Bueddx72799f+jfu0ZKjLyIiIiIi5eLpmYx//etfmDdvHjp16gQAWLp0KQYMGICePXti8eLFAI7woB8AgUAAgUDorxtBtwQ+h/wKJCIiIiJioCd+V5ynZzKysrJQs2bN0n/7/X7MnTsXzZs3x3nnnYddu3YdsY7MzEykpqaGvDYeXFmZzRYREREREcLTQcaJJ56Ir776KuS96OhovPHGGzjxxBNxySWXHLGOjIwMZGVlhbxaJneqrCaLiIiISDUXhM+zV3Xh6ZL07dsXzzzzzGHv/zrQ6NChwxHvyfD7/UhJSQl56VIpERERERHveHpPxsSJE5GXl1dmWXR0NN566y1s3bq1ilslIiIiIiIV4ekgIzo6GikpKcby7du3Y/z48Zg1a1Z4FdsmUrHpWBmLl2Nxb5FOziqxbL/tzU2sTtuoYBtsuW2xdWK7bCz5zyaW1Xb9MzQG+Si5Ca4ylptFTftIVKXtMcOG7XJXxjGIRoKS6Vj0qgGNQWX9le6LlstNmu+SKHPH4hjlxpIz8ix+lCxbMMY8ncPqpEiccaSPa1Udn8piidmysVh7ExbHfDRh+1w1U6InflfYUb0G9+3bhxdeeMHrZoiIiIiISBg8PZMxf/58Wv7DDz9UUUtERERERH4R5I/slXLwdJDRr18/OI5Db+4+0nMyRERERETk6OLp5VINGzbE3LlzEQwGy3ytXr3ay+aJiIiIiIgFTwcZHTt2xKpVq4zlRzrLISIiIiISaSWuz7NXdeHp5VKjRo1Cbm6usbxVq1ZYsmRJFbZIREREREQqytNBRvfu3Wl5YmIi0tLSwq+4uNhcZh1xaRkDydrCWEQ90jYeK0zrkkUCsuUOWv4iwPoJK7PdBqZYQBZ9adtGhkV7VuX9UbYxwWy/of3EXKdL6nRonaSsKvdvdgyisbhxdm0hEZduTNn7sUPWh0tiRB2W+mkbd8qiXEk7g3EswtbQUNLPixPMH8/R+1h8qrks6De30SXbzSH7hy/fcp8zTkPaQSaj+6lD9m+yDYIkRjgq0sffqv78ZvtHVUbQH8VKju4A1mOC1qCIiIiIiESUp2cyRERERESONkHbBxVLKZ3JEBERERGRiNIgQ0REREREIuqoulzKdV18+OGH+P7779GwYUP06dMHMTExXjdLRERERI4juvG74jwdZFx00UV49dVXkZqain379uGiiy7CihUrUKdOHezduxcnn3wyPv74Y9StWze8illyi02qC2CftlAZ05mwBKZIz6uq62T1seW23d4kaaXKU51s0AQQ0ka23LZJVzaqOgWONcU2JcpmGSqjb9keFxjLtjhFhpQl1g4WulMZx16WukOm8xWYo64cw77DLvmOKrBMBSN8hXYJRqb2V6Qs0hy2TkgZm84pskx8svkcq4z0Pvq5aXnsYvsH+/yQ45Knw7QFCxYgEAgAAO69914cPHgQGzduxK5du7B582YkJibi/vvv97KJIiIiInKcCbo+z17VxVGzJP/+97+RmZmJFi1aAACaNGmCyZMn44MPPvC4ZSIiIiIiEg7P78lw/ndabv/+/WjZsmVIWatWrbBt2zY6fSAQKD0b8qugWwKfQy4REBERERGRSuP5mYwbbrgBV155JYqKirBp06aQsh07dqBGjRp0+szMTKSmpoa8NuasqsQWi4iIiEh1VgLHs1d14ekgY8iQIahXrx5SU1Nx+eWXIy8vL6T8rbfeQocOHWgdGRkZyMrKCnm1TOpYia0WERERERHG08ulZs+eTcvHjRuHKJaMAsDv98Pv94e8p0ulRERERMRWdboB2yue35PB7Nu3D+PGjcOsWbPCm5ANTGxjDW2jUKsyIrIq43KPVGek2Ub/RZMubhlT65IyJ5r0vUjH1FpySV+mMZC0UrvpTOuStsO2nzOsL9hut6qM9WXtt+2TtvGXTGXEdJrQbWpZp20kqyEG2WHHEjYvy5hdHjdrrtOl65K0xVRnZUSd0hjkSjj20uU2xxkb0fbbdlg2P8sfY2n8s12VUn0d1cO0ffv24YUXXvC6GSIiIiJyHNE9GRXn6ZmM+fPn0/IffvihiloiIiIiIiKR4ukgo1+/fnAcBy47vVuVp9dFRERERKTCPL1cqmHDhpg7dy6CwWCZr9WrV3vZPBERERE5DumJ3xXn6ZJ07NgRq1aZn2lxpLMcIiIiIiJy9PH0cqlRo0YhNzfXWN6qVSssWbKkClskIiIiIse7kmp0RsErng4yunfvTssTExORlpYWfsWxseYyFp1XXGwui4kxl+Xlm8sSE8xlBQFzmY04v7nMNhK0mGTSuSRWz7GN/DXUyepjcZox5i7OomgZh8UJ0phRUmmRoe+xdczieUlfdtj6YlGPrC/YMm0fFtHJIlnZPkwjqkmdLI6SxWX7yXHI1E7WRnYMMvUfgB8PKyM2k/W93IKyC8j2pvflVca+SPo5a0tUjmHZLEUdIJ8rlmf4o3LMnzlOoMg8u2gSe82idk3bxzaRlX5+s88qu8+/qLxCuzpNZew4Qz6rKHbMZuuLtSXSkd7sc0yqNQ3TREREREQkoo7qh/GJiIiIiFS1YDV6XoVXdCZDREREREQiSmcyREREREQOoRu/K05rUEREREREIsrzQca7776L+++/H0uXLgUA/Pvf/8ZFF12ECy+8EM8884zHrRMRERGR403QdTx7VReeXi719NNPY+TIkWjfvj2mTZuGp556CrfccgsGDhyIqKgo3HnnncjPz8cdd9wRXsUs4pKxjZeLJdGSrC0s4tI0HYuPY9Gktg81ZNOxRNNIR9aRSD03zrz+Xb+5LOi36/5RueZYQzcp3lhGox5NEaQBFslqjpyk8Z2kD7mkvzos1pAh83NMsZNB0rmClRBjmUjin1lENWMTccnmxY4zxWRe8XHmMoZFmiabo7mdAznmOh2L6F7SzelxhkV6s+NoIYktZXHGdBkMC0GiueneZqrvCO1w2HQsRpjFJzM2keSMn2xTtt1oVDNZX2y6fBJZbOonrD62bOyzPYr0FNvod/a9he07xuWz/E4mxzxPBxlPPPEEpk+fjmHDhmHJkiW46KKLMGXKFNxyyy0AgLPOOguPPPJI+IMMERERERHxjKeXS23atAl9+vQBAJx33nkoKSlBjx49SsvPPfdcbN68mdYRCASQnZ0d8goGya+/IiIiIiJECXyevcL11FNPoXnz5oiLi0OXLl2wYsUK+vdvvPEGWrdujbi4OJx66ql47733QspzcnIwcuRINGnSBPHx8Wjbti1mzpwZdrs8HWTUrl27dBCxbds2FBcXY8uWLaXlmzdvRq1atWgdmZmZSE1NDXlt3L+8UtstIiIiIuK11157Denp6Rg3bhxWr16N9u3bo0+fPti1a1eZf//pp59i0KBBuOmmm7BmzRr069cP/fr1w9dff136N+np6ViwYAH+/ve/49tvv8Wdd96JkSNHYv78+WG1zXFd24v1K27kyJFYuHAhhgwZgvnz56NNmzb47LPP8Je//AWO42DUqFHo3Lkz/va3vxnrCAQCCAQCIe8NOGM8fD6LK8HY9dxx5P4Jdm0mux7adB0+cPTck8HWCbmGN+L3ZESZr+l0483bpqrvyWDXbNvdk0Hmxe6RYPdkxJL1xe7JCFpuU9ZnTX6zT4eIJtuNrS96T4b5HgPrezLYPVfFhu3Njgnsngy2bJVxTwa5h4Xek2HqX6z97F65qr4ngxyH4CPHX4t7MijLezLotfbseH603JPB7o2shHsy6D53tNyTwRw192SYvb9+ctjTVJV7vhzg2bwfaf9Guf+2S5cu6Ny5M5588kkAQDAYRNOmTXHbbbdhzJgxh/39wIEDkZubi3fffbf0vbPOOgsdOnQoPVvRrl07DBw4EPfdd1/p33Ts2BF9+/bFhAkTyt02T89kTJ48Geeeey7mzJmDDh064JlnnsFNN92Eyy+/HH379kXt2rWRmZlJ6/D7/UhJSQl5WQ0wREREREQ8VtatAL/9QR0ACgsLsWrVKvTq1av0PZ/Ph169emHZsmVl1r1s2bKQvweAPn36hPz92Wefjfnz52Pr1q1wXRdLlizB+vXr0bt377CWw9NBRmJiIp555hn897//xdNPP43Y2FjcfffdyMrKQlZWFpYsWYJ69ep52UQRERERkSpT1q0AZf3ovmfPHpSUlKB+/foh79evXx87duwos+4dO3Yc8e//+te/om3btmjSpAliY2Nx4YUX4qmnngq5b7o8jsqf/OPi4hAXF4effvoJ48aNw6xZs8Ka3mWnVAl2OYgbQy4jKSSX8sSa2+KQSyPcaMP4j12xQtoIdqkLubTGYafQ6eVZlpfymE63kmlccvlMSQK7XMouVs9XSC6JYn2IzM7JM1z+QC9RIuufbRpymZjLLgcptAxUYD9lGMoccrUUj+cl64ttG3aZmO3lJ+w4ZLrkoJhcgsEupygmlzaRYxCNO801XybmRieap2Pred/BsqdJTjJOA5/d5RkuuXzJcSwuXQTo5aNuvnl9OXGGS9bYcS03z1wfuwSOrX9SJ43nJcdYh11qaJ6ZuYjt3+yyZbbd2D7MtsG+A+bJ2OWQhvm57HJadhmo7eVSDDl2Gb9/HAG9JPgYFPTwd/iMjAykp6eHvOdnnwER9te//hXLly/H/Pnz0axZM3z88ce49dZb0ahRo8POgjBH5SDjV/v27cMLL7wQ9iBDRERERORY5Pf7yzWoqFOnDqKiorBz586Q93fu3IkGDRqUOU2DBg3o3+fn5+PPf/4z5s2bh4svvhgAcNppp+GLL77AY489duwMMo50l/oPP/xQRS0REREREflFyTHw5O3Y2Fh07NgRixcvRr9+/QD8cuP34sWLMXLkyDKn6dq1KxYvXow777yz9L1Fixaha9euAICioiIUFRXB95uzwVFRUQiGGQDj6SCjX79+cBwHLODKYadORURERESOU+np6RgyZAg6deqEM888E1OnTkVubi6GDh0KABg8eDAaN25cek/HHXfcgbS0NEyZMgUXX3wx5syZg5UrV+KZZ54BAKSkpCAtLQ2jRo1CfHw8mjVrho8++ggvvvgiHn/88bDa5ukgo2HDhpg+fTouv/zyMsu/+OILdOzYsYpbJSIiIiLHs+AxcCYD+CWSdvfu3bj//vuxY8cOdOjQAQsWLCi9uXvLli0hZyXOPvtsvPLKK7j33nvx5z//GSeddBLefvtttGvXrvRv5syZg4yMDFx77bXYt28fmjVrhokTJ+Lmm28Oq22eDjI6duyIVatWGQcZRzrLISIiIiJyPBs5cqTx8qgPP/zwsPcGDBiAAQPMzwFp0KABZs+eXeF2eTrIGDVqFHJzc43lrVq1wpIlS6qwRSIiIiIiUlGeDjK6d+9OyxMTE5GWlhZ2vW48eUoni2sl8YRBUmfUQXN0YUmCOeYuOoc8Sdj0oFRyZqeYzIs+rZmsE1+xeToa7RnpM1DkKaNBElNbWNO8TgqT7OLpkvPMUYlRLOaVrZMCQ2YrixBmT6MtItP5ko1FNKrZdpvaxEdaPlXWZU8KJ3XS2NUCc6ysk5Jino48bd50rHFJDKfDYiwPmpebHbtYJHYUi9tk64tsg6DhKck+EvfrWD4d2kmIt5rOJU94d0jEs8ueAG3VDhJhy9pBnoLumo4zAFwSg8zmB1/4l5TYblOQ2Fi23SjSfno8YcttOm6zz2FWH3uaPMOOlSSmlh27wGJqfeHdFHy0C7qePkquWtAaFBERERGRiDqqn5MhIiIiIlLVSnBs3Ph9NNOZDBERERERiaijepCxf/9+vPjii143Q0REREREwnBUDzK2bNlS+jAREREREZGqEHQdz17Vhaf3ZGRnZ9PygwcP2lVM0pJc9gRxlu7A6oz3l6dVYdVpneQTaWQY6gZJ+6uwHXx7mydzLVJRgCOkcpAUkCrdpqQdtP2xJLWGpA1RpJ8gquwyumXYPmyLpVmxMqZKtzfZB1ifdCySv8D7kI8lERlnVQm/d7H923KbsuQmWqcpbYitY1If3Rdp6h9JnmLtJ/3EYdMZtgFbj5XSFxjbfZ+WGZaPbRu2Tdkxj9XJEvXMU9HUOfo5ZurnR8v3Galyng4yatSoAYcdZF2XlouIiIiIRJoibCvO00FGcnIyxo4diy5dupRZvmHDBgwfPryKWyUiIiIiIhXh6SDjjDPOAADjA/dq1KgB9win2QKBAAK/eWBOMFgMn0/pvCIiIiIiXvD0XNA111yDuLg4Y3mDBg0wbtw4WkdmZiZSU1NDXj9s/0+kmyoiIiIix4kgHM9e1YWnP/cPGzaMltevX/+Ig4yMjAykp6eHvPf78x6tcNtERERERMTOMX9Nkd/vh98fmu6kS6VERERExFZJNYqS9Yrn38bz8/OxatUq1KpVC23btg0pKygowOuvv47BgweHVadTWGwuM0XLATxStoDUWWyOnnOKyfxsouBILCONlmOLHSTReSyO0iFxiJFOrCMxqL78QmOZf795urg95umC0ebpYnaRaOUicz9h3OKyp6Nxjix5zVAfAPiy8sxl2VV8UDV1FBY/WlRkNSuH9WUSH+nEmA+TJfVSzNOVsCjRssuc2BjzNKQdLNI0KqfAXCdrI1nPvt1Z5raQfcAtLrtO60jWKBLPG7DrJxSLlTUsGwD4ohMNBXb7m1O7ptV07rYd5jrpNiAfIOwzySKOlsbbkuOaLbZ/u/lkuhqp4c+MfNaWNDBvUxq9z4qKyGc0iStnn39W0dBKCT1ueXpPxvr169GmTRv06NEDp556KtLS0rB9+/bS8qysLD2MT0RERESqVND1efaqLjxdktGjR6Ndu3bYtWsX1q1bh+TkZHTr1g1btmzxslkiIiIiIlIBng4yPv30U2RmZqJOnTpo1aoV3nnnHfTp0wfdu3fHDz/84GXTRERERETEkqeDjPz8fERH///1kI7jYMaMGbj00kuRlpaG9evXe9g6ERERETkeBV3Hs1d14emN361bt8bKlSvRpk2bkPeffPJJAMBll13mRbNERERERKQCPD2TccUVV+DVV18ts+zJJ5/EoEGDjvjEbxERERGRSNLD+CrOcavht/i+J482F5LIQxZPyGR1qGssS1290zxhQcBcFkOiLE3YsjE0Hu8o6eyW0cMuiSekQ2xapzn6z3eQxIWyZTBhsZLxceYyEqvsJvjNZZbLTZH17Ms17AOs35HYVSfXHM9L928WHVtIolD9seYym0Mri+iMtjzxTJaNbu9YMr8oEil9INc8XY6hLMkQ8Qrw7caiVW3Xl2VEsmtaNgBOHNlXTfUdzDHXl2qOTmb9zmX7BzvW+EhUcHKSeTobLCLVR45BltuNYp/RifHmMlOfZeuYRWxbRh3Tfcd2PbPIe4t2vL9lavj1VZFrP+MPjK5ML3d51rN5R1L1yckSEREREZGjgucP4xMREREROZpUpxuwvaIzGSIiIiIiElE6kyEiIiIicojq9ORtrxyVa7Bnz57YvHmz180QERERERELnp7JmD9/fpnvf/zxx3j33XfRtGlTAHpehoiIiIhUHd2TUXGeDjL69esHx3HKfBbGbbfdBuCXp4CXsNi3sthGspJYRpSYoxKj8+3iVWnkrCmakcVi2kbYMpVR5zHOKWIxhJbryybulE3Dykj7ncrY3KROp9jQz9l+Uxmp2zQm1XJ+NvGRLDrScp2wmFp6LpuVsXVC4k6RkBD+NCDH12Al7G+Wsd2OTew4w+K3WRw2WTaH1VkZEaqRro9FFtvGrbO+wKKtyXeCiB/PWfq59fGJFZLPuMo4Vkq15ek3yD59+qBv377YsWMHgsFg6SsqKgpff/01gsFg+AMMERERERHxlKeDjPfffx/nn38+OnXqhHfffdeqjkAggOzs7JBXMEgeaCUiIiIiQuiJ3xXn+bUwd911F+bPn4/Ro0dj+PDhyMsjTyQtQ2ZmJlJTU0NeG/ctr6TWioiIiIjIkXg+yACADh06YOXKlXAcBx06dCjzHg2TjIwMZGVlhbxa1jqrElsrIiIiItVZ0HU8e1UXR81zMuLj4zFz5kzMnz8fS5YsQZ06dco1nd/vh9/vD3nP5ztqFktERERE5Lhz1H0bv+yyyxRZKyIiIiJyDPN8kJGfn49Vq1ahVq1aaNu2bUhZQUEBXn/9dQwePDi8SvPyzWXsUqw4v7msIGAs+umCusay1msKzHWyyD2/ITqPtZ+0kaIRfrFkOssIPxoxbHEFX7H5Rn+HRSWyyEnbOMTiCKeh2a5HFmNJ2ujGmddJMJH0BUu+nLL7bDDBPC/fwfDu2/r/Ccn6ClpuN7YNAmR/NEVj0lhisr+ROE0n3+4Y5CQa4mYBoLDQXMaWwdT3giQ+le1TbP2zeFvb/ZvFljKmiF7SJ1kkrlsz2TwvlvKaS/YdFk3K+rLNMYrNi2Ex87axuGyTklhfNzmRTFd2W5x8y/2GxlBbXvXOYpAZ20cEHIOq02VLXvH0noz169ejTZs26NGjB0499VSkpaVh+/btpeVZWVkYOnSohy0UEREREZFweTrIGD16NNq1a4ddu3Zh3bp1SE5ORrdu3bBlyxYvmyUiIiIixzHd+F1xng4yPv30U2RmZqJOnTpo1aoV3nnnHfTp0wfdu3fHDz/84GXTRERERETEkqeDjPz8fERH//9tIY7jYMaMGbj00kuRlpaG9evXe9g6ERERETke6UxGxXl643fr1q2xcuVKtGnTJuT9J598EgCUMiUiIiIicgzy9EzGFVdcgVdffbXMsieffBKDBg0K68F8IiIiIiLiPcetht/i+za+zW7C+DhzGYmHLTy5obEsdv12YxmiSWyjKSrRNiKORmOSsoR4u+mqEovoZEpINCaLBYwhJwCLzHG6dNuZ6iTr2CURo87BXPO8GDa/Gkl2dZIYSGdfdtjzcliErW2fZNGk5LgQTDSX+Q4cNNdpiMt2o839zikoMtdHYpxRSKZjUsj23p9lLosm+4fpeMLit1nUJjuG2sZJs9hutp5ZBLqpLSR21fWb2xFMJPMihy5fgbn9wVjzdovKIvsci7eNdKQp26asL7BodLYN4s3bwI0jZSxq1yAqm3yO2cbbsuMaq5Ptc7aRuQbvr58c0foiqe/Hd3g27/d7TPNs3pHk6ZkMERERERGpfjx/GJ+IiIiIyNGkOt2A7RWdyRARERERkYjydJDx1ltvIS+PXOspIiIiIiLHHE8HGQMGDEDDhg3xxz/+EZ999pmXTRERERERAaDnZESC55dL3X333Vi5ciW6du2Kdu3aYerUqdi7d6/XzRIREREREUueRtj6fD7s2LED9erVw6pVq/C3v/0Nr776KvLz83HZZZdh2LBhuOCCC8Kut29TEjvGFpdFs5FIN7d2qrHMFNF5RCyOz4TF1TFsnUSxdVIJXYfECRqx5WbtZ0gsoEtifZ0sEltKI2xjyn6ftd8ySteNNcwLAEj0oksiLinSv5wiQyQlWVc0ypXFErP1zyJUTdvmSIIWbWHxryw+lWERtiyqkkWysthoNp1pX62E/ZTGnbLp2P7BBArNZWydmJDjWjA10TwdS2vNI21k64v1PdZnTWy/drB22O6nllwSX+0a+pdTaG6/w/ZT1l9Zme1+RZiWDYC575FpFnw1oWINqkTnL0n3bN6Lz3vcs3lHkudnMn7VsWNHTJ8+Hdu3b8ezzz6L3bt348ILL0SLFi28bpqIiIiIiITB0whbp4zRbVxcHK6//npcf/31+P777zF79mxaRyAQQOA3DwMKusXwOUrnFREREZHwVad7I7zi6ZmMI12p1apVK0ycOJH+TWZmJlJTU0NeG7NXRrKZIiIiIiISBk8HGZs2bULdunUrVEdGRgaysrJCXi1TOkWohSIiIiIiEi5Prylq1qxZhevw+/3w+0NvqtOlUiIiIiJiy9XlUhXm+bfx/Px8rFq1CrVq1ULbtm1DygoKCvD6669j8ODBVdMYf6y5LC/fWHTg1JrGsppLDpjrjCIJUqbEDnaJmW06BauTJXaw82AuSa1xyISmNAzWxmKSDGSbMENSORxWJ0t8ssFSrkgSlMO6AlmXLsx1liSS/YNtH7IMMbvKTuMKJpmTW5w8sr1p4pldG2miTby5nci1SBuifZIcL9hyswQplqjHEpHYscZnkYzHVEZqHp0f2b/pcYj0kyKLiwZIQpzjJpinI82nKWrsuEzT11j/MvTnykgmLLL8/GNoHyL7vmFVsmMvnRdNgLTc39jnGEsEZMmXxi/lnoWYisc8vVxq/fr1aNOmDXr06IFTTz0VaWlp2L59e2l5VlYWhg4d6mELRUREROR4E4Tj2au68HSQMXr0aLRr1w67du3CunXrkJycjG7dumHLli1eNktERERERCrA00HGp59+iszMTNSpUwetWrXCO++8gz59+qB79+744YcfvGyaiIiIiIhY8nSQkZ+fj+hD7j1wHAczZszApZdeirS0NKxfv97D1omIiIjI8SjoOp69qgtPb/xu3bo1Vq5ciTZt2oS8/+STTwIALrvsMi+aJSIiIiIiFeDpmYwrrrgCr776apllTz75JAYNGnTEB/aJiIiIiESS6zqevaoLx62G3+L7njzaXMjiO2PNca00tjS/wFxncqK5zgCJuLSI3GPtZxwScenGkLi6EhJzx2IzCdcUqUmGww5phxtLTtaxvhBtnqEv2xxnTGMsWURkvqFOFjHqJxGjpvoAHk3K4hBtoxIZU0wniRAG228KLfcpFtVM6nTr1zbPrtDcF0z93CFR2XS77c8ylyWZj0E0SjRAYkstY16DuXllvu+LjyfVmY8lDuuTsSRymWFRqIRLtp2TYFg+FlHN+kKThuVtVujs9u43z4/FEpPjmnHZALtjBvvssNxPKRpfTfp5HXN0vRHZ34JJZP8mnwPss8olx9FgNOl7MWQ6Ep1u873lw/fvCXuaqtJtEfkuWcmWXjDZs3lHkufPyRAREREROZpUp3sjvOLp5VIiIiIiIlL9aJAhIiIiIiIRpculREREREQOUZ1uwPaKzmSIiIiIiEhEHRVnMlasWIFly5Zhx44dAIAGDRqga9euOPPMMz1umYiIiIgcb3Tjd8V5OsjYtWsXfv/732Pp0qU44YQTUL9+fQDAzp07cdddd6Fbt2546623UK9evbDqdeNIzB2NLSWrg8S2OSXm+blx5jodFhloE2FL5kUVkwhbFgHLogZJnYwpjo/F1AZjzesqGG+OsXTJeTwWCxibF/moRNcQm0kjOlkZ2zasn8eYy1wWXciQpjim/dE2AplFCBMOWSc02pNtUxb/bJqORGayPkkjRi2jrZFXdtzskbimWGKY16XLoklJXK4bReJtWZ0E3d7s84NFqFpEubokQtiXT+KFWZ0kFpe13yXRqz56jIrsxRLseEi3G+MjxzVSp1NgsQ1KyHEtgUXYhj8rgH/GWWPHPH0nl9/w9HKpW265BSUlJfj222/x448/4rPPPsNnn32GH3/8Ed9++y2CwSBuvfVWL5soIiIiIiJh8vRMxgcffICPP/4Yv/vd7w4r+93vfocnnngC5557Lq0jEAgg8JtffILBYvh8R8WVYCIiIiJyjKl+j6quep6eyfD7/cjOzjaWHzx4EH72VGMAmZmZSE1NDXn9sPvTSDdVRERERETKydNBxsCBAzFkyBDMmzcvZLCRnZ2NefPmYejQoRg0aBCtIyMjA1lZWSGvE+ueXdlNFxEREZFqKgjHs1d14ek1RY8//jiCwSCuvvpqFBcXIzb2l5t0CwsLER0djZtuugmPPfYYrcPv9x92tkOXSomIiIiIeMfTb+N+vx8zZszA5MmTsWrVqpAI244dOyIlJcXL5omIiIjIcUgP46s4z3/y//bbb7F8+XJ07doV5513Hr777jtMmzYNL730Eq677jr07Nkz/Eot79ZxWAwkiWQtrp9qLIvel2ueoSG2FABguheFLJsvh0TqsShBUqdTROIjSZSdwyJIWTyvYToWYWuMQQXfbq7PvE5Y/Ghx7SRjWfSOA8Yyup5NcZvx8eb6ou2iIwub1DSWudHmbROVaxcPy5TEl30Iit1L4lOLzLHEtN+R7U23DYvozCP7HOvnpthMtk+xWGh27xqLzWRizeuZcch0jiGW1Ykj7beMM6bbm6CRvyzCNt8cD2sT5cpiY+Gy4ys5rpE4YFss/tm43LZ31JJtah3VTATzC8zzs4glBol39uWa58X6D/sc87HYcbYN6LFLz3CW8vN0kLFgwQJcfvnlSEpKQl5eHubNm4fBgwejffv2CAaD6N27NxYuXGg30BAREREREU94OiR98MEHMWrUKOzduxezZ8/GNddcg2HDhmHRokVYvHgxRo0ahUmTJnnZRBERERE5zgRdx7NXdeHpIGPt2rW44YYbAABXXXUVDh48iP79+5eWX3vttfjqq688ap2IiIiIiNjw/J4M53/X/vl8PsTFxSE19f/vb0hOTkZWVpZXTRMRERGR45Aexldxnp7JaN68OTZs2FD672XLluGEE04o/feWLVvQsGFDL5omIiIiIiKWwjqTceDAAcybNw//+c9/sHnzZuTl5aFu3bo4/fTT0adPH5x9dngPwRsxYgRKDkm7aNeuXUj5+++/r5u+RURERESOMeUaZGzbtg33338/Xn75ZTRq1AhnnnkmOnTogPj4eOzbtw9LlizBY489hmbNmmHcuHEYOHBguWZ+88030/KHH364XPV4rjJOqek8XeWzPI/nsulY7KQptpTOjMV3WkQoAnBJO1gZfQgpizxkfdlmG9iefz2a9inWT6qSTZ+sDEfTtmFtOVq2G0OPGWyySli2o2W72sa1Hq/Y+jqO1qWek1Fx5RpknH766RgyZAhWrVqFtm3blvk3+fn5ePvttzF16lT89NNPuPvuuyPaUBEREREROTaUa5DxzTffoHbt2vRv4uPjMWjQIAwaNAh79+6NSONERERERKqazmRUXLkuPDh0gJGbS55gXcbfi4iIiIjI8SXsq5vr16+PG2+8EZ988klltEdERERERI5xYQ8y/v73v2Pfvn3o2bMnTj75ZEyaNAnbtm2zboDruti0aROKi4sBAIWFhXjttdfw4osvYs+ePdb1ioiIiIjY0BO/Ky7sQUa/fv3w9ttvY+vWrbj55pvxyiuvoFmzZrjkkkswd+7c0sFCeaxbtw4tWrRAq1at0KZNG2zatAlnn302brrpJowYMQJt2rQJeY6GiIiIiIgc/ayf+F23bl2kp6cjPT0df/3rXzFq1Ci89957qFOnDm6++WaMGTMGCQkJtI7Ro0ejffv2eOeddzBr1ixcfPHFOPnkk7Fs2TIEg0EMGDAADz74IF566aWw2uaUVEJsXok5FtApJpGBZDrKFAVH4xUt58WQYahjG/XI6iwxFLD1yNpBtg1vv7nIibWLjqV8hpUSTXZRFhfo2Obz2k3GIw8tqiPL5rDlNq1HgMe1sv4aReqMdESnbRvZcrM2smNlZawv07aLIvtUiemgQOo7ErZsbF06LL7TPJ1jWj6yHh0aJ23Zz9l+xbYBi8W12d62LJeNltHtbVmnzTSWn6cO6ZP08GT7dYHtA0dLZHGEVLPF8YT1IGPnzp144YUX8Pzzz2Pz5s3o378/brrpJvz888+YPHkyli9fjoULF9I6Pv30UyxcuBCnnnoqJkyYgGnTpuGZZ55BTEwMAGDMmDEYNGiQbRNFRERERMQDYQ8y5s6di9mzZ+ODDz5A27Ztccstt+C6665DjRo1Sv/m7LPPRps2bY5YV05ODmrVqgUASExMRGJiIho2bFha3rRpU+zcuZPWEQgEEAgEQt4LBovh81mPn0RERETkOKYI24oL+5qKoUOHonHjxli6dCm++OILjBw5MmSAAQCNGjXC2LFjj1hXo0aNsGXLltJ/P/LII6hXr17pv3fv3o2aNWvSOjIzM5Gamhry2rh3WXgLJSIiIiIiERPWIKO4uBiZmZkYP348OnfubPy7+Ph4jBs37oj19erVC999913pv0eMGIHk5OTSfy9cuBBnnHEGrSMjIwNZWVkhr5a1u5ZjaUREREREpDKEdU1RdHQ07r77blx88cURmfnMmTNp+cCBAzFkyBD6N36/H36/P+Q9XSolIiIiIrZ0uVTFhf1t/Mwzz8SaNWvQrFmziDTg22+/xfLly9G1a1e0bt0a3333HaZNm4ZAIIDrrrsOPXv2DL/SYpJGwuICokm6BkmFiNq62zy7WinmKlk7LdrhxseapyPJUyyNy40mJ7vIdI5ldIUxVYhtGxasE0fWCekLbox5flH780idZLmDpKGm6OecXPM0frJsBQFjkf+n/ebpqjIpBkD0fsM2YPtGoJCUmZebio+zmsxNJNMVkWUwrGaHbDe6bQrJOmH9hKVEsbawfYelQRn6uZtfYJ6GoIlILJmNpWrZHJcBu+Qjdnwl7XcT/cYyxhcfb66T7Dt0m0Y6gsc2tdA2XZHMz4kl+w77TDKJM2+3YI1EYxlL22Of0W40mY4cT1zSl4OxlsmFclwKe5Bxyy234E9/+hN+/vlndOzYEYmJoTvGaaedVu66FixYgMsvvxxJSUnIy8vDvHnzMHjwYLRv3x7BYBC9e/fGwoUL7QYaIiIiIiIWlGBbcWEPMq6++moAwO233176nuM4cF0XjuOghP3i8RsPPvggRo0ahQkTJmDOnDm45pprMGLECEycOBHAL/dbTJo0SYMMEREREZFjSNiDjE2bNkVs5mvXrsWLL74IALjqqqtw/fXXo3///qXl1157LWbPnh2x+YmIiIiISOULe5ARqXsxfvXr03t9Ph/i4uKQmppaWpacnIysrKyIzk9EREREhNGN3xVndQfPSy+9hG7duqFRo0bYvHkzAGDq1Kn4xz/+EVY9zZs3x4YNG0r/vWzZMpxwwgml/96yZUvIw/lEREREROToF/YgY8aMGUhPT8dFF12EAwcOlN6DUaNGDUydOjWsukaMGBFyD0e7du0QfUiixvvvv6/7MURERESkarkevqoJx3XDy59r27YtHn74YfTr1w/Jycn48ssvceKJJ+Lrr7/Gueeeiz179lRWW8utb4v0yFfKIhtTzNFzTjaJIGVxfCx20sQUg3okrAtUQmwp5VicXAuyqFCLmEGAxoUGU8wxkL7d5PI+FnFpaqc/xjwN2zYkhrOERCUyjm1EJGGKX/QVkL5MomGdwiLzdGx9sQhYNh3rr2x728zLNiqUrROGRuaSOlkc8MGc8KexxY6vNvsiwI81ufnmsqQEc5kJifV169YMvz4AzgHD+gf45wfre2zb2RzPGRYRzuKMbbFtUDvVWGbiFJD9xvb4yo4ZPpbvbnk8YccFC+9/mxnR+iLp5Dcf8mze6/vf59m8I8nqxu/TTz/9sPf9fj9yc8kXahERERGRY4Duyai4sIekLVq0wBdffHHY+wsWLECbNm0i0SYRERERETmGhX0mIz09HbfeeisKCgrgui5WrFiBV199FZmZmXjuuecqo40iIiIiInIMCXuQ8Yc//AHx8fG49957kZeXh2uuuQaNGjXCtGnTSh/UJyIiIiJyrLK9bUX+X9iDDOCXh+Rde+21yMvLQ05ODurVqxfpdomIiIiIyDHKapDxq4SEBCQkWKRm/E8gEIDP50NMzC8JOhs3bsSsWbOwZcsWNGvWDDfddBNatGhRkSaKiIiIiIRFN35XXNiDjBYtWpQ+pbssP/zwQ7nr6tOnD0aOHIn+/ftj6dKlOP/88/G73/0Obdq0wXvvvYe//OUv+Ne//oWuXbuG18gSEgXHzn/FkLjQEnO8nxtjjs6jXZRFwUUb6mSxjLZYnaZ2ADxOkG0DFh9pKqJxoKSNpP1uDOn+hmhVAHCKybLFkj7ElBiiMQsC5mlYZCOJo4w6aF42N9a8TkxxsxURlW2OiDQqIHGzLIbTJlIWoHHA8JNtwGJeow3rOcjaT+ZVVAnRmOxYyepkfdZ0jGXr2DbWlx27GNtI7Di/ucwU5cr6JKvPNk2axc2ybUqjoclxwbQubT/HTPsNYB/hztpC1hf7HHBNfZatYxY3S/YBl/Vzdsy2vBaIfd+p8sh7OeqFPci48847Q/5dVFSENWvWYMGCBRg1alRYda1Zswbt27cHAIwdOxa33HILHn/88dLy++67D6NGjcInn3wSbjNFRERERMQjYQ8y7rjjjjLff+qpp7By5cqw6iopKSl94vd3332HadOmhZTfcMMNYT9FXERERESkQnS5VIVF7PqHvn374q233gprmi5duuCdd94BALRs2RJffvllSPkXX3yBWrVq0ToCgQCys7NDXkHX8rSpiIiIiMgx5KmnnkLz5s0RFxeHLl26YMWKFfTv33jjDbRu3RpxcXE49dRT8d577x32N99++y0uu+wypKamIjExEZ07d8aWLVvCaleFbvw+1JtvvnnEAcFvTZgwAX379kVubi4GDRqEP/3pT9iwYQPatGmDdevW4YknnkBGRgatIzMzE+PHjw95r2VKF5yUelbYyyAiIiIicqxE2L722mtIT0/HzJkz0aVLF0ydOhV9+vTBunXrykx//fTTTzFo0CBkZmbikksuwSuvvIJ+/fph9erVaNeuHYBfgpjOOecc3HTTTRg/fjxSUlKwdu1axMWR+7rK4LhueKvx9NNPD7nx23Vd7NixA7t378b06dPxxz/+MawGLFu2DOnp6fjss89C3m/UqBFGjRplvDzrV4FAAIFA6I2GA075M3yOYfxke+M3uaksWDfVWObbm22uk90kZbrpj92kVkhuiGXYTdrsRrvKuPHbdGMlm8byxnXbG78ZJ9fiRmbAfOM3Y3njN7uZsapv/PblkxtKTar6xm/Wl/2x5jKrG78tbzqujBu/2Y2obD2zfmmjqm/8Zmy3gemzhfVJso7dRLtUR6eIbDfbG7/ZZ0RV3vjN9h2GtYUFeZAy043fToAcu47TG78/WPmAVTuqwomvPuzZvH8Y9Ody/22XLl3QuXNnPPnkkwCAYDCIpk2b4rbbbsOYMWMO+/uBAwciNzcX7777bul7Z511Fjp06ICZM2cCAK6++mrExMTgpZdeqtByhH0mo1+/fiH/9vl8qFu3Ls4991y0bt067AZ07doVy5Ytw+7du/HDDz8gGAyiYcOGaN68ebmm9/v98PtDv5AbBxgiIiIiIkfi4ZmMsn5AL+v7bmFhIVatWhVy1Y/P50OvXr2wbNmyMuv+9cf9Q/Xp0wdvv/02gF8GKf/85z9xzz33oE+fPlizZg1atGiBjIyMw8YARxL2t/Fx48aFOwn17bffYvny5Tj77LPRpUsXfPfdd5g8eTICgQCuu+469OzZM/xK2S9KbPTORuHkLEFu82RjWfLPu8x1+klEYcDiF958y1/RbdmeSyyxmI79GMsiM8mv0PQ3F/YLI4uBZL96M6YzV+TXLfYLlsN+TSZ9i/3SGaxl7udUFFmX+3PLnlfNROMkPvZrbDTZpxjWh9j2ZvsAOztliq9mxwR2ppK1MYucTWVq1TCXHSB1kuhV13Dmh/7Ca/urN/scYNi6ZNiZURNynHHjzWfJXL9lVDb5ZTsYb66TRk2z+GHXMD/bzw52RoWdVbSNViXH2GAC2T6G47aPRaMfzDe3g1w14JSwY725iJ6NJPucw6L3WZyxhKWsWwHGjRuHBx54IOS9PXv2oKSkBPXr1w95v379+vjuu+/KrHvHjh1l/v2OHTsAALt27UJOTg4mTZqECRMmYPLkyViwYAGuvPJKLFmyBGlpaeVejrCPilu3bsVbb72F9evXIzY2Fr/73e9w1VVXoWbNmuFWhQULFuDyyy9HUlIS8vLyMG/ePAwePBjt27dHMBhE7969sXDhQruBhoiIiIjIMSYjI+Owsw2/PYtRWYL/u3Ty8ssvx1133QUA6NChAz799FPMnDkzrEFGWMPO6dOno2XLlrjzzjvx97//HbNmzcKIESPQpEkTvPrqqwB+uUdjzZo15arvwQcfxKhRo7B3717Mnj0b11xzDYYNG4ZFixZh8eLFGDVqFCZNmhROE0VEREREKsR1Hc9efr8fKSkpIa+yBhl16tRBVFQUdu7cGfL+zp070aBBgzKXq0GDBvTv69Spg+joaLRt2zbkb9q0aRN2ulS5Bxn//Oc/cfvtt2PkyJHYunUrDhw4gAMHDmDr1q0YPnw4hgwZgk8++QTXXnttaSztkaxduxY33HADAOCqq67CwYMH0b9//9Lya6+9Fl999VVYCyQiIiIiUt3FxsaiY8eOWLx4cel7wWAQixcvRteuXcucpmvXriF/DwCLFi0q/fvY2Fh07twZ69atC/mb9evXo1mzZmG1r9yXSz366KMYM2YMJkyYEPJ+w4YN8fjjjyMhIQEXXHABGjRogMzMzHI34NekKp/Ph7i4OKSm/n9SU3JyMrKysspdl4iIiIhIhR0jEbbp6ekYMmQIOnXqhDPPPBNTp05Fbm4uhg4dCgAYPHgwGjduXPrd/I477kBaWhqmTJmCiy++GHPmzMHKlSvxzDPPlNY5atQoDBw4ED169MB5552HBQsW4J133sGHH34YVtvKfSZj9erVuP76643l119/PQKBAD766KNyj3SaN2+ODRs2lP572bJlOOGEE0r/vWXLFjRs2LC8TRQREREROW4MHDgQjz32GO6//3506NABX3zxBRYsWFB6c/eWLVuwffv20r8/++yz8corr+CZZ55B+/bt8eabb+Ltt98ufUYGAFxxxRWYOXMmHnnkEZx66ql47rnn8NZbb+Gcc84Jq23lPpNRUlKCGPIciZiYGMTHx4cMEo5kxIgRKDkkgeXQBQSA999/Xzd9i4iIiIgYjBw5EiNHjiyzrKyzDwMGDMCAAQNonTfeeCNuvPHGCrWr3IOMU045Bf/4xz9K7zT/rbfffhunnHJKWDO/+eabafnDD3v3IJTDkMi9rBbmCLnkT0idNg8Hs32gmK2j5ZGXtjGWVY1FJbJtZxGx6FTGtqHtj/zsjFgSsG0MNWO7bSqjLTZs52W9viwfVmmKO6XtqOJ+zspsH2xos3+TqO8wn6H7/3UWmeNmHfaQNRbNfbR8RlQGy+U2HpvZumIPE6yMzz+2bKyf0+1t+UDEo5TrVvH3rWqo3IOMW2+9FSNGjIDf78cf//hHRP/vyZvFxcV4+umnce+992L69OmV1lARERERETk2lHuQMWTIEPz3v//FyJEjkZGRgZYtW8J1Xfzwww/IycnB7bffXpoUJSIiIiJyzKrGJ+mqSlgP43vsscfQv39/vPrqq6U3bPfo0QODBg3CWWedVSkNFBERERGRY0vYT/w+66yzNKAQERERkWpM92RUVLnu3gv3CX9bt261aoyIiIiIiBz7yjXI6Ny5M4YPH47PP//c+DdZWVl49tln0a5dO7z11lthNeLf//43HnzwQYwYMQK33norpkyZEvL8DBEREREROXaU63Kpb775BhMnTsQFF1yAuLg4dOzYEY0aNUJcXBz279+Pb775BmvXrsUZZ5yBRx55BBdddFG5Zr5r1y5ceumlWLlyJXw+H4LBIE4//XTMnTsXo0ePRnp6Oh555JHwl4pFrNmWkQjCJu/ttpqOR8hZ3HFUGVGCtuvLlik6rzKiSRka5cqiHotIGYmkjCl7V3QChaQdpI1R5PeD/GJzmWuejsVfUmxdFpfdFsfw/hGx5WbYvsjWcz7ZPmzb2ew7BQFzWRTpk4ztPuz3W7WlsFFqme/H7sg218fictm2cck2dcx1utFsXZrLSlLjjWXR+3PLnhdZtmCyeR37cklfYB8rCbHGsmIyv2iyD/tyCswztDlusz5ZQo5BdLvZYevLl28+1rtRZS+3U2wZgWx7XLOtkzwT7biiG78rrFw9t3bt2nj88cexfft2PPnkkzjppJOwZ8+e0rMN1157LVatWoVly5aVe4ABALfffjsaNWqE/fv3IycnB7fccgtOOeUUbN++HQsXLsSsWbMwbdo0uyUTERERERFPhHXjd3x8PPr374/+/ftHZObvv/8+Pv30U6SkpAAAJk2ahJo1a+Kvf/0revbsialTp2LChAm44447jHUEAgEEAqG/6gTdEvicyP+qISIiIiLHAZ3JqLCqfIbvYfx+P5xDTqf6fD6UlJSg+H+XSpx99tn48ccfaR2ZmZlITU0NeW3MXV2ZzRYREREREcLTQcY555yD+++/H7m5uSgqKsKf//xnnHjiiahVqxYAYPfu3ahZsyatIyMjA1lZWSGvlolnVEXzRURERESkDGE/JyOSHnvsMfTu3Rs1atSA4zhITEzEG2+8UVr+7bffHvEp4n6/H/7f3ISoS6VERERExJqr52RUlKeDjBNPPBFfffUVli5dikAggLPOOgt16tQpLT/SAENERERERI4+ng4yAGDz5s34+eef0bVrV9SpUwffffcdpk2bhkAggOuuuw49e/YMu07XMv7SKSKRdCSOcnuvusayhi/tMs+PxcTFGspItK17MMdcH4sSJJGBjk2UbkWYYvVIG90isr1Z5KElh61Ltk1ZNGO0ablJfSR21Y0z79oOjYhksZ92fYiWmSKLGbJ/uwkkWpVwSPyla9o2OMIxg/Q9p9iwLllcK1uPhghk4Aj7B+HQiG2yX0WR2M+AYToaGW0uomg8L9l3Yu32HV+heT0HE8vul6xvMab6jsQxrX8AbrTd/s1iXo2RrWy/D5rbwX5XdmPIFQy2n38FZJvGm4/NriHGOSorz9wOFnnNsGMGXc+WEfosNto2av4oVRkp/ccbq0HGhg0bsGTJEuzatQvB33TU+++/v9z1LFiwAJdffjmSkpKQl5eHefPmYfDgwWjfvj2CwSB69+6NhQsXWg00RERERETEG2EPMp599lmMGDECderUQYMGDUJ+1XUcJ6xBxoMPPohRo0ZhwoQJmDNnDq655hqMGDECEydOBPDLTd2TJk3SIENEREREqo7OZFRY2IOMCRMmYOLEiRg9enSFZ7527Vq8+OKLAICrrroK119/fcgzOK699lrMnj27wvMREREREZGqE/YFofv378eAAQMi1oBfz4T4fD7ExcUhNTW1tCw5ORlZWVkRm5eIiIiIiFS+sAcZAwYMwMKFCyMy8+bNm2PDhg2l/162bBlOOOGE0n9v2bIFDRs2jMi8RERERETKxXW8e1UTYV8u1apVK9x3331Yvnw5Tj31VMT8Jk3n9ttvL3ddI0aMQMkhCSzt2rULKX///fft7sewTEZw8wvM0xnSIgCg4Uf7ytOqwxUEzGWmlAbbZAcWk0CmcwsrIfGCMYV5sCQMNi9WZpmE4dapYa5yX7ZVnaaEFock1vj25xrLSuomG8voU2TIOvHlWvRXgCeD1Uopu7p80u/i44xFVkk3ABzHrg+5efnmOmPNbXELy06lcqJJSpRhGgBwSdqQjxy76PGEIctWXCPBWFZYs+zpfEXm1KOg37xOfAGyfwTM64t+DpCUIpd0k8Ka5n7p31N2P3HIchfWNq/HuG+3mRvCkqDqmx9u6xSZ94+ChonGMv8e8+dm0F/2enZsUuUARJF9oCTZvP5ZSiLbBoxv1wFzoWn5SIJUMN98LKlqDjvmkW3HppPjU9iDjGeeeQZJSUn46KOP8NFHH4WUOY4T1iDj5ptvpuUPP/xwuM0TEREREakQRzd+V1jYg4xNmzZVRjtERERERKSaqNC5Ldd14eppJSIiIiIicgirQcaLL76IU089FfHx8YiPj8dpp52Gl156KdJtExERERGpeq6Hr2oi7MulHn/8cdx3330YOXIkunXrBgD45JNPcPPNN2PPnj246667wqovGAzCV8bNQsFgED///HNI2pSIiIiIiBz9wj6T8de//hUzZszA5MmTcdlll+Gyyy7DI488gunTp+OJJ54odz3Z2dm46qqrkJiYiPr16+P+++8PSZravXs3WrRoEW7zREREREQqRhG2FRb2mYzt27fj7LPPPuz9s88+G9u3by93Pffddx++/PJLvPTSSzhw4AAmTJiA1atXY+7cuYj9Xzyi7f0eTkzYi/ULGvVIxmN7DpjL4szRkiBxlYiNMZcZOCTak0bAMmwbVOX9OCwilW03Np1l3J7LVqXFdgMAX17Z0YZOAYnhJA2J3meOtwWJgaTrq8SyDxFOsSE+stgcTcr6XVQ2iaE2zQsAokkfYtG3bJ8jnChDBCzpkw7pW46h/wDgy8ZYxl5H7zlorjK67MhiFo/MYpzpcc1yuX0FZF2y7VNC1pepD7FDEPueEWN3nAn6zdO5UWTZSLytU0BihG0/d4wVmtsYlUMiti0/x0pS4s1NIevLGJHsmtc/ra8yWH422sYPy/Ep7F7dqlUrvP7664e9/9prr+Gkk04qdz1vv/02nn76afTv3x9/+MMfsHLlSuzevRuXXnopAoFfDhaO5XMMRERERESs6Z6MCgv7J//x48dj4MCB+Pjjj0vvyVi6dCkWL15c5uDDZPfu3WjWrFnpv+vUqYN//etf6NOnDy666CI899xz4TZNRERERESOAmGfyfj973+Pzz77DHXq1MHbb7+Nt99+G3Xq1MGKFStwxRVXlLueE044Ad9++23Ie8nJyVi4cCHy8/PLXVcgEEB2dnbIK+jaPcFTREREREQqzurmhY4dO+Lvf/97hWbcu3dvzJ49GxdddFHI+0lJSfjggw9wwQUXlKuezMxMjB8/PuS9lgkdcVJS5wq1T0RERESOU9XosiWvlGuQkZ2djZSUlNL/Z379uyMZP348tm3bdtj7rusiOTkZixYtwurVq49YT0ZGBtLT00PeG9Dq7nK1QUREREREIq9cg4yaNWti+/btqFevHmrUqFHmDdmu68JxnJAY2iPVWbNmzcPe9/v9+PLLL9GmTRukpaUdsR6/3w+/PzTByedYpqmIiIiIiOhMRoWVa5Dx73//G7Vq1QIALFmyJCIz/u3Zh1+VlJRg0qRJqF27NoBfHv4XNhYNy7BoNhaTyrDpWKxeAYnjM7GNeWVYG9n6soyHNdbJ6mPLzcosIzrpnUwsJpVwiiymY/2cLRvrC2ybBkkbbZPgog2Rjiwu17ZvsTYWkthSFoltG3ttWs8s8to2Mtr2eMjWVxGJQY41xPMC8JEo1IhjbSTr0iHRty7rCgHz/uGY5kd2KbquLONO3RjzdD4S1cwOesZlA+yiexm2n/rJxmHxwoztZ6rpx1Y/2b/z881ltlHytsdl9jlQldH1cswr16fPoWcUWrRogaZNmx52NsN1Xfz000/lnvHUqVPRvn171KhR47B6vv32WyQmJirCVkRERETkGBT2T1wtWrQovXTqUPv27UOLFi3KfbnUww8/jGeeeQZTpkxBz549S9+PiYnB888/j7Zt24bbNBERERGRiqtGT972StgnLX+99+K3cnJyEBdX/qffjhkzBq+99hpGjBiBu+++G0Xs1LaIiIiIiBwzyn0m49d7KBzHwX333YeEhITSspKSEnz22Wfo0KFDWDPv3LkzVq1ahVtvvRWdOnXCyy+/rEukRERERMRTjm4/qbByDzLWrFkD4JczGf/9738Re8jNfbGxsWjfvj3uvjv86NikpCS88MILmDNnDnr16lXuy61EREREROToVO5Bxq+pUkOHDsW0adPK/TyM8rr66qtxzjnnYNWqVWjWrFlE6xYRERERkaoT9o3fs2fProx2AACaNGmCJk2aVLwi29jSSMdRVmQ6UzvZsrGoTdvYOTY/GjNqG51naCeLbGSxsWy5Lc+aObkkXphFD7NlKCDRjCZse+cXmMt8pJ8XF5vLKiMq0bTt2Lqi0c9kPbqW8anszrV4ch+axXHIjTNE+gJw2P7G9gHb4yE7PrF+TtoSZaqTLJtTyOKkycZh64std5F5H3DIrYPRbD0bolxZ/GtUvrkdwdQEYxnb3uw+Vl+heX1Fk7Y4AYv7KdmuaPoMAHj0MIsBZ3WyYw2ZLphs3gaO6bOFbBuHfWbSdUJWpkOWzTIG+biKsD2OFrWylGuQceWVV+L5559HSkoKrrzySvq3c+fOjUjDRERERETk2FSuQUZqamrpDdmpqamV2iARERERETm2lWuQceglUpV5uZSIiIiIiBz7wr4nIz8/H67rlkbYbt68GfPmzUPbtm3Ru3fviDdQRERERKQqKcK24sK+8+fyyy/Hiy++CAA4cOAAzjzzTEyZMgWXX345ZsyYUaHGbNq0CYsWLcLXX39doXpERERERMQ7YQ8yVq9eje7duwMA3nzzTTRo0ACbN2/Giy++iCeeeKLc9dxyyy3IyckB8MvZkf79+6NVq1bo06cP2rdvj549e5aWi4iIiIjIsSPsy6Xy8vKQnJwMAFi4cCGuvPJK+Hw+nHXWWdi8eXO563n66afxwAMPICkpCQ899BA+++wz/Otf/0KXLl2wZs0aDBkyBBMnTkRmZma4TeRY5CGLZqORrJZRcIxNJCiNo7Q878fi8dj8WFaiTQSebXxqjDkSlG5TFgOZaI4tdWwjYKMNsbJ0HZOyQx6WeRgWXchicUm0p3VksWl+hSQWk0VN07hWstxF5vk5hSQak0W5WsRXO6z9pI10u9FtYxkP6yf9i+xzwZT4smeVY16Pbqx5ezuGaNhfKmWRoOYiuu/7zes5GG9ebl9e2dHKLln/LmlHVFaesYzx+c3BLSxOl20DN9a8TpxCcswwsY1ypZ+ZlsdRwpeTby40fQ6wSNlCFr9tGTXNsOMoc1xF2Fp+rkmpsL8dt2rVCm+//TZ++uknfPDBB6X3YezatSusB/S5h3TUd955B4888gjOO+88JCQkoFu3bnj88ccVhysiIiIicgwKe5Bx//334+6770bz5s1x5plnomvXrgB+Oatx+umnh1XXr7/a7dixA6eddlpIWfv27fHTTz8dsY5AIIDs7OyQV9C1e8iaiIiIiAhcD1/VRNiXS/Xv3x/nnHMOtm/fjvbt25e+f/755+OKK64Iq6777rsPCQkJ8Pl82LZtG0455ZTSsr179yIxMfGIdWRmZmL8+PEh77VMPhMnpXQJqy0iIiIiIhIZYQ8yAKBBgwZo0KABfv75ZwBAkyZNcOaZZ4ZVR48ePbBu3ToAQNu2bQ+7n+O9994LGXSYZGRkID09PeS9AW0ywmqLiIiIiIhETtiDjGAwiAkTJmDKlCml6U/Jycn405/+hLFjx8LHbhY8xIcffljm+67rwnEcXHPNNbjhhhuOWI/f74ff7w95z+dY3tAkIiIiIlKNLlvyStiDjLFjx+Jvf/sbJk2ahG7dugEAPvnkEzzwwAMoKCjAxIkTK9Qgv9+PL7/8Em3atKlQPSIiIiIi4o2wBxkvvPACnnvuOVx22WWl75122mlo3LgxbrnllnIPMn57idOvSkpKMGnSJNSuXRsA8Pjjj4fbRB71yCLw4s3RpMgvMM+udVNjWcx35OZ1Fq9qExPH4jQZNq+EsiMnAQAlltF5LGrQJgqVbBtW5pJ4XodEDTpsPdNIU7JspkhQEonrppjvWXIOkojLPItYSQBunRp200WZl9u3c3/Y83Ky7eI7KRbP+5szpYdyU5ONZc6BbHOdsYZ9n0YIk+Mai79k+wc7HrJ9P5v0c1KnG2tYX0W5xmmcANunyFlrtk1tIsIBOPnm9ezEmWN9nQLDtmPtSDIXFdctf5JjeQVqmvu5f4+5Dzm5pH9FPIqdfOaw7U3nZ24ji/UtqW3eQK7hig4WuRy1nzwXzDbCnU1XUgkBOSyi9xikJ35XXNiDjH379qF169aHvd+6dWvs27ev3PVMnToV7du3R40aNULed10X3377LRITE3lmvIiIiIiIHJXCHmS0b98eTz755GFP937yySdD0qaO5OGHH8YzzzyDKVOmoGfPnqXvx8TE4Pnnn0fbtm3DbZqIiIiISMXpTEaFhT3IeOSRR3DxxRfjX//6V+kzMpYtW4affvoJ7733XrnrGTNmDM4//3xcd911uPTSS5GZmYkYdvmQiIiIiIgcE8K+gC4tLQ3r16/HlVdeiQMHDuDAgQO48sorsW7dOnTv3j2sujp37oxVq1Zh9+7d6NSpE77++mtdIiUiIiIicowL60zGjz/+iEWLFqGwsBBXX3012rVrV+EGJCUl4YUXXsCcOXPQq1cvlFTGzUgiIiIiIuWly6UqrNyDjCVLluCSSy5Bfn7+LxNGR2PWrFm47rrrItKQq6++Gueccw5WrVqFZs2aVawy9qwOdu6GDXDIpVwx67daTUeZ0mcqI2WCTccSO2xTQBjTtmPbNNac6oJoc/qME026P1knbpx5mzrZlukaplQh0iedPJK6E0VSdyyXmyXrsOQsem7SkOjm5OSzqewE2f7NUp3M+4CTfdCuLYWGtCG2T1kmYNHtzfZTUxsBvs8RTpHFj0hs2Vj7TcdQwC69D4CbYG5LMJYca0zHDHaciTEfS2J2kOQysmxFDcypVFEB87p0ybKxdWK1vVniGet3tldBkPXlRpm3QfSOA+Y6TctA0gKtP6NZaiHb99l3E/K5KRKOcn8juu+++3DBBRdg69at2Lt3L4YNG4Z77rknoo1p0qQJLr/8ciQmmqM5RUREREQqk+N696ouyj3I+Prrr/Hwww+jYcOGqFmzJh599FHs2rULe/furcz2iYiIiIjIMabcg4zs7GzUqVOn9N8JCQmIj49HVlZWpTRMRERERESOTWHd+P3BBx8gNTW19N/BYBCLFy/G119/XfreoU8CFxERERE55pCnwUv5hDXIGDJkyGHvDR8+vPT/HccJKx3qyy+/xKpVq3DuuefixBNPxNq1a/HUU08hGAziiiuuQJ8+fcJpnoiIiIiIHAXKPcgI2qYFGcydOxdXXXUVatSogUAggHnz5mHAgAHo1KkToqKicPHFF+PFF1/ENddcE9H5ioiIiIhQ1egGbK+E/cTvSJk4cSLGjx+PsWPHYs6cORgwYADS09Nx3333AQCmTJmCRx999NgYZFjGIVrVyeL9Ij0vgEfnRXjgSetk7aiE6EJrbPuwqEHTdA65bcq2L7A7sWxjkEsivC7ZsrmV0e/YdiPT2e47kcb6li26DSyX22adsHnRPlkJ/YRwoyyWjayPYDSLYmfHElLkN0eTlpDIXIf0hSibbWp77GIx1CCxq5Z9KEjWiRW23Yot+2vVdnPK1cOU5TcivAeV37p163DttdcCAAYOHIjc3Fz069evtPyKK67A999/71HrREREROR4pQjbivNskJGcnFwaf3vgwAEUFxeHxOHu3bsXSUlJXjVPREREREQseXa5VK9evXDrrbfitttuw2uvvYbevXsjIyMDs2fPhuM4GDVqFM4555wj1hMIBBAIhD75OOiWwOfoiZUiIiIiIl7w7EzGY489hpSUFNx8880oLCzEa6+9hk6dOqFt27Zo27Yttm3bhkmTJh2xnszMTKSmpoa8NuasqoIlEBEREZFqyfXwVU1YDTIOHDiA5557DhkZGdi3bx8AYPXq1di6dWu566hfvz4WLlyIgwcPYsGCBUhNTcVf//pXfP/99/jyyy/xzTffoGXLlkesJyMjA1lZWSGvlkkdbRZLREREREQiIOzLpb766iv06tULqamp+PHHHzFs2DDUqlULc+fOxZYtW/Diiy9WqEGtW7fGl19+iejo8jXN7/fD7/eHvKdLpURERETEVnW6AdsrYQ8y0tPTccMNN+CRRx5BcnJy6fsXXXRRWHGz6enpZb5fUlKCSZMmoXbt2gCAxx9/PNwm2kceRpHBSXGxucp6tYxlzu79dm2xiVdlD0K0jSZlgz0fi1e1zNUz1cnaX1RkV2bJcRONZW5ennk61r/KOagOmVei31jmFER+uYOpCRGv05dV9vpyU8zzcvILzRXa9rvsg+ayKNLP2XYjxwxjf46JMU9TROorJNublTFsuQ8GzGVJZNsVGpaBxRKTxaZs931/rLHIKTIfY6OzzevEOJ1jXu6ofPOCB5PijGUMixiNLjAvW1SueV2ydWKMqrX97GP7QKx5u9nGKjskBjmYHP7x0CHHBOeg+bODt5/sp7HkeEJYxTED/DuBHJfC/mbz+eef4+mnnz7s/caNG2PHjh3lrmfq1Klo3749atSoEfK+67r49ttvkZiYCEeZyyIiIiIix5ywBxl+vx/Z2dmHvb9+/XrUrVu33PU8/PDDeOaZZzBlyhT07Nmz9P2YmBg8//zzaNu2bbhNExERERGpOF0uVWFhn9u67LLL8OCDD6Lof6eiHcfBli1bMHr0aPz+978vdz1jxozBa6+9hhEjRuDuu+8urU9ERERERI5tYQ8ypkyZgpycHNSrVw/5+flIS0tDq1atkJycjIkTJ4ZVV+fOnbFq1Srs3r0bnTp1wtdff61LpERERETEW4qwrbCwL5dKTU3FokWLsHTpUnz55ZfIycnBGWecgV69elk1ICkpCS+88ALmzJmDXr16oYTdvCwiIiIiIkc96yd+d+vWDd26dYtYQ66++mqcc845WLVqFZo1axaxekVEREREwqEI24oLe5Bx++23o1WrVrj99ttD3n/yySfx/fffY+rUqdaNadKkCZo0aWI9/a9cFh1JOCQCj9VZXNMcZRe9bZd5fiz+0nTZGDnT4wZItKflZWh0KhY1aIouBACfZZyuaRLb7c0iZdn6IjF9DosgZfF+MYa+UECiQknqp0uiCx0Saer6zX0yv5FdhK1LVmXS/pwy3w/GmtsRlUfWie3llqy/sthMci+Zm19gLHP8hvhh1n4WRRtn7ltuITku0L5Mjk8sGpMcR4tqxpc9Sa55XdFtw6JvGbYvWsadsthPx9SHSDui8szbzQmQvkBiV2NJxHNJonm7+dj8ii2uPrCNmmZs2gHQPhR10LwNfKzPmvpJgBy72HGGfVaRIpccR1mccUkyiQMmXFPsta6CP26FfU/GW2+9VeYZjLPPPhtvvvlmRBolIiIiIiLHrrAHGXv37kVqauph76ekpGDPnj0RaZSIiIiIiBy7wh5ktGrVCgsWLDjs/ffffx8nnnhiRBolIiIiIiLHrrDvyUhPT8fIkSOxe/fu0ofoLV68GFOmTKnQ/RgiIiIiIkcF3fhdYWEPMm688UYEAgFMnDgRDz30EACgefPmmDFjBgYPHhzxBoqIiIiIyLEl7MulAGDEiBH4+eefsXPnTmRnZ+OHH36olAHG/v378eKLL0a8XhERERERqTzWz8kAgLp160aqHWXasmULhg4dGvYAhkbDsohUEunG4k5/7ll2LCMAtPiORc9ZRKiSZXP8luf22DohcZQ2cbNHZIrAIxzbBzhaxtRWClNULVvHLDIzh8QrkmhSp8Q8v6gCu9jJIGmnaR9wo8n6Z+uExTgz0WRfZGUJ5n3fYX0ozhBhy/Y3i30D4NHc1licbk6usSh2s2H7sPhRy2M2nY4dM0gsMUgcsC8v3zydqZ0kntchUc0lNRLN05HlZtG3LK7VYXGtLELctNy2nx0s+jZItqlltLWvwLxOgqnmSG9TPKwvQKLFD5L+w6Ka2XeCQvO2cUjUsY/0PZcdD03H+sr4rlAF9JyMigt7kLFz507cfffdWLx4MXbt2gX3N50nnCd2Z2dn0/KDBw+G2zwREREREfFY2IOMG264AVu2bMF9992Hhg0bwrF9+BWAGjVq0Old1z1i/YFAAIHfPOAm6JbA55DRtoiIiIiIic5kVFjYg4xPPvkE//nPf9ChQ4cKzzw5ORljx45Fly5dyizfsGEDhg8fTuvIzMzE+PHjQ95rmdgJJyV1rnD7REREREQkfGEPMpo2bXrYJVK2zjjjDABAWlpameU1atQ44rwyMjKQnp4e8t6Ak0ZFpH0iIiIichzSmYwKC/tOwqlTp2LMmDH48ccfKzzza665BnFxccbyBg0aYNy4cbQOv9+PlJSUkJculRIRERER8U7YZzIGDhyIvLw8tGzZEgkJCYiJCU1L2LdvX7nrGjZsGC2vX7/+EQcZIiIiIiJydAl7kFGZT/XOzc3F66+/ju+//x4NGzbEoEGDULt27fArMsVDHoGbQmIB95nj3hovIdFzLE6XJHG5B3MMDSGxpfHms0I0Lpdh65JEarokvtNhMYSm5SNxe8g3xyu6LGrTNvqWRVUybNuZ2plIIlKLyToJmCMIkWuOGHXJOokrIlGVDItyPZBV5tvRLK6VRWay6RzSX7PMSXdOjDl2Mpho3ud8JJbVTSp7uoJGycZp4n4ibczNM5bBTyJsWfAGC90g/ctJMkd7BmuUvXy+vWX3AwD8GET2RdcmUhY8rhxJ5s8I24hhk5IUc99i+z6Lfy5ommIsC8aY10nCz+YyXx6Jjc42fI6xy6DJ54NL+jmNjGZlpC+U1DD3ZZC4bze27D504OQk4zQ11piX22ERzz6yD7OYc/bxRz4HaGS8aT1HeN+oKoqwrbiwBxlDhgyJ2Mzbtm2LTz75BLVq1cJPP/2EHj16YP/+/Tj55JOxceNGPPTQQ1i+fDlatGgRsXmKiIiIiEjlshpebty4Effeey8GDRqEXbt2AQDef/99rF27Nqx6vvvuOxT/71fJjIwMNGrUCJs3b8aKFSuwefNmnHbaaRg7dqxNE0VERERE7LgevqqJsAcZH330EU499VR89tlnmDt3LnJyfjkd+uWXX1bo/olly5bhgQceQGpqKgAgKSkJ48ePxyeffGJdp4iIiIiIVL2wBxljxozBhAkTsGjRIsTG/v+1vj179sTy5cvDbsCvD9srKChAw4YNQ8oaN26M3bt3h12niIiIiIh4J+x7Mv773//ilVdeOez9evXqYc+ePWE34Pzzz0d0dDSys7Oxbt06tGvXrrRs8+bNdjd+i4iIiIhY0o3fFRf2IKNGjRrYvn37YTdjr1mzBo0bNw6rrt9eXpWUFJq88M4776B79+7hNpGnDdHUBJbWY07Q2HGWOQHohG9JEg5JrXFq1yy7gKVykOWmiTxBUqdjl6Tk2D6w0ZROwZJuWFJMrHkdw7FL3aFpNyzNitVpSgYjfZKl/9CkMbK9nWhzsk5xXXPyEUXaGW3YH4MJ5m3jKyBpNi7Zh0mZk2xOfWEJcb4sc1IX8kgSjmF/jM8jqWDkGEST3grIcYGgfYiVkSQcX47heEIScuhyx5i3Dd2mNvsiQJOPaJlpfmS7RWWZj7009YiI20b6K0ki8mWTzwF2rDetS5ouxY5P5CsLS1Gz5Msh3yXiyPwMx+0aX+41TuLkk32fIcdsh/VzhiSsuWR+dnf5SnUW9iDj6quvxujRo/HGG2/AcRwEg0EsXboUd999NwYPHhxWXUe6h+PRRx8Nt3kiIiIiIhWjMxkVFva48+GHH0br1q3RtGlT5OTkoG3btujRowfOPvts3HvvvZXRRhEREREROYaEfSYjNjYWzz77LO677z58/fXXyMnJwemnn46TTjqpMtonIiIiIlK1dCajwsIeZPzqhBNOwAknnBDJtoiIiIiISDUQ9iDjxhtvpOWzZs2yboyIiIiIiBz7wh5k7N+/P+TfRUVF+Prrr3HgwAH07NkzIo3q2bMnZs+ejWbNmkWkPhERERGR8lKEbcWFPciYN2/eYe8Fg0GMGDECLVu2DKuu+fPnl/n+xx9/jHfffRdNmzYFAFx22WXhNtPIcewy1lwSf5nb3BzZ6BaZo2MdEmFrjEIlMYlsXjQikrAMwOMRkTZI5KHL4nlJpKzLIicJH42xJEclEldpjOJ0zH2LxjaQCELkHiR1miv1pdrFZrKoXeSUHanpJJljoalYEivJ9oECEh/JoippnC7pC6YyFrnMolzZcsMcW8qOh24cidjOJR8dCeZt58aX3U4nQPprgMRCszhNdjwkXLINHBLP68aSKNGSsre3S2Jj2b5I+xZDqgyS7e3LI32PRb+b9jl6nDQXoZDFOJMJ2bGXrUvLzzHXsNws9tY5aI68plg3Z8c1EufPvmc4bD2L/Ib1PRmH8vl8SE9Px7nnnot77rmn3NP169cPjuPALWMnv+222wD8kvNcwnYGEREREZFI0pmMCovYo1M2btyIYvaLchn69OmDvn37YseOHQgGg6WvqKgofP311wgGgxpgiIiIiIgcY8I+k5Genh7yb9d1sX37dvzzn//EkCFDwqrr/fffx1/+8hd06tQJ06dPxyWXXBJucxAIBBAIhJ4SDLol8Dk6pSciIiIi4oWwz2SsWbMm5PXVV18BAKZMmYKpU6eG3YC77roL8+fPx+jRozF8+HDk5YV3XWJmZiZSU1NDXhtzV4fdDhERERERAL9cLuXVK0xPPfUUmjdvjri4OHTp0gUrVqygf//GG2+gdevWiIuLw6mnnor33nvP+Lc333wzHMex+o4f9pmMJUuWhD2TI+nQoQNWrlyJu+66Cx06dCjzHg2TjIyMw86uDGh1d6SbKCIiIiJyVHnttdeQnp6OmTNnokuXLpg6dSr69OmDdevWoV69eof9/aeffopBgwYhMzMTl1xyCV555RX069cPq1evRrt27UL+dt68eVi+fDkaNWpk1baI3ZNRUfHx8Zg5cyamTJmC2267DXXq1CnXdH6/HykpKSEvXSolIiIiIrYc17tXOB5//HEMGzYMQ4cORdu2bTFz5kwkJCQYn1s3bdo0XHjhhRg1ahTatGmDhx56CGeccQaefPLJkL/bunUrbrvtNrz88suIYWmoRNhnMk4//XQ45Yx1W706vMuWcnNzsWfPHiQkJOD111/HoEGDULt27XCbeITYObu4ABbblrSRRBeaokkBHlEYbZiOROk6trGGttNVRp0WkYE0Cti0HnGEeF7WDjY/2zhEUzQmicx0WCwji9mN85vLSD9nUbTWfS8+LvxpyD5gG1tKtymrk7XFJv6SReIyReaYV9vYbiffsi35BeY6TUEeLOCD9a0iFp9Klpv1Zcs+RNcXiyY21VdAonsTWGQxqbPYvL58uaT9JAqcRsea4m1t4p0BHllMo8Xt9lP6TSKWfLYYltth+7fP8sdRtk78pN+x6GGGfe5EOrr+OFbW/cZ+vx9+f+jnd2FhIVatWoWMjIzS93w+H3r16oVly5aVWfeyZcsOuwKoT58+ePvtt0v/HQwGcf3112PUqFE45ZRTrJcj7E+fCy+8EBs3boTf78e5556Lc889F3Fxcdi4cSN69+6Nyy+/vPR1JG3btsW+ffsAAD/99BNOOeUU3HXXXVi0aBHGjRuHtm3bYtOmTeEvlYiIiIjIMais+40zMzMP+7s9e/agpKQE9evXD3m/fv362LFjR5l179ix44h/P3nyZERHR+P222+v0HKEfSZj9+7duP322/HQQw+FvD9u3Dj89NNPxtMzZfnuu+9KY28zMjLQuHFjfPnll0hNTUVOTg6uuOIKjB07Fq+88kq4zRQRERERsePhczLKut/4t2cxKsuqVaswbdo0rF69utxXLpmEfSbjjTfewODBgw97/7rrrsNbb71l3ZBly5bhgQceQGpqKgAgKSkJ48ePxyeffGJdp4iIiIjIsaSs+43LGmTUqVMHUVFR2LlzZ8j7O3fuRIMGDcqsu0GDBvTv//Of/2DXrl044YQTEB0djejoaGzevBl/+tOf0Lx587CWI+xBRnx8PJYuXXrY+0uXLkVcXPjXV/86SiooKEDDhg1Dyho3bozdu3eHXaeIiIiIiK1j4cbv2NhYdOzYEYsXLy59LxgMYvHixejatWuZ03Tt2jXk7wFg0aJFpX9//fXX46uvvsIXX3xR+mrUqBFGjRqFDz74IKx1GPblUnfeeSdGjBiB1atX48wzzwQAfPbZZ5g1axbuu+++cKvD+eefj+joaGRnZ2PdunUh8VmbN2+2u/FbRERERKSaS09Px5AhQ9CpUyeceeaZmDp1KnJzczF06FAAwODBg9G4cePSezruuOMOpKWlYcqUKbj44osxZ84crFy5Es888wwAoHbt2od9946JiUGDBg3wu9/9Lqy2hT3IGDNmDE488URMmzYNf//73wEAbdq0wezZs3HVVVeFVde4ceNC/p2UlBTy73feeQfdu3cPt4kiIiIiIvY8vCcjHAMHDsTu3btx//33Y8eOHejQoQMWLFhQenP3li1b4Dskbe/ss8/GK6+8gnvvvRd//vOfcdJJJ+Htt98+7BkZkeC44Tz57hjRt8EtdhPSGEtzhF9ex+bGsoRVP9rNz2+IKAySzZWbay6zxWIgGdsoV5v5sXhChsTbUix2lbWFxHS6yQllvk9jLMl6dOPMfYtFbbLpimqV3UYg/FzvX8VsO1B2OxLMN7jRqFAW58jk5JnLWPxornk6l8XKmvoeOyYUk2jVRPO2wcEccxlTI9Vcxvo5ie0OJpa9XX055thbesxjxxkS/8y48eTmSnJ4csnxxGeILnVpjLm5jO4DlhGwQRKL6xSa+55TYrF92DZlWLwwjYsnnzmsLWw61r9M25XEO9MYZ/aZycpozLnlcjMWNwm/v+lxu3lVgVP/9BfP5v3fKXd5Nu9IsvoGeeDAATz33HP485//XBpBu3r1amzdujWijRMRERERkWNP2D/lfvXVV+jVqxdSU1Px448/4g9/+ANq1aqFuXPnYsuWLXjxxRcro50iIiIiIlWj2l3nU/XCPpORnp6OG264ARs2bAhJk7rooovw8ccfR7RxIiIiIiJy7An7TMbnn3+Op59++rD3GzdubHy6oIiIiIjIsaJij6ETwOJMht/vR3Z29mHvr1+/HnXr1o1Io0RERERE5NgV9iDjsssuw4MPPoii/yWnOI6DLVu2YPTo0fj9738fVl1vvfUW8vJIqouIiIiIiBxzwr5casqUKejfvz/q1auH/Px8pKWlYceOHejatSsmTpwYVl0DBgxAcnIyBg4ciJtuugldunQJtzmRZZnm6yuyjFhkjFFwpL7KiKQ7FtjG5dqqjDojPS/byENLLpsd2wUi3BbXsj7rVhwtCeBsuW2PCzT201zEtoFrimWlsdbmItD4VNJIxzKam6mEKo1Yv7M9HlofM46SfaAyPuNsj0+mbcC2DYuFZlHHkW4jQPdvup6PluNhpFSzxfFC2IOM1NRULFq0CEuXLsWXX36JnJwcnHHGGejVq5dVA+6++27MmzcPzz33HNq2bYs//OEPuP766/WkbxERERGRY5Tl08iAbt26oVu3bhVuwPDhw3Hfffdh1apV+Nvf/obx48djzJgxuOyyyzBs2DBccMEFdPpAIIBAIBDyXtAtgc+xfCiXiIiIiBzXbB8yK/+v3Cd4ly1bhnfffTfkvRdffBEtWrRAvXr18Mc//vGwL/vh6NixI6ZPn47t27fj2Wefxe7du3HhhReiRYsWdLrMzEykpqaGvDbmrrZuh4iIiIiIVEy5BxkPPvgg1q5dW/rv//73v7jpppvQq1cvjBkzBu+88w4yMzPDmrlTxrWEcXFxuP7667FkyRKsW7cO11xzDa0jIyMDWVlZIa+WiWeE1Q4RERERkVKuh69qotyXS33xxRd46KGHSv89Z84cdOnSBc8++ywAoGnTphg3bhweeOCBcs/cPcJNQq1atTrizeR+vx9+vz/kPV0qJSIiIiLinXKfydi/fz/q169f+u+PPvoIffv2Lf13586d8dNPP4U1802bNunZGiIiIiIi1Uy5z2TUr18fmzZtQtOmTVFYWIjVq1dj/PjxpeUHDx5ETExMWDNv1qxZyL9zc3Px+uuv4/vvv0fDhg0xaNAgu5SpKHImg8XERZPpSkqMRdu7xhrLWqwlqzjKIvOQxcfFhrf+S7F1EuY2LWUbZWe1TiyzI9m8WCwg6Qs0LpRwCorKLigm8/Kbt42TW2Cejmwbp8i83FF5xeY6bdn0E9JfnXzL+8JYX2DbNCmRtIVsgxjDccE2RrSYbBvrfk6OC4FCc5WkLVGm6dg+xdphHVtK+lChYV8E+L6TS/qeoZ0OO3YFzG10E+PM0xFOoXnb+PLM2xSsL9vEAVdGvHCAbDcax8zigM3fCdx48+e+6TOJ9tYi0n62Tmw+MwHAsTyes+9X1U01umzJK+XunRdddBHGjBmD//znP8jIyEBCQgK6d+9eWv7VV1+hZcuWYc28bdu22LdvHwDgp59+Qrt27XDXXXdh0aJFGDduHNq2bYtNmzaFVaeIiIiIiHir3IOMhx56CNHR0UhLS8Ozzz6LZ599FrGx/z+SnzVrFnr37h3WzL/77jsU/+9Xr4yMDDRq1AibN2/GihUrsHnzZpx22mkYO3ZsWHWKiIiIiFSE43r3qi7KfblUnTp18PHHHyMrKwtJSUmI+s0pszfeeANJSUnWDVm2bBlmzpyJ1NRUAEBSUhLGjx+Pq6++2rpOERERERGpelZP/C5LrVq1rBrwa4xtQUEBGjZsGFLWuHFj7N6926peERERERHxhvUTvyPl/PPPR3R0NLKzs7Fu3Tq0a9eutGzz5s12N36LiIiIiNiqRpctecXTQca4ceNC/v3by63eeeedkJvLRURERETk6HdUDTJ+69FHH7WrmMUy2sadstjMDtnm6Vi05G8eIhiCRERazYutE1ssOs82wtZniMdjy0ai/1y2Hll0L2OKHwXgxJFoSdb34i1iAVn0ItveObnhzwtApQQXZh8s822HxUnb7lOsv+bkmcuSyDZlEcM2YkksZoBEpLLpWCQr4bJ+zuI2483ry40yRLmWkL5cQPZh1k8KLY6hAFwSBe4Umfue6yfry9RPSJ90o8jHMzu+kn3fJccgN9683D4WI8yOo6a2uJaxymybsn3ANqKaRd+SbeBGG+ZHYm/pPsUif13SxgJyzGDHUba9bSNzj0HV6QZsrxw/vUVERERERKqE5/dkiIiIiIgcVXQmo8J0JkNERERERCLK8zMZhYWFePvtt7Fs2TLs2LEDANCgQQOcffbZuPzyy0Me+CciIiIiIkc/T89kfP/992jTpg2GDBmCNWvWIBgMIhgMYs2aNRg8eDBOOeUUfP/99142UURERESOM3rid8V5eiZjxIgROPXUU7FmzRqkpKSElGVnZ2Pw4MG49dZb8cEHH4RXcRxJmCFJEm5KgrHMmA4CIGVusnl+0XvMZQxLkrERZZkNRJJWaJ0slYOleZiSK4otU2RKzNvNJWW0jSStx4kh64sxpRuxvszaz5Y7L988HUlMcWzTuAg3v6DseeXatdFlaUOGZCMAcAzt+GU6ksiTEG+uk+0fhgSm4pqJxkmi95N9iqXBsFQz26QYsg8Ea5iPhzktyy5L+WafcRqXpHs5OWS7sWQdkmDEtpubaG5LMMF81j0qi/Rng5Jkc9+K3pVlnpBs75LaScayQB3zsvlNaUkAfAfN69kxHWtYf2UsE8MQTb7qkJSlYJ2yHz4MAL6DpO8ZtkFxDfM2jSkg6VI2CV4ATXqjCZbss4XNz1RWGemWckzwdJCxdOlSrFix4rABBgCkpKTgoYceQpcuXTxomYiIiIgct6rRGQWveHq5VI0aNfDjjz8ay3/88UfUqFGjytojIiIiIiIV5+mZjD/84Q8YPHgw7rvvPpx//vmoX78+AGDnzp1YvHgxJkyYgNtuu43WEQgEEPjNpUVBtxg+x/N72kVEREREjkuefhN/8MEHkZiYiEcffRR/+tOf4Pzvuj3XddGgQQOMHj0a99xzD60jMzMT48ePD3mvZc2uOKlWt0prt4iIiIhUY7pcqsI8/7l/9OjRGD16NDZt2hQSYduiRYtyTZ+RkYH09PSQ9wacPi7i7RQRERERkfLxfJDxqxYtWpR7YHEov98Pvz80gUeXSomIiIiIreoUJesVT7+Nr169GjVr1iwdXLz00kuYOXMmtmzZgmbNmmHkyJG4+uqrw6841xAHegROkTlCzi0wx9Xt7GaOl6v5T/N0DouHtXgIocuWm0XSETR4zibK7ogzNEzHIllJBKFrG7tKIngdtm1YVCLh1jBES5aQSNkistzx5jYaYyWPpIY5zpHFylKm7cqie1mMJYmppVGuieZoSTfFHCvrZOWY62Qx1IZ+GU2iskFidt1ahyf0/co5SNoYJOuExYIkmuO+2TZI3Fp233NjzfuNk2/e3m68uZ84Lov9JLHESea+wI5DThE5RsWUHYvrsmhYVh/Zv9mx15dr7pMxseboXl+B+VjD+okp4tmxPV7k28VJ0+NCiXldsvXF4rJN2zV6T665HZbfW9j2dvPJsZ7FCLPPTXYcFfkNT3vL0KFDsXHjRgDAc889h+HDh6NTp04YO3YsOnfujGHDhmHWrFleNlFEREREjjeuh69qwtMzGRs2bMBJJ50EAJg+fTqmTZuGYcOGlZZ37twZEydOxI033uhVE0VEREREJEyenslISEjAnj2/PBF769atOPPMM0PKu3Tpgk2bNnnRNBERERERseTpIKNv376YMWMGACAtLQ1vvvlmSPnrr7+OVq1aedE0ERERETlOOa7r2au68PRyqcmTJ6Nbt25IS0tDp06dMGXKFHz44Ydo06YN1q1bh+XLl2PevHleNlFERERERMLk6ZmMRo0aYc2aNejatSsWLFgA13WxYsUKLFy4EE2aNMHSpUtx0UUXedlEERERETne6MbvCvP8gRI1atTApEmTMGnSJK+bYh3DGbPfHGVH6yQxioiqRr0sEmyjaGmdluvYti0s1tcYC0jmVcj6FpmX7XKzGGTbOo31sXhFMi8Sr+iSGEunMuKYbdieJq/qWEkWm0naUhJX9keOU2iOSHVInDT9mYwdX0mdtJ8EzWVBEsMbZYqjZd08zvy5Ep3L+om5jC1bkMTpmiJ4AcBhxwVDlS5Zj4wTRT5raV9gfYisS5f1BbLxDNHjdBq277Pp2Dphx8rKaIvIbyjwWEREREREIsrzMxkiIiIiIkcTPfG74nQmQ0REREREIkpnMkREREREDqUzGRXm+SBj7969+Oqrr9C+fXvUqlULe/bswd/+9jcEAgEMGDAAbdq08bqJIiIiIiISBk8HGStWrEDv3r2RnZ2NGjVqYNGiRRgwYACio6MRDAYxadIkfPLJJzjjjDO8bKaIiIiIHEd0T0bFeTrIGDt2LAYMGIDHH38cTz/9NPr164cLL7wQzz77LADgxhtvxEMPPRT+A/kSE8xlLHoxwW+ebK+5ykb/KTJPF2euk8UoIi9gLjPNK4ZsTtuIy9gYc1kMKYsi86MRpIZ1UmSel3Mwx1hGjxFs/ZM2uoVke7PoP1LmHDhYdkF8nLm+YhL7mWOOGXQD5r7lknhCx418dKFbUHZbHMP7vxRWQqRsfoF5diwiMiHeXMb6uWG7ltQ0H7uissz7N4uApftpNDlmsM2dl28uq5VoLMptFFvm+zX25pqbkWg+hvqy8sztIP2c8ZFjZTCFbG92PDFg8a9uVNnrCgDtryzS1K2ZZCwriTMvd1SBuc6oIhJvm23YPuxYwo6hReSYx/Y39nlUYm5LMNXcl305ZB8wRP4Wk30jmrTDNjbWYZ/7RebPMRSTWGLGtA9UZQy4HFU8vfF71apVSE9PR3JyMu644w5s27YNw4YNKy0fOXIkPv/8cw9bKCIiIiIi4fL0TEZhYSHi43/5ZSgmJgYJCQmoU6dOaXmdOnWwdy85hQAgEAgg8JtfrIJuMXyO57ebiIiIiMixSJdLVZinZzKaNm2KH374ofTfc+bMQcOGDUv/vX379pBBR1kyMzORmpoa8tq4/7NKa7OIiIiIiHCeDjKuvvpq7Nq1q/TfF198cemZDQCYP38+zjzzTFpHRkYGsrKyQl4ta3aptDaLiIiISPXmuN69qgtPrykaN24cLR87diyi2E2XAPx+P/z+0BsDdamUiIiIiIh3jupv4wkJJCVKRERERESOSp4OMlavXo2aNWuiRYsWAICXXnoJM2fOxJYtW9CsWTOMHDkSV199dfgVk4hRFmXn7CGRdCQKLuG7XcYyGhHJpCSX/T6LgrOMbKRRm2xdkjjBiEfWsQi/WHPUo49FjLLIQBZ5yGJl2Tph84vmZ+zKxKIeY0ncaYN65jI2PxZ5aMmpXcswL7IeSVQojXKNIktn2t8Avm1YlCiLfzZMF8XqY2yjsoPmqEonyxCrDNB1EnXAHCtbcwWp09SOAOsLZD9l+z47PpFlc/LN+0B0LomGNkSaMtEHyOcR+1why+Y7YI4Kji8oLE+zyqjUvA1cU/wwORTaxoC70ZZXf5O2+LLM64t+lygou59Es2h6EmdMj/W2/CQiOc7y89sxrBOLeOejQjW6bMkrnt6TMXToUGzcuBEA8Nxzz2H48OHo1KkTxo4di86dO2PYsGGYNWuWl00UEREREZEweXomY8OGDTjppJMAANOnT8e0adNCnpPRuXNnTJw4ETfeeKNXTRQRERGR40x1ugHbK56eyUhISMCePXsAAFu3bj0sSapLly7YtGmTF00TERERERFLng4y+vbtixkzZgAA0tLS8Oabb4aUv/7662jVqpUXTRMRERGR45XreveqJjy9XGry5Mno1q0b0tLS0KlTJ0yZMgUffvgh2rRpg3Xr1mH58uWYN2+el00UEREREZEweXomo1GjRlizZg26du2KBQsWwHVdrFixAgsXLkSTJk2wdOlSXHTRRV42UUREREREwuT5czJq1KiBSZMmYdKkSVUzQ3YaisWksshDl0xne9rLNJ1tNCyLqbVdJyw+kq4vi3Viu21YbKxlVKJ9GVsGw65ou71ZZGBJJSybLVOdbF3BHAfqsvVl2SdZnVUazMjWP1s2tu9XBtttYGIbf8m6EOGyqGM2HfvNznSstN2mkV7HFajTJZ8DxjhaH9nfgmR/s11fDGkL7XuR3gb0czjC3yMAvn9Y//xsqDTo6e/Z1nTjd8Udm1teRERERESOWp6fyRAREREROaroTEaF6UyGiIiIiIhElKeDjJ9//rn0ORkA8J///AfXXnstunfvjuuuuw7Lli3zsHUiIiIiImLD00HG73//eyxfvhwA8I9//APnnnsucnJy0K1bN+Tl5SEtLQ3vvvuul00UERERkeOME/TuVV14ek/G2rVrccoppwAAMjMz8fDDD2P06NGl5U8++STuv/9+XHLJJeFVHOc3l0WRcVV8nLksv8BcVlhkLrNNYDqYYy4ziTKn7tAy1g5/rLksmnSfGFJmk/jEpsnPN1dH1qNbUhJ+O47Al5JsLmTbwLR8ifHhTwPQ/uruzzKXFReb67RNbyEcwzpx2L4YE2MuSzRP58aY17+za5+5LDnJPD/WTrZ9EsveP0pqJBgnidqXa66PiSXri21T1hdI/wrWrWEsK6xV9vqK+8ncJ0uSzOs46gBZJ3nm4wLjuOZ9Lphq3j5uNElZKir7WOMUm48zxSnmz7GYLPL5QI4zJXVTzGXx5mO2QxLpovLMn39OrqGfsLRAlqRUWGieF9sX2ecwaUsw2by9fTnkc8df9rosrFfDOI3/5wPGMrDPKoamStrGr7GEL8N6tk2Ik2Oep2cyoqOjcfDgQQDApk2b0Ldv35Dyvn37Yt26dV40TURERESOV66Hr2rC00FGWloaXn31VQDA6aefjg8//DCkfMmSJWjcuLEHLRMREREREVueXi41adIkdO/eHdu2bcM555yDsWPH4vPPP0ebNm2wbt06vPbaa5g5cyatIxAIIBAIhLwXDBbD51M6r4iIiIiIFzw9k9GmTRt89tlnKCwsxCOPPILc3Fy8/PLLeOCBB/D9999jzpw5uOGGG2gdmZmZSE1NDXlt3L+8ahZARERERKodx/XuVV14/nN/y5Yt8eqrr8J1XezatQvBYBB16tRBDLu58xAZGRlIT08PeW/AGeMro6kiIiIiIlIOng8yfuU4DurXrx/2dH6/H35/aAqHLpUSEREREWuWKZPy/zz9Nr569WrUrFkTLVq0AAC89NJLmDlzJrZs2YJmzZph5MiRuPrqq8OvmMXjkU7jxpnPnjjF5gi5YINaxjLf1t3mtpD4SDdgiOoj0zhRZLmjLePqWIQti7+MdBmJA3WLzBGKLJLVLSIRnYQpdvVIbXFYv/SXHVfpkuhFh0U12yLbxhdP4nQtub+5n6qU5bKxmFpaRvqJE02miyL7YwHpX/6yjzUsRjSKxEDS+NSDhnUM0Bhql8R3OiTCtiTBfBwN1Ch7fv6dlh9FthG8tscnEuXKPk2dYsO+b3ofgK+IHc/JzFhcK/n8K4k1TxeTTY5rhnheOj/baFVyfKVx0oxlNLdLoqHdWENEtd9u29ivL7IPsM9Gtu8QjiJs5Tc8vSdj6NCh2LhxIwDgueeew/Dhw9GpUyeMHTsWnTt3xrBhwzBr1iwvmygiIiIixxndk1Fxnp7J2LBhA0466SQAwPTp0zFt2jQMGzastLxz586YOHEibrzxRq+aKCIiIiIiYfL0TEZCQgL27NkDANi6dSvOPPPMkPIuXbpg06ZNXjRNREREREQseTrI6Nu3L2bMmAHglwfzvfnmmyHlr7/+Olq1auVF00RERETkeKUnfleYp5dLTZ48Gd26dUNaWho6deqEKVOm4MMPPyx9GN/y5csxb948L5soIiIiIiJh8vRMRqNGjbBmzRp07doVCxYsgOu6WLFiBRYuXIgmTZpg6dKluOiii7xsooiIiIgcZ3Tjd8V5/kCJGjVqYNKkSZg0aVLkKmURfoQpdg4AHFJnYR1z1GMci2YkMYQOicczT0Ri4kjsKsUeisiiPWnUrnldmqZjAXgOayOL8GPryzIf2zFE0drOz2HtJ7G+CJJYyUrgkhhFY6whw5aNcAJ2UZsslhikTsSQ2EyyTkzRxL7C8KcBAKeQbG/LiE66D5D1FZVriN8GEJNX9r7qFJr7Oe09tvs3KyNR0w5bJySO1lgnWThj7O2RkDayOn1F5ul8ZPvQyHhTFKrl/k23G4u3ZZG/JeS4EDRHuLNYe5SUvX9E55NpyOep9T5Mj2vkMzrS87OtT455np7JEBERERGR6sfzMxkiIiIiIkcVPfG7wnQmQ0REREREIkpnMkREREREDlGdbsD2iueDjPz8fLz66qv45JNPsH37dvh8Ppx44ono168fzj//fK+bJyIiIiIiYfJ0kPH999+jV69eyM/Ph9/vx88//4yLLroIn3/+OWbMmIErr7wSr7zyCqJZMoSIiIiISCTpTEaFefrt/fbbb8eFF16IGTNmwHEcTJ48GR999BGWL1+ODRs2oHfv3pgwYQIeeOCB8CrOy7dqj49F4AUC/9fe/cfXWP//A39cZz/OZrP5OTNs8yO/isnPRqIw0puhSCmk9FZ4i3d+LAlJ8zMKUb01haTeIaX8LCot3n6XxDCUESnDxmY7z+8ffXc+Drue27m2Oczjfrud241znet5XteP13X2Otd1PY7ppOP3mMfcVd9hHucIX/P5oEWhmlHaqMb7abQ44MtKBJ5WU4u3NYs7NYtCBPSbs7TYT6s3dWmDXm19aTF+gSYxyFrUo4/WDvNYX+OS+X6ibrfgIPP5tHWpbe8/z+Y+oYS/eT1lPzeyzdshUNal0t/E33yacVHp39p6Nllf3lospta/tfWVqfR9Q+lX/taOT1ocrf+v53N9Xo28Trtk3g6tD/hZi5MWZbm1ZYND6ftmu56XstxK5LLWRrW/Ke33VqKHjYtKNLQWYWu2DRwWj71K/Ln6earNJ0qUvLK+xFeJcPfO/f18/lL+Nrmo7OcaLSJcO2Zon6na9rFZi3+mW5NHb/zetGkT/v3vfzszmYcOHYr169fjzJkzuO222zBz5ky89957nmwiERERERG5yaNnMkqVKoXz5//vm6309HRkZWXB9/9/I1G/fn2cOHFCrZGRkYGMq0brDsmGzbD443NEREREdEvjjd8F59EzGe3atcOwYcPwyy+/IDk5GQMGDECDBg1QsmRJAMCxY8cQEhKi1oiPj0dwcLDL49D5bdej+URERERElAuPDjKmTJmCjIwM1K1bFzVq1MAPP/yA+fPnO6efPn0aw4cPV2vExcUhNTXV5VG9ZOOibjoRERERFVcO8dyjmPDo5VIhISFITExEUlISMjIyULt2bZckqYceeijPGna7HfarbkLkpVJERERERJ5zQ2TD3nbbbZ5uAhERERERFRKPDjJ27NiB0qVLo2rVqgCAhQsXYt68eTh27BgiIiIwaNAg9OzZ0/3CWoyoGrVpLR4v8FelLVqMosZsPq39FmNE1fm06D+N1XhYM2psrBLZqK1/raa2vrToPy1GUYv3u2QSHynKPFr7Lyvz+ftZqil+5rG4Vhlm0b3afqdtGyUG0jQeOY/3M7RIbB9lnVjZvzKVSFxtuS+kW5tPocbzKvulcS7NfD6zywDUWMwiiDtV9gV1uRXGBSUq2G5yXFAjwpX41EClDyvHXi0W1+svZR/S4lV9lT6QbfJ+Vj8fbMpVClrfscpLeT9vJeo4K/f+oUYga8cSjbafaxHP2ueRxVj4Yqf4XLXkMR7dW5544gkcOnQIAPCf//wH//znP9G4cWOMHj0aTZo0Qf/+/fHuu+96solEREREROQmj57JSEpKcl4q9eabb+L1119H//79ndObNGmCiRMnol+/fp5qIhERERHdYhhhW3AePZNRokQJ/PHHHwCA48ePo2nTpi7TmzVrhuTkZE80jYiIiIiILPLoIOP+++/H3LlzAQCtWrXCf//7X5fpH330EWrUqOGJphERERHRrUrEc49iwqOXS02ePBktWrRAq1at0LhxY0yfPh0bN25EnTp1sH//fvzwww9Yvny5J5tIRERERERu8uiZjLCwMOzcuRPR0dFYvXo1RARbt27F2rVrUblyZWzevBkdO3b0ZBOJiIiIiMhNHv+djFKlSmHSpEmYNGlS4RW1GvOqxYWqcW8W21LYLC+3Ns3iOFSLnVRLKuvZTLaFeQAASpyg1chfq9PMIjwdFqNctUjQLIttLAqF/X5aTG1R1LS6va2wsv8AgBJbajmO0mpcttnbWZmnqFiM/FW3gdl61uJHsy3UAyBK+82iVQFAvJVYXyvLptE+H7TPYY3WT7X2W6WsZ/HKfVoRtOLGYrXv3KB443fB3UKBx0REREREdD14/EwGEREREdENhWcyCoxnMoiIiIiIqFBxkEFERERERIXqhhhkOExuqnY4HDh27Nh1bg0RERER3coMEY89iguPDjLOnTuHHj16ICAgABUqVMBLL72E7Oz/S0I5ffo0qlat6sEWEhERERGRuzx64/eYMWOwe/duLFy4EGfPnsUrr7yCHTt2YNmyZfD19QUAiJUR3fWMlQRwuaTVtmixuDfESaabnxapZzmq0st8mtX9y6wtosSPGha7r9Z+L/Np4qPMZ5Fh0hY1TlONlFX6VFHE22qu83Go0FntH9qymUWXmkR+qvMARRNNqrGybIAegW4m27zvazG1ek3zdqgVtZhwbdnM1onVmFqrtG1zo7B6TFDjgC1O02reSrmu13k3LY48+pfsihUr8NZbb+Ghhx7CU089hW3btuH06dPo1KkTMjIyAABGMctdJiIiIiIq7jw6yDh9+jQiIiKc/y9XrhzWr1+P8+fPo2PHjkhPT8+zRkZGBs6dO+fycGjf/hIRERERKXhPRsF5dJARHh6Offv2uTxXsmRJrF27FhcvXkTXrl3zrBEfH4/g4GCXx6Hz24qqyURERERElAePDjLatWuHhISEa54PDAzEmjVr4Ofnl2eNuLg4pKamujyql2xcFM0lIiIiIrqhzJkzB5GRkfDz80OzZs2wdetW9fUff/wxateuDT8/P9SrVw9ffPGFc9rly5cxcuRI1KtXDwEBAQgLC0Pv3r2RkpLidrs8Osh4+eWXMW7cuFynlSxZEuvWrcNXX32l1rDb7QgKCnJ52IzCv0GViIiIiG4R4sGHG5YuXYphw4Zh7Nix2LFjB6KiotC+fXucOnUq19d///33eOSRR/Dkk09i586d6NKlC7p06YKffvoJAJCeno4dO3ZgzJgxzjCm/fv3o3Pnzu41DIAhluKbCsfgwYPRo0cPtGzZslDr3l/5X9Zm9FYGJ1nm93mkR1U2nVZi16/mNZUkH9h9c39e21xpyj0sNovjyaJImCmKxCczXspy+5qs4zyI3TzVychS4iiUfUj8fHKvd+myeT1lm5rVAwDjsnk7HHbz+bKC7eZtUWhhJD6nzuf6vPgq61hdJxb3n0sZ1ubLVNqSlWU+zWzbaccErU/5K2d9L14yn6b1t6BA86ZY7Kdm29V2KdN8Ji3ZyGp6jtZ+7XNASd0RX/P5jIsmy6cdl7U2ZirrS1tubyWRzte872vHLkupSFa3jTaf1c84LR1La4vWv836sXacud6fmVYSzwryfia+/O2NQq1XmNrcG++x997wdVy+X9usWTM0adIEs2fPBvD3b8xVqVIFgwcPxqhRo655/cMPP4y0tDR8/vnnzufuuusuNGjQAPPmzcv1Pf73v/+hadOmOHr0KMLDw/PdNo+eyZgzZw5at26NmjVrYvLkyTh58qQnm0NERERE9PfAz0OP3EKNclJXr5SZmYnt27ejbdu2zudsNhvatm2LxMTEXBcrMTHR5fUA0L59e9PXA0BqaioMw0CpUqXcWoUe/zGGtWvXomPHjpg2bRrCw8MRGxuLzz//3PRXwImIiIiIiqvcQo3i4689s/LHH38gOzsbFSpUcHm+QoUKpl/cnzx50q3XX7p0CSNHjsQjjzyCoKAgt5bD44OMevXqYebMmUhJScGiRYuQkZGBLl26oEqVKhg9ejQOHjzo6SYSEREREV0XuYUaxcXl/xKqwnL58mX06NEDIoK5c+e6Pb/HBxk5fHx80KNHD6xevRqHDx9G//79sXjxYtSqVcvTTSMiIiKiW4ghnnvkFmpkt197X2S5cuXg5eWF33//3eX533//HaGhobkuV2hoaL5enzPAOHr0KNatW+f2WQzgBhpkXCk8PBzjxo1DcnIyVq9e7enmEBERERHdUHx9fdGoUSNs2LDB+ZzD4cCGDRsQHR2d6zzR0dEurweAdevWubw+Z4CRlJSE9evXo2zZspbap0RNFL2IiAh4KWkqhmGgXbt217FFRERERHTLu0l+eXvYsGHo06cPGjdujKZNm2LmzJlIS0vDE088AQDo3bs3KlWq5LynY8iQIWjVqhWmT5+OBx54AB9++CG2bduGt99+G8DfA4yHHnoIO3bswOeff47s7Gzn/RplypSBrxvJnB4dZCQnJ1//Ny2C6LwLYebRfyV2KW2xGrdpRou/1GjrxEfZRbT5lKhHdbmtxONp8YpaOy4r8aOG+Uk+w2oogdIW01hWLSZRiS010pTYUoVN2aZe9iL4/RmzbadE6SJb2d6ZyjSNGo2pLHdACfNp6RfNp5n1K2W/gyj7nbZOtP1Vi/1U+pWhRXEq/dtIM5ug9HstwtbqMdRqTKrWh7XtbRZJrlGihx3lg92vB8B2Qdlu2nFUWzbtDw5tny1sWh/Q+pVG647BSsSzd+4zGhfN15WhxWFrtMhlre9oxwXtc1OLhTfrVzfJH+s3q4cffhinT5/GSy+9hJMnT6JBgwZYvXq18+buY8eOwXbFsb558+b44IMP8OKLL+KFF17AbbfdhhUrVuCOO+4AABw/fhwrV64EADRo0MDlvb7++mu0bt06323z6CCDiIiIiOhGY9xEIaeDBg3CoEGDcp22cePGa57r3r07unfvnuvrIyMjUVg/oXdD3pNBREREREQ3Lw4yiIiIiIioUPFyKSIiIiKiK/FekgLjmQwiIiIiIipUN9yZjOTkZBw8eBAVK1Z03ulORERERHTd8ERGgXl0kPHss89iypQpCAwMxMWLF/H4449j+fLlEBEYhoFWrVph5cqVCAw0j4rLlRpBqMVAWot7u6w1z2rcqRmr0Ysaq+tEo7Ul22I8npX30mjLpqW1GhZjfTVm21WLJVZjVy2epNT2r6JI2rASeajFK1qNqtSSULWYVK3vaMtgOs1iPW+Lh/LCPj4B1mKobyRq31HWlxYlalZTO975KjHOVinvJ8p2M9Rl07a3yXxaH7bKajR6URxHTWqq8efa55HVKGBt2bR4dG37OAo5gp6KNY9eLvXWW28hPT0dADBhwgRs2bIF69evx4ULF/DNN9/g2LFjmDhxoiebSEREREREbvLoIOPKHN7PPvsMU6ZMwb333osSJUqgRYsWeO2117Bs2TK1RkZGBs6dO+fycIjFH+QiIiIiolueIeKxR3Hh8Ru/jf9/eu3kyZOoX7++y7SoqCj8+uuv6vzx8fEIDg52eRw6v63I2ktERERERDqPDzLGjBmDYcOGwWazISUlxWXamTNnEBAQoM4fFxeH1NRUl0f1ko2LsslEREREVJyJeO5RTHj0xu977rkH+/fvBwDUrVsXR48edZn+xRdf4Pbbb1dr2O122O12l+dshnbXLhERERERFSWPDjI2btyoTn/00UfRt2/f69IWIiIiIiIARZOoeIvx6CBj8ODB6NGjB1q2bJnr9GrVqlkrXOixknrNgJPWom/VuDcrUXBaJJ3VCL+iiLjUmEXnqZG4yo3+RbH+7UUQJWoWLalFXGoxg9p8lzKUmubry5ZZBIeLy5dzf96uxQQr29TX12I7lL6jxQhnWFuXptvOS4vLtRi1abUPWz1lr/UBsyjULKUPe1lsh3Y81GhxrVq2dZYSD+tnEkerbDdDOwZZjFYVZf8Su3lkrpFlN50mvubrxMgy2fesRqOr+4nVmF2F0k4t8td0+6gR4RYDa6xG32rHE/VvAqUtHr8An240Ht0l5syZg9atW6NmzZqYPHkyTp486cnmEBERERFRIfD4uHPt2rXo2LEjpk2bhvDwcMTGxuLzzz+H43p/g05EREREBEbYFgaPDzLq1auHmTNnIiUlBYsWLUJGRga6dOmCKlWqYPTo0Th48KCnm0hERERERG7w+CAjh4+PD3r06IHVq1fj8OHD6N+/PxYvXoxatWp5umlEREREdCthhG2B3TCDjCuFh4dj3LhxSE5OxurVqz3dHCIiIiIicoNHBxkRERHwUhIhDMNAu3btrmOLiIiIiIiooDwaYZucnFw0hUsGWptPiy68aD6t1PZT5vPZzaP/YChjvHPnzaeZ8TGPIFSXTYuysxJHCegRqlrknlnEnxYEkK7EDGZkKu1QIgO105Vp6aaTjIAS5vNpEYtmUa7+fu7PA+jRixfSlPmUdXKm8L+TMIuBNLRIWWW/E3WfNI+PNM6mms8XEGA+rYS/+TQtTteknVmlzfcf7z+V7aZFe2rt1/qp1geU6F4pZX78zQ7KfX15nzpnOo8j0Hwd21KVdaL1fY1yXJYgpX/7arHLJutSOa45Asw/O2xnleVW4m2zy5jvC6LFvBrm28C4rOx7aSb7iRatqslUjnkai59HjmDz9WW7ZL5/icm+kFExyHQe+wnTSdYjfzXasd4qs89vK5H8N4JidNmSp9yQl0sREREREdHNy6NnMoiIiIiIbjj8JYUC45kMIiIiIiIqVBxkEBERERFRofL45VK7d+/G9u3b0bp1a1SrVg179+7FnDlz4HA40LVrV7Rv397TTSQiIiKiW0hx+uVtT/HomYxly5ahUaNGGDFiBKKiorB+/XrcfffdSEpKwpEjR/DAAw/ggw8+8GQTiYiIiIjITR49kzFx4kSMHz8eo0ePxocffoju3btj2LBhGDNmDABg+vTpmDp1Kh599FH3CqdfNJ+mRallKbGZSiygGh2rxcRpkYGBJtF5WjsuXlLaodzBpI3WLyrrUmuLtp619zObT12P5u1QI2W1dig1paQSCarte9r7mUXVqvursk4CzaNvjUCL60R7P42yDOJjcgjSImytfrukzafEXou/eRStcd48zhg2LRI7977qrfVhbf1rUbqXzONmoSWCavueEs+rRZp6/24SFay8lxYVCrty7NUox16xK9HDyrIZShytWaSpttxGhrJxzPoNzGOhAcDrrPn+Kj7K/qrR+rdZDK/FG2oN5bNW/CzuC8pxwZam9EctLjsr9wW0H/vTfB4tctnKZ2Ze82m0mmrU8U0aVWuGZzIKzKNnMvbv349evXoBAB5++GGkpaWhS5cuzuldu3bFwYMHPdQ6IiIiIiKywqNnMkqWLIkzZ84gMjISZ8+eRVZWFs6cOeOcfubMGQQG6j+sl5GRgYyrfhjKIdmwGRa/lSEiIiKiWxvPZBSYR89ktG3bFgMHDsTixYvRp08fxMTEIC4uDr/88gv279+P4cOH4+6771ZrxMfHIzg42OVx6Py267QERERERER0NY8OMqZNm4agoCAMGDAAmZmZWLp0KRo3boy6deuibt26SElJwaRJk9QacXFxSE1NdXlUL9n4Oi0BERERERFdzaOXS1WoUAFr1651eW7WrFkYOnQo0tPTUbt2bXh760202+2w211vLuOlUkRERERkGS+XKjCPDjIGDx6MHj16oGXLli7PV6tWrWCFre4YSjqIOluAeZKP7dx58xl9LaZhmNGWuygSebT1pSVPWXk/q+3QpllM7DC05COLxNtkfWntECXNxktLrbGY3KQllFlkmC2ftt20BBOz9ZjXfNr+6lUEX1pY2c+VaeJr3kYjzWIyjZYiAyVRSNsGJutZy6PX0pI0htUUHK39SjsdWuKWhe3t8DOv53U2zfy9tLQnJRFJTZfSEvyUxC3TvmNYPJZkKctmsZ9qqWDwUvYF5Xhouu+px1dlPWrTrne6lMbq5z4VWx7dI+bMmYPWrVujZs2amDx5Mk6ePOnJ5hARERER/R217KlHMeHxYefatWvRsWNHTJs2DeHh4YiNjcXnn38Oh8WzCkRERERE5FkeH2TUq1cPM2fOREpKChYtWoSMjAx06dIFVapUwejRo/k7GURERERENxmPDzJy+Pj4oEePHli9ejUOHz6M/v37Y/HixahVq5anm0ZEREREtxBDxGOP4uKGGWRcKTw8HOPGjUNycjJWr17t6eYQEREREZEbPJouFRERAS8lEcIwDLRr1+46toiIiIiIbnnF6IyCp3h0kJGcnFw0hbW4N2VQI8p8WiygkalEgmqxn1oUqmEWaWoxplZbJxqrsaVehdw5tXZoUZUOJRpTiYA1zNZ/XrIsrmez6D+rUYLafq4kJxta+zMyrbVFYxbTmam8l/bbOdr60sIktGhPJdLU0NaJn918mll/VNqhRtgqcafasUuVx28UmVJiLE3XpbJtxGIsppF52dJ86vbOVraBFgFrtnxaPYtRuvofRsp+rhwzHH7m02xqDHLuTxtZ1qLFtfhtsStfWGrrWfuIy1L2hSxl/zJ7P+V4IRkZ5vWUzzGV8tmofsZZ/EyFjYE95MqjgwwiIiIiohuO1cEdOd2Q92QQEREREdHNi4MMIiIiIiIqVLxcioiIiIjoSrzxu8Bu6DMZv//+O15++WVPN4OIiIiIiNxwQw8yTp48ifHjx3u6GURERER0KxHx3KOY8OjlUnv27FGn79+/31phX/M4R3hpkXRKpKxS07ioRM/5KnmhWlylWTvVuEBlzKjNp00TJaLTak2NWefSIvV8tExWLcLPWhvFz/z91IpahKqVtiixvmoMpxoXqsRAZljcpgrxyr2moUUW+yoRqSb1AMBQv1NR4owzlKhKbZtaiRnVIoSV9zIuK/Np/UPrVwoJ8DOd5vC30B8vKdGkSjSsvtzWPt60uFOVtu9dzH0fUt9Li7A9fyG/rXJ9P3/z7ab1DvEtYV5T2wZmRa0mnaqf30pRZT0bWnS9FlGdlm4+zax/a8c1q0lGanS9Et2rxeFrSeza+1n93Kdiy6ODjAYNGsAwDEguO23O85Yz3omIiIiIyCM8OsgoU6YMpkyZgjZt2uQ6fe/evejUqdN1bhURERER3dKK0WVLnuLRQUajRo2QkpKCiIiIXKefPXs217McV8rIyEDGVb+U6ZAs2AwGZxEREREReYJHb/weMGAAIiMjTaeHh4cjISFBrREfH4/g4GCXx6Fz2wq5pURERER0y3CI5x7FhEcHGV27dsVjjz1mOr106dLo06ePWiMuLg6pqakuj+pBjQu7qURERERElE83/TVFdrsddrvd5TleKkVERERElonVGDTK4fG/xmfPno2tW7eiY8eO6NmzJxYuXIj4+Hg4HA5069YNL7/8Mry93Wym9nolAk+dT5t2WYm41GIUlbhQ0zhaq1GuWmcxlHViNXa1sCNstXtztHWsbTct8tdiTJ8obTG0eFKzGEiL5xrVWEl1RovbTY1BLuTIQ6WekanEUGsltajjTC2iU9lAWv/2UqZZmUeJ4VSjsjXacU1bXxb2PTVGVI3otLifayz2HS0J0TReVYk0VaOTrd6Qqr1fpvn72S6ZT1P7nJUIW6vHPK0d2jFDjb4tgv3LrB0WI5ctX1qj/S2hHZe1GF6LkdhUfHl0kPHKK69gypQpiImJwdChQ3H06FFMnToVQ4cOhc1mw4wZM+Dj48Mf5CMiIiIiuol4dJCxYMECLFiwAN26dcPu3bvRqFEjvPfee+jVqxcAoHbt2hgxYgQHGURERER0/TDCtsA8euN3SkoKGjf++ybtqKgo2Gw2NGjQwDm9YcOGSElJ8VDriIiIiIjICo8OMkJDQ/Hzzz8DAJKSkpCdne38P/D3j/GFhIR4qnlEREREdCtihG2BefRyqV69eqF3796IjY3Fhg0bMGLECDz//PM4c+YMDMPAxIkT8dBDD3myiURERERE5CaPDjLGjx8Pf39/JCYmon///hg1ahSioqIwYsQIpKeno1OnTpgwYYInm0hERERERG7y6CDDZrPhhRdecHmuZ8+e6NmzZ8EKa5GNWoStFttmtWZhx71pp9G0dkgRxM3eKLRoTyViVCxGe4q3eU11TWoxiibTRKto8aY08bG23EWxl4jJtlNjS7X9XKPFZnopS5epzKhFPFvp31bjfi3GKmsXzGqRyw5fJarZ0n6pRIVa3c+1baolc2vbVDmei7JOYBZHq0SLG8rxSS5bi2o21Mhla8caQ2uLlb6qfFZpUdNqG7O1GGRr2xtK/xCz7arV09aV9rlv9ZihpfNq+4LWluuX+Ht98MbvAvPoPRlERERERFT8ePzH+IiIiIiIbig8k1FgPJNBRERERESFimcyiIiIiIiuxDMZBXZDnsmoVq0akpKSPN0MIiIiIiKywKNnMt54441cnz927BgSEhIQGhoKAPjXv/51PZtFREREREQF4NFBxnPPPYdKlSrB29u1GQ6HA++//z58fHxgGIbbg4zsUgGm08TH/OSNtxL3llWqhOk02+4DptMcUTVNp3lduGQ6zUjPMJmgtLFCsHk9LXpRic7zOudrOk2lRSVqpyDN4iO1lMFgf/NyvkrcrJb8p0Qeep86ZzpNStjNpymRmsYfZ3N/3lvpolnm0ZHG72fMp5Uw35e1msgw2ScLwGYziem8bBL5CQBpF00nGdp8CilTynxiRqb5NGW+rNLmxyGv87n3K+OC+bJJoPl+bpy9YDots1qI6TRblrKf/2m+n3ulpptOQ5oyzWz/0vZzjRbRWcJ8fanHoPPm61Kbz6YttxktLveSeX8ztNhujdJ+OWe+3DYlrhXZFnJLtc8HZZsa/n7m86Wb950iocS8Gg6T5VM+HxCgHJe1/UQ7ZmvMjr2AHmFrZXvfrJcdaeud8sWjg4ynn34aW7ZswQcffIA6deo4n/fx8cHatWtRt25dD7aOiIiIiIis8Og9GfPmzcNLL72E9u3bY/bs2ZZqZGRk4Ny5cy4Ph8PiyJ6IiIiISMRzj2LC4zd+d+3aFYmJiVi+fDnuv/9+nDx50q354+PjERwc7PJIPvFtEbWWiIiIiIjy4vFBBgBUqlQJ69evxz333IM777wT4sYoLi4uDqmpqS6PqhVbFmFriYiIiIhIc8P8ToZhGIiLi0NMTAy+++47VKxYMV/z2e122O2uN1PZbDfMYhERERHRzaYYXbbkKTfcX+ONGjVCo0aNPN0MIiIiIiKyyOODjNmzZ2Pr1q3o2LEjevbsiYULFyI+Ph4OhwPdunXDyy+/fE3EbZ60i8C0yEONNpsW6aZFwVmM8TMj2nI7zOup8ylRrkVxsZ0WHWupnsO8nvZeWuSvGq8qFiN/zb4xsRL3C+j7j9Z+paZcthqVqEQ9msQvihbP6+NjsR3KDlsUF45qXdhsNavbW6lXFN+4adGY2nGtsGn7sjZNieaG1r+t0tpiZfso+6uIeR82DIvx4VY/q9TPP5P5LH8OW9wXtOXW5tP6gPLZIpL7/mVY3V+1+ZR2qHyUv6u0tmRZ3L9uRlbXLTl5dJDxyiuvYMqUKYiJicHQoUNx9OhRTJ06FUOHDoXNZsOMGTPg4+OD8ePHe7KZRERERETkBo8OMhYsWIAFCxagW7du2L17Nxo1aoT33nsPvXr1AgDUrl0bI0aM4CCDiIiIiK4bszNSlH8eTZdKSUlB48aNAQBRUVGw2Wxo0KCBc3rDhg2RkpLiodYREREREZEVHh1khIaG4ueffwYAJCUlITs72/l/ANi7dy9CQkI81TwiIiIiIrLAo5dL9erVC71790ZsbCw2bNiAESNG4Pnnn8eZM2dgGAYmTpyIhx56yJNNJCIiIqJbDW/8LjCPDjLGjx8Pf39/JCYmon///hg1ahSioqIwYsQIpKeno1OnTpgwYYLbdR1288VyeJufvPFSpjnsXqbTDCVdI9vffD5bmpK6Y5bSoKRMqG30UpIwvJXkCh/zmqIlXijrUk1FspBO4fC1uN205KnL5m300pJpLKafmKUpaRksoqW6KAwv83WiJsVYpR2ozfYFbR4l8UWU5Cw1dUe79FapKbYA85K+5uvZ9Fij7Vtan1LaqPUB2JQ0MWVfsJq+ZpZQZnq8y4uaeqS0UU1RU9LXFOo6MXsv5ZpvdX9V3ksMre8o07S+751p/n5aEpzZscZqupT22ZFh3kaVkqplOVHPbH9W1rEon7Xa56K6JpX5RDk+WU2+tNyPqdjy6CDDZrPhhRdecHmuZ8+e6Nmzp4daRERERES3PA6aCsyj92QQEREREVHxw0EGEREREREVKo//4jcRERER0Q1FuweI8sWjZzKmT5+Oo0ePerIJRERERERUyDw6yBg+fDiqV6+Odu3aYenSpcjMtJgOQURERERUWEQ89ygmPH651H/+8x+sWLECjz/+OIKCgvDYY4/hqaeewh133GG5piiZbuKlTVTi3rTYNiUSVI151dpiNp8Wg6pE8RlKrKHaRm1ntxpDaIXWDmW5Hdo6VqbZtGXz9TGdJErMqGExDdEKNf5Si7BVogutRHTmydukLco21dpvZFmM4NXiYRWGdjpdzR822Z+1/dxiX1T7gPI9k7YPiRITbumoYPUYpNH2Iavfr2nRymoCqftRzaLECxfFZRyi1FSjSbV1Yva5Y/XrTW0/0ba31d87UJfbQhS7cnwyLinRyVo7tMhl7e8W87n09axFHfN3JegqHr/xu2PHjlixYgV+++03jBgxAmvWrEFUVBSaNm2Kd955B+fPn/d0E4mIiIjoFiIOh8cexYXHBxk5QkJCMGLECOzbtw8bN25E3bp1MXToUFSsWNHTTSMiIiIiIjd49HIpw+SUXMuWLdGyZUu88cYbWLp0qVojIyMDGRkZLs85HFmw2Tx+JRgRERER0S3Jo2cyJI9rbYOCgtC/f3/1NfHx8QgODnZ5HDm2qTCbSURERES3Et74XWAeHWQ4HA6EhIQUqEZcXBxSU1NdHpHhrQqphURERERE5K6b/poiu90Ou93u8hwvlSIiIiIiy5iWVWAe/2t89uzZ2Lp1Kzp27IiePXti4cKFiI+Ph8PhQLdu3fDyyy/D29u9ZoqXEr1oJTYWgHhbi2u1Gplr5XSZGlUphb9OtPNgauSvVtNCLK7WfvGxtt2KJNvBSgykxdOmppGZyCO6UItxVmpaZdoWiwd3y20s5L4IAA4rsbhWI6O17ab0AYdaUotQNZ+kshLdayUi1Wo78no/q/3DSvyzNktRXE5RFOvEJO7U0PJ+NRbeKy+GQ+mnhZzyY/m4bP0NrU3TqPtC8UlFosLh0UHGK6+8gilTpiAmJgZDhw7F0aNHMXXqVAwdOhQ2mw0zZsyAj48Pxo8f78lmEhERERGRGzw6yFiwYAEWLFiAbt26Yffu3WjUqBHee+899OrVCwBQu3ZtjBgxgoMMIiIiIrp+eGamwDx643dKSgoaN24MAIiKioLNZkODBg2c0xs2bIiUlBQPtY6IiIiIiKzw6CAjNDQUP//8MwAgKSkJ2dnZzv8DwN69ewucPkVERERE5A5xiMcexYVHL5fq1asXevfujdjYWGzYsAEjRozA888/jzNnzsAwDEycOBEPPfSQJ5tIRERERERu8uggY/z48fD390diYiL69++PUaNGISoqCiNGjEB6ejo6deqECRMmeLKJRERERETkJo8OMmw2G1544QWX53r27ImePXsWqK5hNbZNnaZMshjvV9i0NEdtnYjV8DzlnijDVgTRklbaYfG+LUPbpjfKqUytHbYiiE4uiuW2UvN6/xpqIUcuF2g+K7TjgtX+cT23gbYv3yysbO8babm1tqgx59dvGQxDiWm3egOvrXDjbQ0vJbpXid5XI5CL4hikTdPaKTfQPlsYeON3gXn0ngwiIiIiIip+PP5jfEREREREN5LidAO2p/BMBhERERERFSqeySAiIiIiuhLvySgwnskgIiIiIqJCxUEGEREREREVLinmLl26JGPHjpVLly7dkPVulpo3QxuLoubN0MaiqHkztLEoat4MbSyKmjdDG4ui5s3QxqKoeTO0sShq3gxtLIqaN0MbqXgyRK536Pz1de7cOQQHByM1NRVBQUE3XL2bpebN0MaiqHkztLEoat4MbSyKmjdDG4ui5s3QxqKoeTO0sShq3gxtLIqaN0Mbi6LmzdBGKp54uRQRERERERUqDjKIiIiIiKhQcZBBRERERESFqtgPMux2O8aOHQu73X5D1rtZat4MbSyKmjdDG4ui5s3QxqKoeTO0sShq3gxtLIqaN0Mbi6LmzdDGoqh5M7SxKGreDG2k4qnY3/hNRERERETXV7E/k0FERERERNcXBxlERERERFSoOMggIiIiIqJCxUEGEREREREVqmI9yJgzZw4iIyPh5+eHZs2aYevWrZZrzZ07F/Xr10dQUBCCgoIQHR2NL7/8ssBtPH78OB577DGULVsW/v7+qFevHrZt22a53vnz5/Hcc88hIiIC/v7+aN68Of73v//le/5vvvkGnTp1QlhYGAzDwIoVK5zTLl++jJEjR6JevXoICAhAWFgYevfujZSUFMs1AaBv374wDMPl0aFDB8v1Lly4gEGDBqFy5crw9/dH3bp1MW/ePLWN8fHxaNKkCUqWLImQkBB06dIF+/fvd3nN22+/jdatWyMoKAiGYeDs2bMFqpdDRHD//ffnuizu1Dxy5Mg16zHn8fHHH+daM6/9+tKlSxg4cCDKli2LwMBAPPjgg/j9999N25hXvX/+85+oXr06/P39Ub58ecTGxuKXX34xrZefmgCQmJiI++67DwEBAQgKCsI999yDixcvWq556NAhdO3aFeXLl0dQUBB69OihLvfVJk2aBMMw8NxzzwEA/vzzTwwePBi1atWCv78/wsPD8a9//QupqamWawJA69atr9nWAwYMsFzv5MmTePzxxxEaGoqAgAA0bNgQn3zyiWmNcePGXfP+tWvXdk53p8/kt2aO/PabvGpa6TdA3sduEcFLL72EihUrwt/fH23btkVSUpLazrxqjhs3DrVr10ZAQABKly6Ntm3bYsuWLQWqCQD79u1D586dERwcjICAADRp0gTHjh2zVO/3339H3759ERYWhhIlSqBDhw7qckdGRua67gcOHGi532g1Aff7TV713O03AJCdnY0xY8agatWq8Pf3R/Xq1TFhwgRcmcWzbNkyxMTEoGzZsjAMA7t27SpwzSsNGDAAhmFg5syZluuZ9Z2pU6eqbaVbQ7EdZCxduhTDhg3D2LFjsWPHDkRFRaF9+/Y4deqUpXqVK1fGpEmTsH37dmzbtg333XcfYmNjsXfvXstt/Ouvv9CiRQv4+Pjgyy+/xM8//4zp06ejdOnSlms+9dRTWLduHRYuXIgff/wRMTExaNu2LY4fP56v+dPS0hAVFYU5c+ZcMy09PR07duzAmDFjsGPHDixbtgz79+9H586dLdfM0aFDB5w4ccL5WLJkieV6w4YNw+rVq7Fo0SLs27cPzz33HAYNGoSVK1ea1ty0aRMGDhyIH374AevWrcPly5cRExODtLQ0l+Xv0KEDXnjhBXV581svx8yZM2EYRoFrVqlSxWUdnjhxAuPHj0dgYCDuv//+XGvmtV8PHToUn332GT7++GNs2rQJKSkp6Natm2kb86rXqFEjJCQkYN++fVizZg1EBDExMcjOzrZcMzExER06dEBMTAy2bt2K//3vfxg0aBBsNvPDm1YzLS0NMTExMAwDX331FTZv3ozMzEx06tQJDodD30gA/ve//+Gtt95C/fr1nc+lpKQgJSUF06ZNw08//YQFCxZg9erVePLJJ/OsZ1YzR//+/V22+ZQpUyzX6927N/bv34+VK1fixx9/RLdu3dCjRw/s3LnTtNbtt9/u8v7fffedc5o7fSa/NXPkt9/kVdNKv8nPsXvKlCl44403MG/ePGzZsgUBAQFo3749Ll26ZLlmzZo1MXv2bPz444/47rvvEBkZiZiYGJw+fdpyzUOHDuHuu+9G7dq1sXHjRuzZswdjxoyBn5+f2/VEBF26dMHhw4fx6aefYufOnYiIiEDbtm1zPfYBf++LV677devWAQC6d+9uud9oNXO402/yqmel30yePBlz587F7NmzsW/fPkyePBlTpkzBrFmznK9JS0vD3XffjcmTJ6vL607NHMuXL8cPP/yAsLCwAtW7uu+8++67MAwDDz74YL7aTMWcFFNNmzaVgQMHOv+fnZ0tYWFhEh8fX2jvUbp0afnPf/5jef6RI0fK3XffXWjtSU9PFy8vL/n8889dnm/YsKGMHj3a7XoAZPny5eprtm7dKgDk6NGjlmv26dNHYmNj3W6fWb3bb79dXn75ZZfn3F0Hp06dEgCyadOma6Z9/fXXAkD++uuvAtfbuXOnVKpUSU6cOJGv9Z3fNuZo0KCB9OvXL981Rf5vvz579qz4+PjIxx9/7Jy2b98+ASCJiYlu18vN7t27BYAcPHjQUhtFRJo1ayYvvviiW/NrNdesWSM2m01SU1Od086ePSuGYci6devUGufPn5fbbrtN1q1bJ61atZIhQ4aYvvajjz4SX19fuXz5suWaeb2Hu/UCAgLk/fffd3l9mTJl5J133sm11tixYyUqKirP93Snz+Snprv9Jr/tzJFXv8nr2O1wOCQ0NFSmTp3qfO7s2bNit9tlyZIllmrmJjU1VQDI+vXrLdd8+OGH5bHHHsvX++VVb//+/QJAfvrpJ+dz2dnZUr58edN96GpDhgyR6tWri8PhyHV6fvuNVtNKv9HqudtvREQeeOCBa/axbt26Sa9eva55bXJysgCQnTt3qu3Kb83ffvtNKlWqJD/99JNERETIjBkzCtzGHLGxsXLfffep7aRbR7E8k5GZmYnt27ejbdu2zudsNhvatm2LxMTEAtfPzs7Ghx9+iLS0NERHR1uus3LlSjRu3Bjdu3dHSEgI7rzzTrzzzjuW62VlZSE7O/uab6D8/f1z/SawMKSmpsIwDJQqVapAdTZu3IiQkBDUqlULzzzzDM6cOWO5VvPmzbFy5UocP34cIoKvv/4aBw4cQExMTL5r5JyOL1OmjOV25FUvPT0djz76KObMmYPQ0NBCqXml7du3Y9euXfn+tvzq/Xr79u24fPmySz+qXbs2wsPD89WP8uonaWlpSEhIQNWqVVGlShVLbTx16hS2bNmCkJAQNG/eHBUqVECrVq3c2t+vrpmRkQHDMFx+ZMrPzw82my3PugMHDsQDDzzgss7MpKamIigoCN7e3gWquXjxYpQrVw533HEH4uLikJ6ebrle8+bNsXTpUvz5559wOBz48MMPcenSJbRu3dq0XlJSEsLCwlCtWjX06tXL9DIbd2g1rfab/LYzP/0mr2N3cnIyTp486bKOg4OD0axZM9O+4+7nQWZmJt5++20EBwcjKirKUk2Hw4FVq1ahZs2aaN++PUJCQtCsWTPTy8/yqpeRkQEALp9BNpsNdrs9X30yMzMTixYtQr9+/UzPUuW33+RV091+o9Wz0m+aN2+ODRs24MCBAwCA3bt347vvvjM9e5Yf+anpcDjw+OOPY/jw4bj99tsLXO9Kv//+O1atWpXvzxy6BXh6lFMUjh8/LgDk+++/d3l++PDh0rRpU8t19+zZIwEBAeLl5SXBwcGyatWqArXTbreL3W6XuLg42bFjh7z11lvi5+cnCxYssFwzOjpaWrVqJcePH5esrCxZuHCh2Gw2qVmzptu1kMc3hBcvXpSGDRvKo48+WqCaS5YskU8//VT27Nkjy5cvlzp16kiTJk0kKyvLUr1Lly5J7969BYB4e3uLr6+vvPfee/luY3Z2tjzwwAPSokWLXKe7eybDrN7TTz8tTz75pLosVtsoIvLMM89InTp18qxltl8vXrxYfH19r3l9kyZNZMSIEW7XyzFnzhwJCAgQAFKrVq18ncUwq5mYmCgApEyZMvLuu+/Kjh075LnnnhNfX185cOCApZqnTp2SoKAgGTJkiKSlpcmFCxdk0KBBAkCefvpp03pLliyRO+64Qy5evCgi+relp0+flvDwcHnhhRfUNuZV86233pLVq1fLnj17ZNGiRVKpUiXp2rWr5Xp//fWXxMTEOPtOUFCQrFmzxrTeF198IR999JHs3r1bVq9eLdHR0RIeHi7nzp1zeZ07fSavmlb6TX7bKZK/fpPXsXvz5s0CQFJSUlzm6969u/To0cNSzRyfffaZBAQEiGEYEhYWJlu3brXczpwzQSVKlJDXXntNdu7cKfHx8WIYhmzcuNHtepmZmRIeHi7du3eXP//8UzIyMmTSpEkCQGJiYtR1KiKydOlS8fLykuPHj+c6Pb/9Jq+a7vabvOq5229E/j6Gjxw5UgzDEG9vbzEMQ1599dVcX5vfMxn5qfnqq69Ku3btnGdhtDMZ7rRRRGTy5MlSunRp5/GFiIMMN2RkZEhSUpJs27ZNRo0aJeXKlZO9e/darufj4yPR0dEuzw0ePFjuuusuyzUPHjwo99xzjwAQLy8vadKkifTq1Utq167tdi3twzszM1M6deokd955p8tlJQWpmePQoUPqJQB51Zs6darUrFlTVq5cKbt375ZZs2ZJYGBgnpe65BgwYIBERETIr7/+mut0dwcZudX79NNPpUaNGnL+/Hl1Way2MT09XYKDg2XatGl51jLbr60OMvLqJ2fPnpUDBw7Ipk2bpFOnTtKwYcM8P5TMaub8MRcXF+fy+nr16smoUaMs1RQRWbNmjVSrVk0MwxAvLy957LHHpGHDhjJgwIBcax07dkxCQkJk9+7dzufMBhmpqanStGlT6dChg2RmZpq2z52aOTZs2GB6+Vl+6g0aNEiaNm0q69evl127dsm4ceMkODhY9uzZY/qeV/rrr78kKCjomsvjrFximFvNgvabvNqZ336T17HbyiAjv58HFy5ckKSkJElMTJR+/fpJZGSk/P7775Zq5nxWPvLIIy6v6dSpk/Ts2dNSG7dt2yZRUVHOz6D27dvL/fffLx06dMi1jVeKiYmRf/zjH7lOy2+/cadmDq3f5KeelX6zZMkSqVy5sixZskT27Nkj77//vpQpUybXLxnzO8jIq+a2bdukQoUKLgMkbZDhThtFRGrVqiWDBg1S20i3lmI5yMjIyBAvL69rPnh69+4tnTt3LrT3adOmjfrNZl7Cw8NdvpETEXnzzTclLCysoE2TCxcuOD/gevToIR07dnS7htmHd2ZmpnTp0kXq168vf/zxR6HUvFq5cuVk3rx5btdLT08XHx+fa+5LefLJJ6V9+/Z51hs4cKBUrlxZDh8+bPoad/5gMqs3ZMgQ5x+wOQ8AYrPZpFWrVgVu4/vvvy8+Pj5y6tSpPNt4tZz9OueD9+rlDA8Pl9dee83ternJyMiQEiVKyAcffGCpjYcPHxYAsnDhQpfpPXr0cOsMm1k7T58+7Vz+ChUqyJQpU3Kdd/ny5c4/qq7cnjnbOOes3Llz5yQ6OlratGmT58AqvzWvdOHCBQEgq1evdrvewYMHr7mePme9/POf/1TbeqXGjRtfM8AryCDjypoF6Tf5aWd++01ex+6cL0qu/qPwnnvukX/961+WapqpUaOG6bfLedXMyMgQb29vmTBhgstrRowYIc2bNy9QG8+ePetcj02bNpVnn31WXY4jR46IzWaTFStWXDPNnX6T35pX0vpNXvWs9pvKlSvL7NmzXZ6bMGGC1KpV65rX5neQkVfNGTNmmPafiIiIArXxm2++EQCya9cutY10aymW92T4+vqiUaNG2LBhg/M5h8OBDRs2FOgeiqs5HA7nNahWtGjR4ppY0wMHDiAiIqKgTUNAQAAqVqyIv/76C2vWrEFsbGyBawJ/x9j26NEDSUlJWL9+PcqWLVsoda/022+/4cyZM6hYsaKl9l2+fPmaZCEvLy81GUhEMGjQICxfvhxfffUVqlat6vZ7u1Nv1KhR2LNnD3bt2uV8AMCMGTOQkJBQ4DbOnz8fnTt3Rvny5d1ue85+3ahRI/j4+Lj0o/379+PYsWNu9SOtn8jfX3S43Y9yakZGRiIsLKxQ+lFu7SxXrhxKlSqFr776CqdOnTJNUmvTpg1+/PFHl+3ZuHFj9OrVC7t27YKXlxfOnTuHmJgY+Pr6YuXKlbmm97hb82o5+1FufSevejnXpLvbd6504cIFHDp0yFLfzU9NK/3GnXbmt9/kdeyuWrUqQkNDXfrOuXPnsGXLFtO+Y/XzQOtfedX09fVFkyZN8v2+7rQxODgY5cuXR1JSErZt25bnZ1BCQgJCQkLwwAMPuDzvbr/JT82raf0mr3pW+016enqB+pqVmo8//vg1/ScsLAzDhw/HmjVrCtTG+fPno1GjRqb3B9EtyqNDnCL04Ycfit1ulwULFsjPP/8sTz/9tJQqVUpOnjxpqd6oUaNk06ZNkpycLHv27JFRo0aJYRiydu1ay23cunWreHt7y8SJEyUpKUkWL14sJUqUkEWLFlmuuXr1avnyyy/l8OHDsnbtWomKipJmzZrl+/Ty+fPnZefOnbJz504B4LxO9+jRo5KZmSmdO3eWypUry65du+TEiRPOR0ZGhqWa58+fl+eff14SExMlOTlZ1q9fLw0bNpTbbrtNLl265HY9kb8vAbn99tvl66+/lsOHD0tCQoL4+fnJm2++adrGZ555RoKDg2Xjxo0uy5Wenu58zYkTJ2Tnzp3yzjvvCAD55ptvZOfOnXLmzBlL9a6GPM7y5LdmUlKSGIYhX375pWmtHHnt1wMGDJDw8HD56quvZNu2bRIdHX3N5RL5rXfo0CF59dVXZdu2bXL06FHZvHmzdOrUScqUKWN6uUd+2jhjxgwJCgqSjz/+WJKSkuTFF18UPz8/9dKHvGq+++67kpiYKAcPHpSFCxdKmTJlZNiwYXmuzytdeSlSamqqNGvWTOrVqycHDx502X75ufcot5oHDx6Ul19+WbZt2ybJycny6aefSrVq1eSee+6xVC8zM1Nq1KghLVu2lC1btsjBgwdl2rRpYhiG6f1n//73v2Xjxo2SnJwsmzdvlrZt20q5cuWc32C702fyW/NqefWb/NZ0p9/k59g9adIkKVWqlPN+s9jYWKlatarpN/F51bxw4YLExcVJYmKiHDlyRLZt2yZPPPGE2O32a75Fd6edy5YtEx8fH3n77bclKSlJZs2aJV5eXvLtt99aqvfRRx/J119/LYcOHZIVK1ZIRESEdOvWTV2f2dnZEh4eLiNHjnR5viD9xqym1X5jVs9KvxH5O1WxUqVK8vnnn0tycrIsW7ZMypUr53Ip6pkzZ2Tnzp2yatUqASAffvih7Ny5U06cOGG55tW0y6XyWy81NVVKlCghc+fONX0fujUV20GGiMisWbMkPDxcfH19pWnTpvLDDz9YrtWvXz+JiIgQX19fKV++vLRp06ZAA4wcn332mdxxxx1it9uldu3a8vbbbxeo3tKlS6VatWri6+sroaGhMnDgQDl79my+58+5rOHqR58+fZynbHN7fP3115ZqpqenS0xMjJQvX158fHwkIiJC+vfvrw4GtXoif/9h07dvXwkLCxM/Pz+pVauWTJ8+3TQSUURMlyshIcH5mrFjx+b5Gnfq5TaP9sdSfmvGxcVJlSpVJDs727RWjrz264sXL8qzzz4rpUuXlhIlSkjXrl1NP+Dyqnf8+HG5//77JSQkRHx8fKRy5cry6KOPyi+//FKgNoqIxMfHS+XKlaVEiRISHR2d6x9I7tQcOXKkVKhQQXx8fOS2227Lc//JzZV/wJvtswAkOTnZUs1jx47JPffcI2XKlBG73S41atSQ4cOHu3WP1NX3ZBw4cEC6desmISEhUqJECalfv/410ZxXevjhh6VixYri6+srlSpVkocffthlcOdOn8lvzavlZ5CRn5ru9BuRvI/dDodDxowZIxUqVBC73S5t2rSR/fv3W6558eJF6dq1q4SFhYmvr69UrFhROnfurN74nZ92iojMnz9fatSoIX5+fhIVFaVeXpRXvddff10qV64sPj4+Eh4eLi+++KL6JZTI3/dAAbhm/RSk35jVtNpvzOqJuN9vRP6+BGzIkCESHh4ufn5+Uq1aNRk9erTLukpISMh12ceOHWu55tW0QUZ+67311lvi7+/v1t8adGswREx+CpKIiIiIiMiCYnlPBhEREREReQ4HGUREREREVKg4yCAiIiIiokLFQQYRERERERUqDjKIiIiIiKhQcZBBRERERESFioMMIiIiIiIqVBxkEBERERFRoeIgg4ioGOvbty+6dOmivmbjxo0wDANnz54t8vacOXMGISEhOHLkSJG/V46ff/4ZlStXRlpa2nV7TyKiWx0HGURUpE6fPo1nnnkG4eHhsNvtCA0NRfv27bF582ZPN+2GYRiG8xEcHIwWLVrgq6++KpTar7/+OhYsWOD8f+vWrfHcc8+5vKZ58+Y4ceIEgoODC+U9NRMnTkRsbCwiIyMBAEeOHIFhGNi1a5fzNefPn8e9996LunXr4rfffivwe9atWxd33XUXXnvttQLXIiKi/OEgg4iK1IMPPoidO3fivffew4EDB7By5Uq0bt0aZ86c8XTTbigJCQk4ceIENm/ejHLlyuEf//gHDh8+XOC6wcHBKFWqlPoaX19fhIaGwjCMAr+fJj09HfPnz8eTTz5p+prTp0/j3nvvRVpaGr799ltUrly5UN77iSeewNy5c5GVlVUo9YiISMdBBhEVmbNnz+Lbb7/F5MmTce+99yIiIgJNmzZFXFwcOnfu7PK6p556CuXLl0dQUBDuu+8+7N6926XWpEmTUKFCBZQsWRJPPvkkRo0ahQYNGjin5/YNfZcuXdC3b1/n/zMyMvD888+jUqVKCAgIQLNmzbBx40bn9AULFqBUqVJYs2YN6tSpg8DAQHTo0AEnTpxwqfvuu+/i9ttvh91uR8WKFTFo0CC3liU3pUqVQmhoKO644w7MnTsXFy9exLp16wAAmzZtQtOmTZ3vN2rUKJc/lv/73/+iXr168Pf3R9myZdG2bVvnpUFXXi7Vt29fbNq0Ca+//rrzzMmRI0dyvVzqk08+cS5jZGQkpk+f7tLeyMhIvPrqq+jXrx9KliyJ8PBwvP322+oyfvHFF7Db7bjrrrtynf7rr7+iZcuWCA4OxldffYWyZcvmuU6PHDkCm82Gbdu2udSaOXMmIiIi4HA4AADt2rXDn3/+iU2bNqltJCKiwsFBBhEVmcDAQAQGBmLFihXIyMgwfV337t1x6tQpfPnll9i+fTsaNmyINm3a4M8//wQAfPTRRxg3bhxeffVVbNu2DRUrVsSbb77pdnsGDRqExMREfPjhh9izZw+6d++ODh06ICkpyfma9PR0TJs2DQsXLsQ333yDY8eO4fnnn3dOnzt3LgYOHIinn34aP/74I1auXIkaNWrke1nyw9/fHwCQmZmJ48ePo2PHjmjSpAl2796NuXPnYv78+XjllVcAACdOnMAjjzyCfv36Yd++fdi4cSO6desGEbmm7uuvv47o6Gj0798fJ06cwIkTJ1ClSpVrXrd9+3b06NEDPXv2xI8//ohx48ZhzJgxLpddAcD06dPRuHFj7Ny5E88++yyeeeYZ7N+/33S5vv32WzRq1CjXafv370eLFi1Qt25dfPHFFwgMDHRO09ZpZGQk2rZti4SEBJd6CQkJ6Nu3L2y2vz/mfH190aBBA3z77bem7SMiokIkRERF6L///a+ULl1a/Pz8pHnz5hIXFye7d+92Tv/2228lKChILl265DJf9erV5a233hIRkejoaHn22Wddpjdr1kyioqKc/2/VqpUMGTLE5TWxsbHSp08fERE5evSoeHl5yfHjx11e06ZNG4mLixMRkYSEBAEgBw8edE6fM2eOVKhQwfn/sLAwGT16dK7Lmp9lyQ0AWb58uYiIpKWlybPPPiteXl6ye/dueeGFF6RWrVricDhc2hQYGCjZ2dmyfft2ASBHjhzJtXafPn0kNjbW+f/c1tPXX38tAOSvv/4SEZFHH31U2rVr5/Ka4cOHS926dZ3/j4iIkMcee8z5f4fDISEhITJ37lzT5YyNjZV+/fq5PJecnCwAxNfXV+69917JyspymZ6fdbp06VIpXbq08zXbt28XwzAkOTnZZZ6uXbtK3759TdtHRESFh2cyiKhIPfjgg0hJScHKlSvRoUMHbNy4EQ0bNnR+K757925cuHABZcuWdZ75CAwMRHJyMg4dOgQA2LdvH5o1a+ZSNzo62q12/Pjjj8jOzkbNmjVd3mfTpk3O9wGAEiVKoHr16s7/V6xYEadOnQIAnDp1CikpKWjTpk2u75GfZTHzyCOPIDAwECVLlsQnn3yC+fPno379+ti3bx+io6Nd7pdo0aIFLly4gN9++w1RUVFo06YN6tWrh+7du+Odd97BX3/95da6udq+ffvQokULl+datGiBpKQkZGdnO5+rX7++89+GYSA0NNS5rnJz8eJF+Pn55Tqtc+fO+Pbbb7Fs2TKX5/OzTrt06QIvLy8sX74cwN+Xvd17773Om8tz+Pv7Iz09Pe8VQEREBebt6QYQUfHn5+eHdu3aoV27dhgzZgyeeuopjB07Fn379sWFCxdQsWJFl3sjcuR1w/KVbDbbNZcIXb582fnvCxcuwMvLC9u3b4eXl5fL6668NMfHx8dlmmEYzro5lzGZKciyzJgxA23btkVwcDDKly+vvvZKXl5eWLduHb7//nusXbsWs2bNwujRo7FlyxZUrVo133WsyG1d5dwDkZty5cqZDoBGjx6N+vXr49FHH4WIoEePHgDyt059fX3Ru3dvJCQkoFu3bvjggw/w+uuvX/P6P//802UASURERYeDDCK67urWrYsVK1YAABo2bIiTJ0/C29v7mm+ec9SpUwdbtmxB7969nc/98MMPLq8pX768yw3a2dnZ+Omnn3DvvfcCAO68805kZ2fj1KlTaNmypaV2lyxZEpGRkdiwYYOz7pXysyxmQkNDXe7tyFGnTh188sknEBHn2YzNmzejZMmSzuQlwzDQokULtGjRAi+99BIiIiKwfPlyDBs27Jp6vr6+LmcjclOnTp1rIoY3b96MmjVrXjNAc8edd96JRYsWmU4fM2YMbDYbevXqBRHBww8/nO91+tRTT+GOO+7Am2++iaysLHTr1u2a1/z000946KGHLLefiIjyj5dLEVGROXPmDO677z4sWrQIe/bsQXJyMj7++GNMmTIFsbGxAIC2bdsiOjoaXbp0wdq1a3HkyBF8//33GD16tDMxaMiQIXj33XeRkJCAAwcOYOzYsdi7d6/Le913331YtWoVVq1ahV9++QXPPPOMS1pSzZo10atXL/Tu3RvLli1DcnIytm7divj4eKxatSrfyzRu3DhMnz4db7zxBpKSkrBjxw7MmjUr38virmeffRa//vorBg8ejF9++QWffvopxo4di2HDhsFms2HLli3OG+KPHTuGZcuW4fTp06hTp06u9SIjI7FlyxYcOXIEf/zxR65nHv79739jw4YNmDBhAg4cOID33nsPs2fPdrkB3or27dtj79696uVco0ePxoQJE9CrVy8sWbIk3+u0Tp06uOuuuzBy5Eg88sgj15x1OnLkCI4fP462bdsWaBmIiCifPHpHCBEVa5cuXZJRo0ZJw4YNJTg4WEqUKCG1atWSF198UdLT052vO3funAwePFjCwsLEx8dHqlSpIr169ZJjx445XzNx4kQpV66cBAYGSp8+fWTEiBEuN35nZmbKM888I2XKlJGQkBCJj493ufE75zUvvfSSREZGio+Pj1SsWFG6du0qe/bsEZG/b/wODg52WYbly5fL1YfKefPmSa1atZw1Bg8e7NayXA1X3Pidm40bN0qTJk3E19dXQkNDZeTIkXL58mUREfn555+lffv2Ur58ebHb7VKzZk2ZNWuWc96rb/zev3+/3HXXXeLv7y8AJDk5+Zobv0X+vmG/bt264uPjI+Hh4TJ16lSXNkVERMiMGTNcnouKipKxY8eaLoeISNOmTWXevHnO/+fc+L1z506X102ePFm8vLxk8eLF+V6n8+fPFwCydevWa9731Vdflfbt26ttIyKiwmOI5JJzSER0gxs3bhxWrFjh8kvRdONbtWoVhg8fjp9++skZL1tYJkyYgI8//hh79uxxeT4zMxO33XYbPvjgg2tuaCcioqLBezKIiOi6eeCBB5CUlITjx4/n+hsdVly4cAFHjhzB7Nmznb8fcqVjx47hhRde4ACDiOg64iCDiIiuq6t/mb2gBg0ahCVLlqBLly7o16/fNdNr1KiR6031RERUdHi5FBERERERFSqmSxERERERUaHiIIOIiIiIiAoVBxlERERERFSoOMggIiIiIqJCxUEGEREREREVKg4yiIiIiIioUHGQQUREREREhYqDDCIiIiIiKlT/D1CgEXgfsFonAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 특정 배치의 Attention Score 시각화\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in tmp_loader:\n",
    "        # 모델 예측 및 Attention Score 추출\n",
    "        output, attn_weights = model(batch_X)  # attn_weights: Check the actual shape\n",
    "        \n",
    "        # attn_weights 차원 확인\n",
    "        print(\"attn_weights shape:\", attn_weights.shape)\n",
    "        \n",
    "        # 특정 샘플의 Attention Score 선택 (차원에 따라 조정)\n",
    "        if attn_weights.dim() == 3:  # (seq_len, seq_len) 형태\n",
    "            attention_score = attn_weights\n",
    "        elif attn_weights.dim() == 4:  # (batch_size, num_heads, seq_len, seq_len) 형태\n",
    "            head_idx = 0  # 시각화할 Head\n",
    "            sample_idx = 0  # 시각화할 Sample\n",
    "            attention_score = attn_weights[sample_idx, head_idx, :, :]\n",
    "        elif attn_weights.dim() == 2:  # (seq_len, seq_len) 형태\n",
    "            attention_score = attn_weights\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported attn_weights dimension: {attn_weights.dim()}\")\n",
    "        attention_score = attention_score.squeeze(0)  # (1, 90, 90) -> (90, 90)\n",
    "\n",
    "        # 히트맵 시각화\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(attention_score.cpu().numpy(), annot=False, cmap='viridis', cbar=True)\n",
    "        plt.title(f\"Attention Heatmap\")\n",
    "        plt.xlabel(\"Sequence Position (Key)\")\n",
    "        plt.ylabel(\"Sequence Position (Query)\")\n",
    "        plt.show()\n",
    "        \n",
    "        break  # 하나의 배치만 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>...</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>y13</th>\n",
       "      <th>y14</th>\n",
       "      <th>y15</th>\n",
       "      <th>y16</th>\n",
       "      <th>y17</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID: tensor(1660.)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>617-5_cam01_drunken04_place03_night_summer_173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID: tensor(1660.)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1386</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>1449</td>\n",
       "      <td>1113</td>\n",
       "      <td>1430</td>\n",
       "      <td>1095</td>\n",
       "      <td>1496</td>\n",
       "      <td>1213</td>\n",
       "      <td>1465</td>\n",
       "      <td>1180</td>\n",
       "      <td>1</td>\n",
       "      <td>617-5_cam01_drunken04_place03_night_summer_173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID: tensor(1660.)</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1380</td>\n",
       "      <td>775</td>\n",
       "      <td>...</td>\n",
       "      <td>1396</td>\n",
       "      <td>1113</td>\n",
       "      <td>1411</td>\n",
       "      <td>1089</td>\n",
       "      <td>1452</td>\n",
       "      <td>1200</td>\n",
       "      <td>1466</td>\n",
       "      <td>1183</td>\n",
       "      <td>1</td>\n",
       "      <td>617-5_cam01_drunken04_place03_night_summer_173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID: tensor(1660.)</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1392</td>\n",
       "      <td>768</td>\n",
       "      <td>...</td>\n",
       "      <td>1397</td>\n",
       "      <td>1092</td>\n",
       "      <td>1435</td>\n",
       "      <td>1075</td>\n",
       "      <td>1440</td>\n",
       "      <td>1188</td>\n",
       "      <td>1464</td>\n",
       "      <td>1182</td>\n",
       "      <td>1</td>\n",
       "      <td>617-5_cam01_drunken04_place03_night_summer_173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID: tensor(1660.)</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1407</td>\n",
       "      <td>794</td>\n",
       "      <td>...</td>\n",
       "      <td>1432</td>\n",
       "      <td>1109</td>\n",
       "      <td>1422</td>\n",
       "      <td>1096</td>\n",
       "      <td>1437</td>\n",
       "      <td>1189</td>\n",
       "      <td>1434</td>\n",
       "      <td>1178</td>\n",
       "      <td>1</td>\n",
       "      <td>617-5_cam01_drunken04_place03_night_summer_173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>ID: tensor(460.)</td>\n",
       "      <td>95</td>\n",
       "      <td>1910</td>\n",
       "      <td>886</td>\n",
       "      <td>1914</td>\n",
       "      <td>879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1930</td>\n",
       "      <td>877</td>\n",
       "      <td>...</td>\n",
       "      <td>1949</td>\n",
       "      <td>1066</td>\n",
       "      <td>1946</td>\n",
       "      <td>1070</td>\n",
       "      <td>1954</td>\n",
       "      <td>1122</td>\n",
       "      <td>1954</td>\n",
       "      <td>1126</td>\n",
       "      <td>1</td>\n",
       "      <td>224-3_cam02_drunken03_place03_night_summer_114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24496</th>\n",
       "      <td>ID: tensor(460.)</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224-3_cam02_drunken03_place03_night_summer_114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24497</th>\n",
       "      <td>ID: tensor(460.)</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224-3_cam02_drunken03_place03_night_summer_114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>ID: tensor(460.)</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224-3_cam02_drunken03_place03_night_summer_114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>ID: tensor(460.)</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>224-3_cam02_drunken03_place03_night_summer_114...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24500 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label  frame    x1   x2    x3   x4  x5  x6    x7   x8  ...  \\\n",
       "0      ID: tensor(1660.)      0     0    0     0    0   0   0     0    0  ...   \n",
       "1      ID: tensor(1660.)      1     0    0     0    0   0   0  1386  776  ...   \n",
       "2      ID: tensor(1660.)      2     0    0     0    0   0   0  1380  775  ...   \n",
       "3      ID: tensor(1660.)      3     0    0     0    0   0   0  1392  768  ...   \n",
       "4      ID: tensor(1660.)      4     0    0     0    0   0   0  1407  794  ...   \n",
       "...                  ...    ...   ...  ...   ...  ...  ..  ..   ...  ...  ...   \n",
       "24495   ID: tensor(460.)     95  1910  886  1914  879   0   0  1930  877  ...   \n",
       "24496   ID: tensor(460.)     96     0    0     0    0   0   0     0    0  ...   \n",
       "24497   ID: tensor(460.)     97     0    0     0    0   0   0     0    0  ...   \n",
       "24498   ID: tensor(460.)     98     0    0     0    0   0   0     0    0  ...   \n",
       "24499   ID: tensor(460.)     99     0    0     0    0   0   0     0    0  ...   \n",
       "\n",
       "        y10   y11   y12   y13   y14   y15   y16   y17  y  \\\n",
       "0         0     0     0     0     0     0     0     0  1   \n",
       "1      1449  1113  1430  1095  1496  1213  1465  1180  1   \n",
       "2      1396  1113  1411  1089  1452  1200  1466  1183  1   \n",
       "3      1397  1092  1435  1075  1440  1188  1464  1182  1   \n",
       "4      1432  1109  1422  1096  1437  1189  1434  1178  1   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ... ..   \n",
       "24495  1949  1066  1946  1070  1954  1122  1954  1126  1   \n",
       "24496     0     0     0     0     0     0     0     0  1   \n",
       "24497     0     0     0     0     0     0     0     0  1   \n",
       "24498     0     0     0     0     0     0     0     0  1   \n",
       "24499     0     0     0     0     0     0     0     0  1   \n",
       "\n",
       "                                                FILENAME  \n",
       "0      617-5_cam01_drunken04_place03_night_summer_173...  \n",
       "1      617-5_cam01_drunken04_place03_night_summer_173...  \n",
       "2      617-5_cam01_drunken04_place03_night_summer_173...  \n",
       "3      617-5_cam01_drunken04_place03_night_summer_173...  \n",
       "4      617-5_cam01_drunken04_place03_night_summer_173...  \n",
       "...                                                  ...  \n",
       "24495  224-3_cam02_drunken03_place03_night_summer_114...  \n",
       "24496  224-3_cam02_drunken03_place03_night_summer_114...  \n",
       "24497  224-3_cam02_drunken03_place03_night_summer_114...  \n",
       "24498  224-3_cam02_drunken03_place03_night_summer_114...  \n",
       "24499  224-3_cam02_drunken03_place03_night_summer_114...  \n",
       "\n",
       "[24500 rows x 38 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_3frame_test.csv')\n",
    "test.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x,batch_y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:22\u001b[0;36m\u001b[0m\n\u001b[0;31m    def forward(self, src):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaConfig, RobertaModel\n",
    "# Transformer 모델을 위한 설정\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Multi-Head Attention 레이어\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=dropout)\n",
    "        # Transformer Encoder\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # 시퀀스 길이, 배치 크기, 특성 차원에 맞게 변환\n",
    "        x = x.transpose(0, 1)  # Transformer는 (seq_len, batch_size, features)의 형태를 기대함\n",
    "        # Attention 통과\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(attn_output)\n",
    "        # 마지막 시퀀스 출력을 사용 (기본적으로 클래스 레이블 예측)\n",
    "        output = transformer_output[-1, :, :]\n",
    "        # Fully connected layers 통과\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "# 모델 인스턴스 생성\n",
    "input_size = 34 # 입력 특징의 크기\n",
    "hidden_size = 50\n",
    "num_classes = 1  # 이진 분류\n",
    "model = TransformerModel(input_size, hidden_size, num_classes)\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "    \n",
    "# Forward 패스 및 Attention Weights 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(batch_X)  # 배치 중 일부를 추론\n",
    "    attention_weights = model.attention_weights  # (batch_size, seq_len, seq_len)\n",
    "# 특정 샘플의 Attention Weights 확인 (예: 첫 번째 샘플)\n",
    "sample_idx = 0\n",
    "attention_map = attention_weights[sample_idx].cpu().numpy()  # (seq_len, seq_len)\n",
    "# 좌표별 Attention 가중치 집계\n",
    "# 입력 좌표는 90프레임(시퀀스 길이) × 34 (17개 x, y 좌표)로 구성\n",
    "coordinate_weights = []\n",
    "for t in range(90):  # 시퀀스 길이\n",
    "    attention_per_frame = attention_map[t, :]  # 특정 프레임의 Attention (90,)\n",
    "    attention_avg_per_feature = attention_per_frame.reshape(-1, 2).mean(axis=1)  # x, y 평균 계산\n",
    "    coordinate_weights.append(attention_avg_per_feature)\n",
    "# 키포인트별로 평균 가중치 계산 (17개의 키포인트)\n",
    "average_weights = np.mean(coordinate_weights, axis=0)\n",
    "# 히트맵 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    average_weights.reshape(1, -1),\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    cbar=True,\n",
    "    xticklabels=[f\"Keypoint {i}\" for i in range(1, 18)],\n",
    "    yticklabels=['Attention']\n",
    ")\n",
    "plt.title(\"Keypoint Influence on Model Output\")\n",
    "plt.xlabel(\"Keypoints\")\n",
    "plt.ylabel(\"Attention Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Keypoint with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오른쪽팔 6 8 10\n",
    "# 왼쪽팔 5 7 9 \n",
    "# 오른족 다리 12 14 16\n",
    "# 왼쪽 다리 11 13 15\n",
    "# 이걸로 각도 & \n",
    "# 독립변수 : 12345, 각도 4개 \n",
    "# 독립변수 : 얼굴 하나로 & 전체\n",
    "# 독립변수: 얼굴 하나로 & 각도 4개\n",
    "# 서로 대칭되는 곳끼리 거리 (67)(89)(1011)(1213)(1415)(1617) & 머리\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_angle(pointA, pointB, pointC):\n",
    "    # 벡터 계산\n",
    "    BA = np.array(pointA) - np.array(pointB)\n",
    "    BC = np.array(pointC) - np.array(pointB)\n",
    "    \n",
    "    # 내적과 벡터 크기 계산\n",
    "    dot_product = np.dot(BA, BC)\n",
    "    magnitude_BA = np.linalg.norm(BA)\n",
    "    magnitude_BC = np.linalg.norm(BC)\n",
    "    \n",
    "    # 각도 계산 (라디안 -> 도)\n",
    "    cos_theta = dot_product / (magnitude_BA * magnitude_BC + 1e-6)  # 1e-6은 0으로 나누는 오류 방지\n",
    "    angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv\"  # 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 각도 계산을 위한 키포인트 인덱스\n",
    "# 예: 오른팔 (6, 8, 10), 왼팔 (5, 7, 9)\n",
    "keypoint_indices = {\n",
    "    \"right_arm\": (\"x7\",\"y7\", \"x9\",\"y9\", \"x11\",\"y11\"),\n",
    "    \"left_arm\": (\"x6\",\"y6\", \"x8\",\"y8\", \"x10\",\"y10\"),\n",
    "    \"right_leg\": (\"x13\",\"y13\", \"x15\",\"y15\",\"x17\",\"y17\"),\n",
    "    \"left_leg\":(\"x12\",\"y12\",\"x14\",\"y14\",\"x16\",\"y16\")\n",
    "}\n",
    "\n",
    "angles = []  # 각도를 저장할 리스트\n",
    "\n",
    "# 데이터프레임 순회\n",
    "for _, row in data.iterrows():\n",
    "    frame_angles = {}\n",
    "    for part, (xA, yA, xB, yB, xC, yC) in keypoint_indices.items():\n",
    "        pointA = (row[xA], row[yA])\n",
    "        pointB = (row[xB], row[yB])\n",
    "        pointC = (row[xC], row[yC])\n",
    "        frame_angles[part] = calculate_angle(pointA, pointB, pointC)\n",
    "    \n",
    "    angles.append(frame_angles)\n",
    "\n",
    "angle_df = pd.DataFrame(angles)\n",
    "angle_df['FILENAME'] = data['FILENAME']\n",
    "angle_df['y'] = data['y']\n",
    "angle_df[\"frame\"] = data['frame']\n",
    "angle_df['label']=data['label']\n",
    "\n",
    "angle_df.to_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined.csv')\n",
    "angle_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg']\n",
    "X = angle_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "angle_df[coordinate_cols] = X_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(angle_df, sequence_length)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02023623138666153\n",
      "Epoch 2, Loss: 0.0033999704755842686\n",
      "Epoch 3, Loss: 0.0013166291173547506\n",
      "Epoch 4, Loss: 0.0007548101712018251\n",
      "Epoch 5, Loss: 0.00017962028505280614\n",
      "Epoch 6, Loss: 0.0004608716117218137\n",
      "Epoch 7, Loss: 0.00010064301022794098\n",
      "Epoch 8, Loss: 9.689340367913246e-05\n",
      "Epoch 9, Loss: 0.00033079355489462614\n",
      "Epoch 10, Loss: 4.9731122999219224e-05\n",
      "Epoch 11, Loss: 3.3255593734793365e-05\n",
      "Epoch 12, Loss: 3.359944821568206e-05\n",
      "Epoch 13, Loss: 1.890248131530825e-05\n",
      "Epoch 14, Loss: 1.0078333616547752e-05\n",
      "Epoch 15, Loss: 2.340317951166071e-05\n",
      "Epoch 16, Loss: 4.2501626012381166e-05\n",
      "Epoch 17, Loss: 2.4469745767419226e-05\n",
      "Epoch 18, Loss: 9.645705722505227e-05\n",
      "Epoch 19, Loss: 1.7396492694388144e-05\n",
      "Epoch 20, Loss: 2.6865696781896986e-05\n",
      "Epoch 21, Loss: 1.2095048077753745e-05\n",
      "Epoch 22, Loss: 6.790176939830417e-06\n",
      "Epoch 23, Loss: 6.526624201796949e-05\n",
      "Epoch 24, Loss: 3.276687493780628e-06\n",
      "Epoch 25, Loss: 9.749062428454636e-07\n",
      "Epoch 26, Loss: 4.277771950000897e-06\n",
      "Epoch 27, Loss: 1.4344909686769824e-06\n",
      "Epoch 28, Loss: 1.0581666174402926e-06\n",
      "Epoch 29, Loss: 1.6331905499100685e-06\n",
      "Epoch 30, Loss: 6.040259904693812e-06\n",
      "Epoch 31, Loss: 6.922275588294724e-06\n",
      "Epoch 32, Loss: 2.672893515409669e-06\n",
      "Epoch 33, Loss: 4.3293148337397724e-06\n",
      "Epoch 34, Loss: 2.4775570750534825e-07\n",
      "Epoch 35, Loss: 1.6403396330133546e-07\n",
      "Epoch 36, Loss: 2.1022844975959742e-06\n",
      "Epoch 37, Loss: 8.845237061905209e-06\n",
      "Epoch 38, Loss: 4.596021426550578e-06\n",
      "Epoch 39, Loss: 8.242388503276743e-06\n",
      "Epoch 40, Loss: 1.0386520443717018e-05\n",
      "Epoch 41, Loss: 2.7493084076013474e-07\n",
      "Epoch 42, Loss: 1.8900907889474183e-05\n",
      "Epoch 43, Loss: 2.08514698840645e-08\n",
      "Epoch 44, Loss: 5.341464515140615e-08\n",
      "Epoch 45, Loss: 4.829120143767796e-07\n",
      "Epoch 46, Loss: 1.452168362447992e-06\n",
      "Epoch 47, Loss: 3.674251658480898e-08\n",
      "Epoch 48, Loss: 4.3134260607757824e-08\n",
      "Epoch 49, Loss: 7.987280241650296e-07\n",
      "Epoch 50, Loss: 9.436898835701868e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaConfig, RobertaModel\n",
    "\n",
    "\n",
    "# Transformer 모델을 위한 설정\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention 레이어\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 시퀀스 길이, 배치 크기, 특성 차원에 맞게 변환\n",
    "        x = x.transpose(0, 1)  # Transformer는 (seq_len, batch_size, features)의 형태를 기대함\n",
    "        \n",
    "        # Attention 통과\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(attn_output)\n",
    "        \n",
    "        # 마지막 시퀀스 출력을 사용 (기본적으로 클래스 레이블 예측)\n",
    "        output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Fully connected layers 통과\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = 4 # 입력 특징의 크기\n",
    "hidden_size = 50\n",
    "num_classes = 1  # 이진 분류\n",
    "model = TransformerModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 루프\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(output.squeeze(), batch_y.float())\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1205_onlydegree2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_test.csv')\n",
    "test_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg']\n",
    "X = test_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "# y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "# \n",
    "\n",
    "test_df[coordinate_cols] = X_normalized\n",
    "\n",
    "test_X_seq, test_y_seq = create_sequences(test_df, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_672750/950511128.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_onlydegree2.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 84.12%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.72\n",
      "Recall: 0.90\n",
      "F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_X_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = 4\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "\n",
    "loaded_model = TransformerModel(4,50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_onlydegree2.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_angle(pointA, pointB, pointC):\n",
    "    # 벡터 계산\n",
    "    BA = np.array(pointA) - np.array(pointB)\n",
    "    BC = np.array(pointC) - np.array(pointB)\n",
    "    \n",
    "    # 내적과 벡터 크기 계산\n",
    "    dot_product = np.dot(BA, BC)\n",
    "    magnitude_BA = np.linalg.norm(BA)\n",
    "    magnitude_BC = np.linalg.norm(BC)\n",
    "    \n",
    "    # 각도 계산 (라디안 -> 도)\n",
    "    cos_theta = dot_product / (magnitude_BA * magnitude_BC + 1e-6)  # 1e-6은 0으로 나누는 오류 방지\n",
    "    angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv\"  # 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 각도 계산을 위한 키포인트 인덱스\n",
    "# 예: 오른팔 (6, 8, 10), 왼팔 (5, 7, 9)\n",
    "keypoint_indices = {\n",
    "    \"right_arm\": (\"x7\",\"y7\", \"x9\",\"y9\", \"x11\",\"y11\"),\n",
    "    \"left_arm\": (\"x6\",\"y6\", \"x8\",\"y8\", \"x10\",\"y10\"),\n",
    "    \"right_leg\": (\"x13\",\"y13\", \"x15\",\"y15\",\"x17\",\"y17\"),\n",
    "    \"left_leg\":(\"x12\",\"y12\",\"x14\",\"y14\",\"x16\",\"y16\")\n",
    "}\n",
    "\n",
    "angles = []  # 각도를 저장할 리스트\n",
    "\n",
    "# 데이터프레임 순회\n",
    "for _, row in data.iterrows():\n",
    "    frame_angles = {}\n",
    "    for part, (xA, yA, xB, yB, xC, yC) in keypoint_indices.items():\n",
    "        pointA = (row[xA], row[yA])\n",
    "        pointB = (row[xB], row[yB])\n",
    "        pointC = (row[xC], row[yC])\n",
    "        frame_angles[part] = calculate_angle(pointA, pointB, pointC)\n",
    "    \n",
    "    angles.append(frame_angles)\n",
    "\n",
    "angle_df = pd.DataFrame(angles)\n",
    "angle_df['FILENAME'] = data['FILENAME']\n",
    "angle_df['y'] = data['y']\n",
    "angle_df[\"frame\"] = data['frame']\n",
    "angle_df['label']=data['label']\n",
    "angle_df['x1']=data['x1']\n",
    "angle_df['y1']=data['y1']\n",
    "angle_df['x2']=data['x2']\n",
    "angle_df['y2']=data['y2']\n",
    "angle_df['x3']=data['x3']\n",
    "angle_df['y3']=data['y3']\n",
    "angle_df['x4']=data['x4']\n",
    "angle_df['y4']=data['y4']\n",
    "angle_df['x5']=data['x5']\n",
    "angle_df['y5']=data['y5']\n",
    "\n",
    "angle_df.to_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined_withhead.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.007156191859394312\n",
      "Epoch 2, Loss: 0.0006888690404593945\n",
      "Epoch 3, Loss: 0.0007638198439963162\n",
      "Epoch 4, Loss: 0.00017193883832078427\n",
      "Epoch 5, Loss: 0.00013313803356140852\n",
      "Epoch 6, Loss: 0.00010332732927054167\n",
      "Epoch 7, Loss: 3.75340532627888e-05\n",
      "Epoch 8, Loss: 7.905962411314249e-05\n",
      "Epoch 9, Loss: 3.818572804448195e-05\n",
      "Epoch 10, Loss: 3.56439923052676e-05\n",
      "Epoch 11, Loss: 1.705347494862508e-05\n",
      "Epoch 12, Loss: 1.1809363059001043e-05\n",
      "Epoch 13, Loss: 3.131497578579001e-05\n",
      "Epoch 14, Loss: 1.613763924979139e-05\n",
      "Epoch 15, Loss: 2.272068741149269e-05\n",
      "Epoch 16, Loss: 7.508329872507602e-06\n",
      "Epoch 17, Loss: 4.100137630302925e-06\n",
      "Epoch 18, Loss: 7.242064384627156e-06\n",
      "Epoch 19, Loss: 1.2333428458077833e-05\n",
      "Epoch 20, Loss: 2.0647352357627824e-05\n",
      "Epoch 21, Loss: 5.678684829035774e-06\n",
      "Epoch 22, Loss: 1.6778935787442606e-06\n",
      "Epoch 23, Loss: 3.5763837331614923e-06\n",
      "Epoch 24, Loss: 8.627128408988938e-06\n",
      "Epoch 25, Loss: 1.2479467841330916e-06\n",
      "Epoch 26, Loss: 4.997628821001854e-06\n",
      "Epoch 27, Loss: 7.291733709280379e-06\n",
      "Epoch 28, Loss: 2.804924861266045e-06\n",
      "Epoch 29, Loss: 6.0219381339265965e-06\n",
      "Epoch 30, Loss: 1.6824111526148045e-06\n",
      "Epoch 31, Loss: 7.794147904860438e-07\n",
      "Epoch 32, Loss: 5.752732477048994e-07\n",
      "Epoch 33, Loss: 4.3166730279153853e-07\n",
      "Epoch 34, Loss: 1.108804553950904e-06\n",
      "Epoch 35, Loss: 7.837809903321613e-07\n",
      "Epoch 36, Loss: 1.823464231165417e-06\n",
      "Epoch 37, Loss: 3.7901500604675675e-07\n",
      "Epoch 38, Loss: 1.131357407757605e-06\n",
      "Epoch 39, Loss: 7.289546033462102e-08\n",
      "Epoch 40, Loss: 5.85789621254662e-07\n",
      "Epoch 41, Loss: 8.612310864464234e-08\n",
      "Epoch 42, Loss: 3.247830022701237e-07\n",
      "Epoch 43, Loss: 3.4070561127919063e-07\n",
      "Epoch 44, Loss: 1.457538587601448e-07\n",
      "Epoch 45, Loss: 5.632574584524264e-07\n",
      "Epoch 46, Loss: 8.536446785001317e-07\n",
      "Epoch 47, Loss: 2.261485576582345e-07\n",
      "Epoch 48, Loss: 2.7952066830039257e-06\n",
      "Epoch 49, Loss: 2.863537247321801e-07\n",
      "Epoch 50, Loss: 4.30919158134202e-08\n"
     ]
    }
   ],
   "source": [
    "angle_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined_withhead.csv')\n",
    "angle_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg','x1','x2','x3','x4','x5','y1','y2','y3','y4','y5']\n",
    "X = angle_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "angle_df[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(angle_df, sequence_length)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaConfig, RobertaModel\n",
    "\n",
    "\n",
    "# Transformer 모델을 위한 설정\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention 레이어\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 시퀀스 길이, 배치 크기, 특성 차원에 맞게 변환\n",
    "        x = x.transpose(0, 1)  # Transformer는 (seq_len, batch_size, features)의 형태를 기대함\n",
    "        \n",
    "        # Attention 통과\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(attn_output)\n",
    "        \n",
    "        # 마지막 시퀀스 출력을 사용 (기본적으로 클래스 레이블 예측)\n",
    "        output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Fully connected layers 통과\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = 14 # 입력 특징의 크기\n",
    "hidden_size = 50\n",
    "num_classes = 1  # 이진 분류\n",
    "model = TransformerModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 루프\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(output.squeeze(), batch_y.float())\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1205_degreewithhead2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_672750/1231814340.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_degreewithhead2.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 83.60%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.72\n",
      "Recall: 0.86\n",
      "F1 Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_test_withhead.csv')\n",
    "test_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg','x1','x2','x3','x4','x5','y1','y2','y3','y4','y5']\n",
    "\n",
    "X = test_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "# y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_df[coordinate_cols] = X_normalized\n",
    "\n",
    "test_X_seq, test_y_seq = create_sequences(test_df, sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_X_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = 14\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "\n",
    "loaded_model = TransformerModel(14,50,1)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_degreewithhead2.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 /home/alpaco/project/drunk_prj/data/3_frame_data/final_test_onehead.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_angle(pointA, pointB, pointC):\n",
    "    # 벡터 계산\n",
    "    BA = np.array(pointA) - np.array(pointB)\n",
    "    BC = np.array(pointC) - np.array(pointB)\n",
    "    \n",
    "    # 내적과 벡터 크기 계산\n",
    "    dot_product = np.dot(BA, BC)\n",
    "    magnitude_BA = np.linalg.norm(BA)\n",
    "    magnitude_BC = np.linalg.norm(BC)\n",
    "    \n",
    "    # 각도 계산 (라디안 -> 도)\n",
    "    cos_theta = dot_product / (magnitude_BA * magnitude_BC + 1e-6)  # 1e-6은 0으로 나누는 오류 방지\n",
    "    angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calculate_head_coordinates_for_csv(csv_path, output_path):\n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 얼굴에 해당하는 키포인트 인덱스 (1, 2, 3, 4, 5)\n",
    "    face_indices = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # x_head와 y_head를 계산하여 새로운 열로 추가\n",
    "    x_columns = [f\"x{i}\" for i in range(1, 18)]  # x1 ~ x17\n",
    "    y_columns = [f\"y{i}\" for i in range(1, 18)]  # y1 ~ y17\n",
    "\n",
    "    def calculate_head(row):\n",
    "        x_values = [row[f\"x{i}\"] for i in face_indices if row[f\"x{i}\"] != 0]\n",
    "        y_values = [row[f\"y{i}\"] for i in face_indices if row[f\"y{i}\"] != 0]\n",
    "        x_head = np.mean(x_values) if x_values else 0\n",
    "        y_head = np.mean(y_values) if y_values else 0\n",
    "        return pd.Series({\"x_head\": x_head, \"y_head\": y_head})\n",
    "\n",
    "    # 데이터프레임에 새로운 열 추가\n",
    "    df[[\"x18\", \"y18\"]] = df.apply(calculate_head, axis=1)\n",
    "    df.drop(['x1','x2','x3','x4','x5','y1','y2','y3','y4','y5'],axis=1,inplace= True)\n",
    "    \n",
    "    # 결과 CSV 파일로 저장\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"결과가 {output_path}에 저장되었습니다.\")\n",
    "    \n",
    "    # 입력 CSV 경로와 출력 경로 설정\n",
    "input_csv = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_3frame_test.csv\"\n",
    "output_csv = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_test_onehead.csv\"\n",
    "\n",
    "# 얼굴 중심 좌표 계산\n",
    "calculate_head_coordinates_for_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = \"/home/alpaco/project/drunk_prj/data/3_frame_data/final_test_onehead.csv\"  # 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 각도 계산을 위한 키포인트 인덱스\n",
    "# 예: 오른팔 (6, 8, 10), 왼팔 (5, 7, 9)\n",
    "keypoint_indices = {\n",
    "    \"right_arm\": (\"x7\",\"y7\", \"x9\",\"y9\", \"x11\",\"y11\"),\n",
    "    \"left_arm\": (\"x6\",\"y6\", \"x8\",\"y8\", \"x10\",\"y10\"),\n",
    "    \"right_leg\": (\"x13\",\"y13\", \"x15\",\"y15\",\"x17\",\"y17\"),\n",
    "    \"left_leg\":(\"x12\",\"y12\",\"x14\",\"y14\",\"x16\",\"y16\")\n",
    "}\n",
    "\n",
    "angles = []  # 각도를 저장할 리스트\n",
    "\n",
    "# 데이터프레임 순회\n",
    "for _, row in data.iterrows():\n",
    "    frame_angles = {}\n",
    "    for part, (xA, yA, xB, yB, xC, yC) in keypoint_indices.items():\n",
    "        pointA = (row[xA], row[yA])\n",
    "        pointB = (row[xB], row[yB])\n",
    "        pointC = (row[xC], row[yC])\n",
    "        frame_angles[part] = calculate_angle(pointA, pointB, pointC)\n",
    "    \n",
    "    angles.append(frame_angles)\n",
    "\n",
    "angle_df = pd.DataFrame(angles)\n",
    "angle_df['FILENAME'] = data['FILENAME']\n",
    "angle_df['y'] = data['y']\n",
    "angle_df[\"frame\"] = data['frame']\n",
    "angle_df['label']=data['label']\n",
    "angle_df['head_x'] = data['x18']\n",
    "angle_df['head_y'] = data['y18']\n",
    "\n",
    "angle_df.to_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_test_onehead.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.014883719384670258\n",
      "Epoch 2, Loss: 0.0009586727828718722\n",
      "Epoch 3, Loss: 0.0016589072765782475\n",
      "Epoch 4, Loss: 0.00027685592067427933\n",
      "Epoch 5, Loss: 0.0004059815837536007\n",
      "Epoch 6, Loss: 7.701945287408307e-05\n",
      "Epoch 7, Loss: 0.00017196114640682936\n",
      "Epoch 8, Loss: 5.1766248361673206e-05\n",
      "Epoch 9, Loss: 2.8751264835591428e-05\n",
      "Epoch 10, Loss: 7.226972957141697e-05\n",
      "Epoch 11, Loss: 0.0003938117588404566\n",
      "Epoch 12, Loss: 4.753976463689469e-05\n",
      "Epoch 13, Loss: 6.494703484349884e-06\n",
      "Epoch 14, Loss: 8.922156666812953e-06\n",
      "Epoch 15, Loss: 2.1165964426472783e-05\n",
      "Epoch 16, Loss: 2.6530385639489396e-06\n",
      "Epoch 17, Loss: 2.14610445254948e-05\n",
      "Epoch 18, Loss: 0.0003706550342030823\n",
      "Epoch 19, Loss: 1.3137268979335204e-06\n",
      "Epoch 20, Loss: 7.141207788663451e-06\n",
      "Epoch 21, Loss: 9.588532702764496e-05\n",
      "Epoch 22, Loss: 5.337919446901651e-06\n",
      "Epoch 23, Loss: 1.5386856375698699e-06\n",
      "Epoch 24, Loss: 7.701517461100593e-06\n",
      "Epoch 25, Loss: 0.0003211763978470117\n",
      "Epoch 26, Loss: 2.5165586521325167e-06\n",
      "Epoch 27, Loss: 7.086780442477902e-07\n",
      "Epoch 28, Loss: 8.52551750085695e-07\n",
      "Epoch 29, Loss: 1.1632000678218901e-06\n",
      "Epoch 30, Loss: 2.3668803805776406e-06\n",
      "Epoch 31, Loss: 2.3388849967886927e-06\n",
      "Epoch 32, Loss: 2.9909501790825743e-06\n",
      "Epoch 33, Loss: 2.244786401206511e-06\n",
      "Epoch 34, Loss: 4.355652436061064e-06\n",
      "Epoch 35, Loss: 9.504462354925636e-07\n",
      "Epoch 36, Loss: 2.0567788396874676e-06\n",
      "Epoch 37, Loss: 2.644614369273768e-06\n",
      "Epoch 38, Loss: 6.617351118620718e-06\n",
      "Epoch 39, Loss: 1.001716373139061e-05\n",
      "Epoch 40, Loss: 1.7763818505045492e-06\n",
      "Epoch 41, Loss: 1.4410129267616867e-07\n",
      "Epoch 42, Loss: 3.4114640357074677e-07\n",
      "Epoch 43, Loss: 5.86815076530911e-06\n",
      "Epoch 44, Loss: 2.7711641337191395e-07\n",
      "Epoch 45, Loss: 4.040589828946395e-06\n",
      "Epoch 46, Loss: 7.561624215668417e-08\n",
      "Epoch 47, Loss: 2.0007957957091094e-08\n",
      "Epoch 48, Loss: 1.779342113650273e-07\n",
      "Epoch 49, Loss: 9.22912093415107e-08\n",
      "Epoch 50, Loss: 5.037209689362498e-07\n"
     ]
    }
   ],
   "source": [
    "angle_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined_onehead.csv')\n",
    "angle_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg','head_x','head_y']\n",
    "X = angle_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "angle_df[coordinate_cols] = X_normalized\n",
    "\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(angle_df, sequence_length)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, RobertaConfig, RobertaModel\n",
    "\n",
    "\n",
    "# Transformer 모델을 위한 설정\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_heads=2, num_layers=4, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention 레이어\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 시퀀스 길이, 배치 크기, 특성 차원에 맞게 변환\n",
    "        x = x.transpose(0, 1)  # Transformer는 (seq_len, batch_size, features)의 형태를 기대함\n",
    "        \n",
    "        # Attention 통과\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Transformer Encoder 통과\n",
    "        transformer_output = self.transformer(attn_output)\n",
    "        \n",
    "        # 마지막 시퀀스 출력을 사용 (기본적으로 클래스 레이블 예측)\n",
    "        output = transformer_output[-1, :, :]\n",
    "        \n",
    "        # Fully connected layers 통과\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = 6 # 입력 특징의 크기\n",
    "hidden_size = 50\n",
    "num_classes = 1  # 이진 분류\n",
    "model = TransformerModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 루프\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(output.squeeze(), batch_y.float())\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1205_degreeonehhead.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def calculate_angle(pointA, pointB, pointC):\n",
    "    # 벡터 계산\n",
    "    BA = np.array(pointA) - np.array(pointB)\n",
    "    BC = np.array(pointC) - np.array(pointB)\n",
    "    # 내적과 벡터 크기 계산\n",
    "    dot_product = np.dot(BA, BC)\n",
    "    magnitude_BA = np.linalg.norm(BA)\n",
    "    magnitude_BC = np.linalg.norm(BC)\n",
    "    # 각도 계산 (라디안 -> 도)\n",
    "    cos_theta = dot_product / (magnitude_BA * magnitude_BC + 1e-6)  # 1e-6은 0으로 나누는 오류 방지\n",
    "    angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = '/home/alpaco/project/drunk_prj/data/3_frame_data/first_test_interpolation2.csv'  # 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "# 각도 계산을 위한 키포인트 인덱스\n",
    "# 예: 오른팔 (6, 8, 10), 왼팔 (5, 7, 9)\n",
    "keypoint_indices = {\n",
    "    \"right_arm\": (\"x7\",\"y7\", \"x9\",\"y9\", \"x11\",\"y11\"),\n",
    "    \"left_arm\": (\"x6\",\"y6\", \"x8\",\"y8\", \"x10\",\"y10\"),\n",
    "    \"right_leg\": (\"x13\",\"y13\", \"x15\",\"y15\",\"x17\",\"y17\"),\n",
    "    \"left_leg\":(\"x12\",\"y12\",\"x14\",\"y14\",\"x16\",\"y16\")\n",
    "}\n",
    "angles = []  # 각도를 저장할 리스트\n",
    "# 데이터프레임 순회\n",
    "for _, row in data.iterrows():\n",
    "    frame_angles = {}\n",
    "    for part, (xA, yA, xB, yB, xC, yC) in keypoint_indices.items():\n",
    "        pointA = (row[xA], row[yA])\n",
    "        pointB = (row[xB], row[yB])\n",
    "        pointC = (row[xC], row[yC])\n",
    "        frame_angles[part] = calculate_angle(pointA, pointB, pointC)\n",
    "    angles.append(frame_angles)\n",
    "angle_df = pd.DataFrame(angles)\n",
    "angle_df['FILENAME'] = data['FILENAME']\n",
    "angle_df['y'] = data['y']\n",
    "angle_df[\"frame\"] = data['frame']\n",
    "angle_df['label']=data['label']\n",
    "\n",
    "\n",
    "\n",
    "angle_df.to_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined_interpol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_677139/1538456503.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_degreeonehhead.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 84.45%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.74\n",
      "Recall: 0.84\n",
      "F1 Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_test_onehead.csv')\n",
    "test_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg','head_x','head_y']\n",
    "\n",
    "X = test_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "# y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_df[coordinate_cols] = X_normalized\n",
    "\n",
    "test_X_seq, test_y_seq = create_sequences(test_df, sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_X_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = 6\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "\n",
    "loaded_model = TransformerModel(input_size,hidden_size,num_layers)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_degreeonehhead.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree with Bert, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_combined.csv')\n",
    "angle_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#스케일링 진행 후\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg']\n",
    "X = angle_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "#y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "angle_df[coordinate_cols] = X_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>right_arm</th>\n",
       "      <th>left_arm</th>\n",
       "      <th>right_leg</th>\n",
       "      <th>left_leg</th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>y</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.147797</td>\n",
       "      <td>1.702799</td>\n",
       "      <td>1.175223</td>\n",
       "      <td>0.198660</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147797</td>\n",
       "      <td>1.277608</td>\n",
       "      <td>1.298180</td>\n",
       "      <td>1.308527</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147797</td>\n",
       "      <td>1.402886</td>\n",
       "      <td>0.702736</td>\n",
       "      <td>0.709738</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.402621</td>\n",
       "      <td>0.831924</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>1.409708</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.447156</td>\n",
       "      <td>1.770379</td>\n",
       "      <td>1.027621</td>\n",
       "      <td>0.764657</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78419</th>\n",
       "      <td>-0.099209</td>\n",
       "      <td>-1.541876</td>\n",
       "      <td>-1.888366</td>\n",
       "      <td>-1.785641</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78420</th>\n",
       "      <td>-0.114346</td>\n",
       "      <td>-1.543021</td>\n",
       "      <td>-1.888824</td>\n",
       "      <td>-2.018503</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78421</th>\n",
       "      <td>-0.116652</td>\n",
       "      <td>-1.540482</td>\n",
       "      <td>-1.889738</td>\n",
       "      <td>-1.726800</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78422</th>\n",
       "      <td>-0.081206</td>\n",
       "      <td>-1.539369</td>\n",
       "      <td>-1.889285</td>\n",
       "      <td>-1.471115</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78423</th>\n",
       "      <td>-0.093349</td>\n",
       "      <td>-1.544123</td>\n",
       "      <td>-1.890176</td>\n",
       "      <td>-2.018484</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78424 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       right_arm  left_arm  right_leg  left_leg  \\\n",
       "0       0.147797  1.702799   1.175223  0.198660   \n",
       "1       0.147797  1.277608   1.298180  1.308527   \n",
       "2       0.147797  1.402886   0.702736  0.709738   \n",
       "3       1.402621  0.831924   0.810964  1.409708   \n",
       "4      -0.447156  1.770379   1.027621  0.764657   \n",
       "...          ...       ...        ...       ...   \n",
       "78419  -0.099209 -1.541876  -1.888366 -1.785641   \n",
       "78420  -0.114346 -1.543021  -1.888824 -2.018503   \n",
       "78421  -0.116652 -1.540482  -1.889738 -1.726800   \n",
       "78422  -0.081206 -1.539369  -1.889285 -1.471115   \n",
       "78423  -0.093349 -1.544123  -1.890176 -2.018484   \n",
       "\n",
       "                                                FILENAME  y  frame  \\\n",
       "0          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  0      6   \n",
       "1          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  0      7   \n",
       "2          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  0      8   \n",
       "3          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  0      9   \n",
       "4          C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  0     10   \n",
       "...                                                  ... ..    ...   \n",
       "78419  278-3_cam01_drunken03_place03_night_summer_411...  1     95   \n",
       "78420  278-3_cam01_drunken03_place03_night_summer_411...  1     96   \n",
       "78421  278-3_cam01_drunken03_place03_night_summer_411...  1     97   \n",
       "78422  278-3_cam01_drunken03_place03_night_summer_411...  1     98   \n",
       "78423  278-3_cam01_drunken03_place03_night_summer_411...  1     99   \n",
       "\n",
       "                  label  \n",
       "0                   1.0  \n",
       "1                   1.0  \n",
       "2                   1.0  \n",
       "3                   1.0  \n",
       "4                   1.0  \n",
       "...                 ...  \n",
       "78419  ID: tensor(602.)  \n",
       "78420  ID: tensor(602.)  \n",
       "78421  ID: tensor(602.)  \n",
       "78422  ID: tensor(602.)  \n",
       "78423  ID: tensor(602.)  \n",
       "\n",
       "[78424 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 165/165 [00:02<00:00, 64.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.2174, Accuracy: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 165/165 [00:02<00:00, 70.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Loss: 0.0067, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 165/165 [00:02<00:00, 71.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Loss: 0.0026, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 165/165 [00:02<00:00, 70.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Loss: 0.0015, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 165/165 [00:02<00:00, 75.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 0.0009, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 165/165 [00:02<00:00, 78.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Loss: 0.0006, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 165/165 [00:02<00:00, 74.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Loss: 0.0004, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 165/165 [00:02<00:00, 68.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Loss: 0.0003, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 165/165 [00:02<00:00, 73.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Loss: 0.0002, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 165/165 [00:02<00:00, 75.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.0002, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 165/165 [00:02<00:00, 68.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Loss: 0.0002, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 165/165 [00:02<00:00, 70.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 165/165 [00:02<00:00, 71.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 165/165 [00:02<00:00, 74.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 165/165 [00:02<00:00, 67.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 165/165 [00:02<00:00, 68.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 165/165 [00:02<00:00, 64.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 165/165 [00:02<00:00, 64.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 165/165 [00:02<00:00, 67.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Loss: 0.0001, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 165/165 [00:02<00:00, 73.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 165/165 [00:02<00:00, 70.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 165/165 [00:02<00:00, 73.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 165/165 [00:02<00:00, 72.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 165/165 [00:02<00:00, 71.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 165/165 [00:02<00:00, 71.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 165/165 [00:02<00:00, 71.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 165/165 [00:02<00:00, 72.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 165/165 [00:02<00:00, 75.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 165/165 [00:02<00:00, 77.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 165/165 [00:02<00:00, 78.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 165/165 [00:02<00:00, 78.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 165/165 [00:02<00:00, 77.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 165/165 [00:02<00:00, 78.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 165/165 [00:02<00:00, 78.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 165/165 [00:02<00:00, 78.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 165/165 [00:02<00:00, 78.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 165/165 [00:02<00:00, 78.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 165/165 [00:02<00:00, 78.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 165/165 [00:02<00:00, 78.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 165/165 [00:02<00:00, 79.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 165/165 [00:02<00:00, 78.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 165/165 [00:02<00:00, 78.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 165/165 [00:02<00:00, 79.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 165/165 [00:02<00:00, 80.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 165/165 [00:02<00:00, 80.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 165/165 [00:02<00:00, 80.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 165/165 [00:02<00:00, 80.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 165/165 [00:02<00:00, 80.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 165/165 [00:02<00:00, 79.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 165/165 [00:02<00:00, 79.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: 0.0000, Accuracy: 1.0000\n",
      "Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 6. sequence length 생성하기\n",
    "import numpy as np\n",
    "#Sequence Lenght 설정 후 진행 예정\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    # 'FILENAME'과 'label' 기준으로 그룹화\n",
    "    for _, group in df.groupby(['FILENAME', 'label']):\n",
    "        # 그룹 내 'frame' 기준 정렬\n",
    "        group = group.sort_values(by=['frame']).reset_index(drop=True)\n",
    "        \n",
    "        # frame, FILENAME, label, id, y 제외한 좌표 피처\n",
    "        data_X = group.drop(columns=['frame', 'FILENAME', 'label','y'], errors='ignore').values  \n",
    "        \n",
    "        data_y = group['y'].values  # 이진 분류 레이블\n",
    "        \n",
    "        # 시퀀스 생성\n",
    "        for i in range(len(data_X) - seq_length+1):\n",
    "            x = data_X[i:i + seq_length]\n",
    "            y = data_y[i + seq_length - 1]  # 시퀀스의 마지막 레이블 사용\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "sequence_length = 90\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq, Y_seq = create_sequences(angle_df, sequence_length)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "# 학습 데이터와 테스트 데이터로 나누고, 라벨의 비율을 유지합니다.\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_seq, Y_seq, test_size=0.2, stratify=Y_seq, random_state=42)\n",
    "\n",
    "# 학습 데이터를 다시 셔플하여 모델이 순서에 너무 의존하지 않도록 합니다.\n",
    "train_indices = np.arange(len(train_X))\n",
    "np.random.shuffle(train_indices)\n",
    "train_X, train_y = train_X[train_indices], train_y[train_indices]\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "train_X_tensor = torch.FloatTensor(train_X)\n",
    "train_y_tensor = torch.LongTensor(train_y)\n",
    "valid_X_tensor = torch.FloatTensor(valid_X)\n",
    "valid_y_tensor = torch.LongTensor(valid_y)\n",
    "\n",
    "\n",
    "# 텐서 타입 확인\n",
    "train_X_tensor = train_X_tensor.float()\n",
    "valid_X_tensor = valid_X_tensor.float()\n",
    "\n",
    "train_y_tensor = train_y_tensor.float()\n",
    "valid_y_tensor = valid_y_tensor.float()\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "valid_dataset = TensorDataset(valid_X_tensor, valid_y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "\n",
    "class BertForKeypointClassification(nn.Module):\n",
    "    def __init__(self, input_size, sequence_length, hidden_size=256, num_classes=1):\n",
    "        super(BertForKeypointClassification, self).__init__()\n",
    "        # BERT 설정\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_attention_heads=8,\n",
    "            num_hidden_layers=4,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            max_position_embeddings=sequence_length,\n",
    "            vocab_size=1  # 가상의 토큰 ID\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # 입력 차원 조정\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # 분류기\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()  # 이진 분류용\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, hidden_size)\n",
    "        bert_output = self.bert(inputs_embeds=x)  # BERT의 출력\n",
    "        pooled_output = bert_output.pooler_output  # [CLS] 토큰의 출력\n",
    "        return self.classifier(pooled_output)  # 분류 결과\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = 4  # x1~x17, y1~y17\n",
    "sequence_length = 90\n",
    "hidden_size = 256\n",
    "num_classes = 1  # 이진 분류\n",
    "model = BertForKeypointClassification(input_size, sequence_length, hidden_size, num_classes)\n",
    "model.to(device)\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 50\n",
    "\n",
    "# 학습 및 검증 함수\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "\n",
    "    # F1 Score 계산\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 모델 예측 및 손실 계산\n",
    "            outputs = model(sequences).squeeze()  # (batch_size,)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 손실 및 정확도 업데이트\n",
    "            epoch_loss += loss.item()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Training Complete.\")\n",
    "\n",
    "# 모델 초기화\n",
    "input_size = 4  # x1~x17, y1~y17\n",
    "sequence_length = 90\n",
    "hidden_size = 256\n",
    "num_classes = 1  # 이진 분류\n",
    "model = BertForKeypointClassification(input_size, sequence_length, hidden_size, num_classes)\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 50\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/home/alpaco/project/drunk_prj/models/only_model/1205_Bert.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/degree_test.csv')\n",
    "test_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "coordinate_cols = ['right_arm','left_arm','right_leg','left_leg']\n",
    "\n",
    "X = test_df[coordinate_cols].values  # 34개의 좌표 피처\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_normalized = scaler_X.fit_transform(X)\n",
    "# y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "test_df[coordinate_cols] = X_normalized\n",
    "\n",
    "test_X_seq, test_y_seq = create_sequences(test_df, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_677139/1733674475.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_Bert.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on test data: 84.27%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.78\n",
      "Recall: 0.76\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 PyTorch 텐서로 변환하여 모델 학습에 사용합니다.\n",
    "test_X_tensor = torch.FloatTensor(test_X_seq)\n",
    "test_y_tensor = torch.LongTensor(test_y_seq)\n",
    "\n",
    "# 텐서 타입 확인\n",
    "test_X_tensor = test_X_tensor.float()\n",
    "test_y_tensor = test_y_tensor.float()\n",
    "\n",
    "\n",
    "# PyTorch의 DataLoader를 사용해 데이터를 묶어 관리할 수 있습니다.\n",
    "batch_size = 16  # 배치 사이즈는 한 번에 학습하는 데이터 개수를 뜻합니다.\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "# 모델 초기화\n",
    "input_size = 4  \n",
    "sequence_length = 90\n",
    "hidden_size = 256\n",
    "num_classes = 1  # 이진 분류\n",
    "loaded_model = BertForKeypointClassification(input_size, sequence_length, hidden_size, num_classes)\n",
    "loaded_model.load_state_dict(torch.load('/home/alpaco/project/drunk_prj/models/only_model/1205_Bert.pt'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        outputs = loaded_model(inputs)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 이진 분류로 변환\n",
    "        \n",
    "        # 예측값과 실제값 저장\n",
    "        all_preds.extend(preds.astype(int).squeeze())\n",
    "        all_labels.extend(labels.cpu().numpy().astype(int).squeeze())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        correct += np.sum(preds.astype(int).squeeze() == labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the model on test data: {accuracy:.2f}%')\n",
    "\n",
    "# 혼돈 행렬 계산\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# 혼돈 행렬 출력\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# Precision, Recall, F1-Score 계산\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpolation 넣어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>frame</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>...</th>\n",
       "      <th>y14</th>\n",
       "      <th>x15</th>\n",
       "      <th>y15</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "      <th>FILENAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1737.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>1675.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1720.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1621.0</td>\n",
       "      <td>659.0</td>\n",
       "      <td>...</td>\n",
       "      <td>893.0</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>1654.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>...</td>\n",
       "      <td>821.0</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>1580.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>...</td>\n",
       "      <td>802.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>735.0</td>\n",
       "      <td>1341.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>797.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78419</th>\n",
       "      <td>24195</td>\n",
       "      <td>95</td>\n",
       "      <td>2533.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2509.0</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78420</th>\n",
       "      <td>24196</td>\n",
       "      <td>96</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>1067.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2508.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78421</th>\n",
       "      <td>24197</td>\n",
       "      <td>97</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>1063.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2507.0</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78422</th>\n",
       "      <td>24198</td>\n",
       "      <td>98</td>\n",
       "      <td>2529.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2526.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2515.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2509.0</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>1293.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78423</th>\n",
       "      <td>24199</td>\n",
       "      <td>99</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>1064.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2509.0</td>\n",
       "      <td>2496.0</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>ID: tensor(602.)</td>\n",
       "      <td>1</td>\n",
       "      <td>278-3_cam01_drunken03_place03_night_summer_411...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78424 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  frame      x1      y1      x2     y2   x3   y3      x4  \\\n",
       "0               0      6  1710.0   711.0  1715.0  701.0  0.0  0.0  1737.0   \n",
       "1               1      7     0.0     0.0     0.0    0.0  0.0  0.0  1621.0   \n",
       "2               2      8     0.0     0.0  1499.0  601.0  0.0  0.0  1516.0   \n",
       "3               3      9     0.0     0.0     0.0    0.0  0.0  0.0  1430.0   \n",
       "4               4     10     0.0     0.0     0.0    0.0  0.0  0.0  1352.0   \n",
       "...           ...    ...     ...     ...     ...    ...  ...  ...     ...   \n",
       "78419       24195     95  2533.0  1069.0   955.0    0.0  0.0  0.0     0.0   \n",
       "78420       24196     96  2532.0  1067.0   953.0    0.0  0.0  0.0     0.0   \n",
       "78421       24197     97  2528.0  1063.0   950.0    0.0  0.0  0.0     0.0   \n",
       "78422       24198     98  2529.0  1065.0   948.0    0.0  0.0  0.0     0.0   \n",
       "78423       24199     99  2527.0  1064.0   951.0    0.0  0.0  0.0     0.0   \n",
       "\n",
       "           y4  ...     y14     x15     y15     x16     y16     x17     y17  \\\n",
       "0       706.0  ...  1031.0  1675.0  1000.0  1720.0  1076.0  1660.0  1052.0   \n",
       "1       659.0  ...   893.0  1648.0   919.0  1480.0   939.0  1654.0  1026.0   \n",
       "2       602.0  ...   821.0  1538.0   836.0  1461.0   891.0  1580.0   926.0   \n",
       "3       553.0  ...   802.0  1390.0   728.0  1461.0   884.0  1360.0   785.0   \n",
       "4       500.0  ...   735.0  1341.0   713.0  1376.0   797.0  1329.0   777.0   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "78419  2522.0  ...  2516.0     0.0  1287.0     0.0  2509.0  2497.0  1294.0   \n",
       "78420  2527.0  ...  2516.0     0.0  1287.0     0.0  2508.0  2498.0  1294.0   \n",
       "78421  2523.0  ...  2516.0     0.0  1286.0     0.0  2507.0  2498.0  1292.0   \n",
       "78422  2526.0  ...  2515.0     0.0  1286.0     0.0  2509.0  2501.0  1293.0   \n",
       "78423  2522.0  ...  2517.0     0.0  1284.0     0.0  2509.0  2496.0  1290.0   \n",
       "\n",
       "                  label  y                                           FILENAME  \n",
       "0                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "1                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "2                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "3                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "4                   1.0  0      C_32_8_smp_su_09-11_11-43-00_b_for_DF2_label1  \n",
       "...                 ... ..                                                ...  \n",
       "78419  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78420  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78421  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78422  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "78423  ID: tensor(602.)  1  278-3_cam01_drunken03_place03_night_summer_411...  \n",
       "\n",
       "[78424 rows x 39 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.drop('Unnamed: 0\t',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  frame      x1      y1      x2      y2  x3      y3  x4  \\\n",
      "60224      6000.0      0  2022.0  1205.0  1012.0  2048.0 NaN  1197.0 NaN   \n",
      "60225      6001.0      1  2059.0  1217.0  1047.0  2058.0 NaN  1206.0 NaN   \n",
      "60226      6002.0      2  2086.0  1213.0  1050.0  2063.0 NaN  1201.0 NaN   \n",
      "60227      6003.0      3  2117.5  1214.0  1021.0  2115.0 NaN  1179.5 NaN   \n",
      "60228      6004.0      4  2149.0  1177.0   992.0  2167.0 NaN  1158.0 NaN   \n",
      "...           ...    ...     ...     ...     ...     ...  ..     ...  ..   \n",
      "33263     33263.0    245     NaN     NaN     NaN     NaN NaN     NaN NaN   \n",
      "33264     33264.0    246     NaN     NaN     NaN     NaN NaN     NaN NaN   \n",
      "33265     33265.0    247     NaN     NaN     NaN     NaN NaN     NaN NaN   \n",
      "33266     33266.0    248     NaN     NaN     NaN     NaN NaN     NaN NaN   \n",
      "33267     33267.0    249     NaN     NaN     NaN     NaN NaN     NaN NaN   \n",
      "\n",
      "           y4  ...     y14     x15     y15     x16     y16     x17     y17  \\\n",
      "60224  1994.0  ...  1942.0  2033.0  1371.0  1126.0  1983.0  1968.0  1437.0   \n",
      "60225  1967.0  ...  1985.0  2046.0  1374.0  1143.0  1973.0  1962.0  1435.0   \n",
      "60226  2002.0  ...  2027.0  2045.0  1383.0  1150.0  1976.0  1988.0  1435.0   \n",
      "60227  2078.0  ...  2113.0  2095.0  1377.0  1126.0  1969.0  2049.0  1432.0   \n",
      "60228  2110.0  ...  2115.0  2145.0  1376.0  1102.0  1989.0  2092.0  1416.0   \n",
      "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "33263     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "33264     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "33265     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "33266     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "33267     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
      "\n",
      "                  label  y                                           FILENAME  \n",
      "60224  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60225  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60226  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60227  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60228  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "...                 ... ..                                                ...  \n",
      "33263               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33264               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33265               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33266               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33267               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "\n",
      "[78424 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 예제 데이터프레임 생성\n",
    "df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv')\n",
    "\n",
    "# 0인 값을 결측치로 설정 (label, frame, y, filename 제외)\n",
    "cols_to_interpolate = [col for col in df.columns if col not in ['label', 'frame', 'y', 'FILENAME']]\n",
    "df[cols_to_interpolate] = df[cols_to_interpolate].replace(0, np.nan)\n",
    "\n",
    "# 그룹화 및 보간법 적용\n",
    "def interpolate_group(group):\n",
    "    # 프레임 정렬\n",
    "    group = group.sort_values('frame')\n",
    "    # 각 열에서 처음과 마지막 유효 값의 인덱스 찾기\n",
    "    for col in cols_to_interpolate:\n",
    "        valid_idx = group[col].first_valid_index(), group[col].last_valid_index()\n",
    "        if valid_idx[0] is not None and valid_idx[1] is not None:\n",
    "            # 유효 범위 내 보간\n",
    "            group.loc[valid_idx[0]:valid_idx[1], col] = group.loc[valid_idx[0]:valid_idx[1], col].interpolate(method='linear', limit_direction='both')\n",
    "    return group\n",
    "\n",
    "# filename, label로 그룹화\n",
    "df_interpolated = df.groupby(['FILENAME', 'label'], group_keys=False).apply(interpolate_group)\n",
    "\n",
    "# 결과 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  frame      x1      y1      x2      y2   x3      y3   x4  \\\n",
      "60224      6000.0      0  2022.0  1205.0  1012.0  2048.0  0.0  1197.0  0.0   \n",
      "60225      6001.0      1  2059.0  1217.0  1047.0  2058.0  0.0  1206.0  0.0   \n",
      "60226      6002.0      2  2086.0  1213.0  1050.0  2063.0  0.0  1201.0  0.0   \n",
      "60227      6003.0      3     0.0  1214.0     0.0     0.0  0.0     0.0  0.0   \n",
      "60228      6004.0      4  2149.0  1177.0   992.0  2167.0  0.0  1158.0  0.0   \n",
      "...           ...    ...     ...     ...     ...     ...  ...     ...  ...   \n",
      "33263     33263.0    245     0.0     0.0     0.0     0.0  0.0     0.0  0.0   \n",
      "33264     33264.0    246     0.0     0.0     0.0     0.0  0.0     0.0  0.0   \n",
      "33265     33265.0    247     0.0     0.0     0.0     0.0  0.0     0.0  0.0   \n",
      "33266     33266.0    248     0.0     0.0     0.0     0.0  0.0     0.0  0.0   \n",
      "33267     33267.0    249     0.0     0.0     0.0     0.0  0.0     0.0  0.0   \n",
      "\n",
      "           y4  ...     y14     x15     y15     x16     y16     x17     y17  \\\n",
      "60224  1994.0  ...  1942.0  2033.0  1371.0  1126.0  1983.0  1968.0  1437.0   \n",
      "60225  1967.0  ...  1985.0  2046.0  1374.0  1143.0  1973.0  1962.0  1435.0   \n",
      "60226  2002.0  ...  2027.0  2045.0  1383.0  1150.0  1976.0  1988.0  1435.0   \n",
      "60227  2078.0  ...  2113.0     0.0  1377.0     0.0  1969.0  2049.0  1432.0   \n",
      "60228  2110.0  ...  2115.0  2145.0  1376.0  1102.0  1989.0  2092.0  1416.0   \n",
      "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "33263     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "33264     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "33265     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "33266     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "33267     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "                  label  y                                           FILENAME  \n",
      "60224  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60225  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60226  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60227  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "60228  ID: tensor(124.)  1  209-1_cam01_drunken01_place03_night_spring_499...  \n",
      "...                 ... ..                                                ...  \n",
      "33263               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33264               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33265               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33266               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "33267               6.0  0      C_32_9_smp_su_09-11_13-25-00_c_aft_DF2_label3  \n",
      "\n",
      "[78424 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/final_combined.csv')\n",
    "\n",
    "# 보간 대상 열만 추출\n",
    "cols_to_check = [col for col in df.columns if col not in ['label', 'frame', 'y', 'FILENAME']]\n",
    "\n",
    "# 조건에 맞는 행에 대해 0을 NaN으로 변환\n",
    "mask = (df[cols_to_check] == 0).all(axis=1)  # ['label', 'frame', 'y', 'filename']를 제외한 값들이 모두 0인 경우\n",
    "df.loc[mask, cols_to_check] = np.nan\n",
    "\n",
    "# 그룹화 및 보간법 적용\n",
    "def interpolate_group(group):\n",
    "    # 프레임 정렬\n",
    "    group = group.sort_values('frame')\n",
    "    # 각 열에서 처음과 마지막 유효 값의 인덱스 찾기\n",
    "    for col in cols_to_check:\n",
    "        valid_idx = group[col].first_valid_index(), group[col].last_valid_index()\n",
    "        if valid_idx[0] is not None and valid_idx[1] is not None:\n",
    "            # 유효 범위 내 보간\n",
    "            group.loc[valid_idx[0]:valid_idx[1], col] = group.loc[valid_idx[0]:valid_idx[1], col].interpolate(method='linear', limit_direction='both')\n",
    "    return group\n",
    "\n",
    "# filename, label로 그룹화\n",
    "df_interpolated = df.groupby(['FILENAME', 'label'], group_keys=False).apply(interpolate_group)\n",
    "\n",
    "# 결과 출력\n",
    "print(df_interpolated)\n",
    "\n",
    "\n",
    "df_interpolated[cols_to_interpolate] = df_interpolated[cols_to_interpolate].fillna(0)\n",
    "\n",
    "df_interpolated.to_csv('/home/alpaco/project/drunk_prj/data/3_frame_data/first_interpolation2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
